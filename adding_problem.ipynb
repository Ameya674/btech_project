{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a323578-ebbe-4431-b0a1-2d97e32e8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from helpers.ipynb\n"
     ]
    }
   ],
   "source": [
    "# importing necessary dependancies\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import import_ipynb\n",
    "from helpers import train_and_validate, count_parameters, plot_loss_comparison, plot_test_predictions, compare_mse_loss\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46efb6aa-c2a3-4119-9057-d28f8b891853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use NVIDIA Geforce GTX 1650\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c8c8f7-2eea-45ae-b319-f30e7bb53cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 100\n",
    "input_size = 2\n",
    "hidden_size = 100\n",
    "num_classes = 1\n",
    "lstmnet_learning_rate = 0.001\n",
    "customnet_learning_rate = 0.001\n",
    "momentum = 0.8\n",
    "use_nesterov = True\n",
    "max_norm = 1.0\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7bbe0e-ca99-4321-8893-1970a90e7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_problem_generator(N, seq_len=8, high=1):\n",
    "    \"\"\" A data generator for adding problem.\n",
    "\n",
    "    A single entry has a 2D vector with two rows of the same length. \n",
    "    The first row has random numbers, and the second row contains a \n",
    "    binary mask with ones at only two positions. The label for this \n",
    "    entry is the sum of the numbers from the first row where the mask \n",
    "    is one.\n",
    "\n",
    "     input          label\n",
    "     -----          -----\n",
    "    1 4 5 3  ----->   9 (4 + 5)\n",
    "    0 1 1 0\n",
    "\n",
    "    N: the number of the entries.\n",
    "    seq_len: the length of a single sequence.\n",
    "    high: the random data is sampled from a [0, high] uniform distribution.\n",
    "    return: (X, Y), X the data, Y the label.\n",
    "    \"\"\"\n",
    "    X_num = np.round(np.random.uniform(low=0, high=high, size=(N, seq_len, 1)), 4)\n",
    "    X_mask = np.zeros((N, seq_len, 1))\n",
    "    Y = np.ones((N, 1))\n",
    "    for i in range(N):\n",
    "        # Default uniform distribution on position sampling\n",
    "        positions = np.random.choice(seq_len, size=2, replace=False)\n",
    "        X_mask[i, positions] = 1\n",
    "        Y[i, 0] = np.sum(X_num[i, positions])\n",
    "    X = np.append(X_num, X_mask, axis=2)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bfe027-7a26-44de-93e5-b011d76eb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the train and test datasets\n",
    "X_train, Y_train = adding_problem_generator(10000, 8, 1)\n",
    "X_test, Y_test = adding_problem_generator(100, 8, 1)\n",
    "X_train = torch.tensor(X_train).float()\n",
    "Y_train = torch.tensor(Y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "Y_test = torch.tensor(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2a45ae-a843-4ab5-a175-090bead38034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 8, 2]) torch.Size([10000, 1])\n",
      "torch.Size([1000, 8, 2]) torch.Size([1000, 1])\n",
      "torch.Size([100, 8, 2]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1052d544-1046-44b5-acbf-a84bfc3ea792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddingProblemDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "\n",
    "train_dataset = AddingProblemDataset(X_train, Y_train)\n",
    "val_dataset = AddingProblemDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d5ee41-b7c6-4f0b-841b-9c35d32718ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a371949-577d-46c4-9359-6aa9d1ffd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_, (h_t, c_t) = self.lstm(x)\n",
    "        out = F.relu(self.fc(x_[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60bd2963-0117-4584-a8a6-35488ae78e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnit(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # input weight\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size), requires_grad = True)\n",
    "\n",
    "        # forget gate\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size), requires_grad = True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for weight in self.parameters():\n",
    "            if weight.data.ndimension() < 2:  # Bias or 1D weights\n",
    "                nn.init.zeros_(weight)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(weight)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "\n",
    "        bs, seq, ip = x.size()\n",
    "        hidden_seq = []\n",
    "\n",
    "        if hidden is None:\n",
    "            h_prev, c_prev = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device),\n",
    "            )\n",
    "        else:\n",
    "            h_prev, c_prev = hidden\n",
    "\n",
    "        for t in range(seq):\n",
    "            x_t = x[:, t, :]\n",
    "            x_t = torch.tanh(x_t @ self.W_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
    "            h_t = (f_t * h_prev) + ((1 - f_t) * x_t)\n",
    "            c_t = c_prev\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim = 0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.custom_layer = CustomUnit(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_, (h_t, c_t) = self.custom_layer(x)\n",
    "        out = F.relu(self.fc(x_[:, -1, :]))\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c475247-49cd-4706-b8b5-6e44458cbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet = LSTMNet(input_size, hidden_size, num_classes).to(device)\n",
    "customnet= CustomNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11c79dc-eddc-426e-a9de-acc4bc1044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lstmnet_parameters = count_parameters(lstmnet)\n",
    "total_customnet_parameters = count_parameters(customnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55f812c-0ee4-42c4-b061-244757362590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total LSTMNet Parameters: 41701\n",
      "Total CustomNet Parameters: 20401\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total LSTMNet Parameters: {total_lstmnet_parameters}\")\n",
    "print(f\"Total CustomNet Parameters: {total_customnet_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d6b79c9-23f7-47aa-bcc1-8ee903cd680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lstmnet_optimizer = torch.optim.Adam(lstmnet.parameters(), lr = lstmnet_learning_rate)\n",
    "customnet_optimizer = torch.optim.Adam(customnet.parameters(), lr = customnet_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93e640cd-3539-4c4c-8670-83bd93558e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet_train_loss_array = []\n",
    "customnet_train_loss_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f56d1-a1a1-44b4-8da2-1de2d929f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 1 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 2 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 3 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 4 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 5 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 6 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 7 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 8 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 9 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 10 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 11 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 12 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 13 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 14 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 15 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 16 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 17 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 18 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 19 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 20 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 21 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 22 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 23 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 24 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 25 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 26 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 27 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 28 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 29 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 30 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 31 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 32 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 33 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 34 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 35 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 36 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 37 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 38 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 39 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 40 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 41 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 42 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 43 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 44 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 45 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 46 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 47 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 48 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 49 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 50 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 51 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 52 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 53 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 54 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 55 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 56 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 57 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 58 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 59 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 60 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 61 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 62 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 63 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 64 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 65 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 66 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 67 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 68 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 69 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 70 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 71 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 72 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 73 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 74 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 75 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 76 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 77 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 78 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 79 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 80 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 81 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 82 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 83 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 84 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 85 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 86 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 87 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 88 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 89 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 90 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 91 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 92 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 93 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 94 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 95 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 96 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 97 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 98 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 99 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 100 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 101 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 102 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 103 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 104 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 105 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 106 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 107 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 108 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 109 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 110 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 111 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 112 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 113 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 114 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 115 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 116 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 117 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 118 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 119 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 120 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 121 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 122 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 123 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 124 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 125 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 126 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 127 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 128 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 129 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 130 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 131 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 132 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 133 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 134 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 135 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 136 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 137 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 138 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 139 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 140 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 141 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 142 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 143 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 144 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 145 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 146 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 147 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 148 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 149 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 150 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 151 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 152 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 153 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 154 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 155 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 156 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 157 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 158 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 159 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 160 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 161 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 162 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 163 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 164 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 165 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 166 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 167 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 168 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 169 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 170 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 171 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 172 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 173 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 174 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 175 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 176 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 177 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 178 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 179 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 180 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 181 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 182 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 183 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 184 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 185 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 186 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 187 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 188 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 189 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 190 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 191 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 192 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 193 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 194 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 195 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 196 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 197 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 198 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 199 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 200 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 201 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 202 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 203 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 204 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 205 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 206 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 207 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 208 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 209 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 210 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 211 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 212 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 213 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 214 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 215 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 216 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 217 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 218 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 219 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 220 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 221 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 222 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 223 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 224 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 225 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 226 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 227 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 228 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 229 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 230 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 231 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 232 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 233 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 234 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 235 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 236 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 237 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 238 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 239 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 240 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 241 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 242 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 243 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 244 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 245 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 246 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 247 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 248 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 249 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 250 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 251 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 252 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 253 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 254 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 255 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 256 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 257 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 258 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 259 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 260 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 261 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 262 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 263 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 264 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 265 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 266 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 267 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 268 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 269 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 270 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 271 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 272 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 273 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 274 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 275 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 276 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 277 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 278 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 279 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 280 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 281 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 282 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 283 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 284 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 285 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 286 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 287 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 288 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 289 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 290 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 291 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 292 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 293 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 294 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 295 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 296 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 297 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 298 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 299 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 300 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 301 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 302 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 303 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 304 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 305 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 306 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 307 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 308 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 309 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 310 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 311 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 312 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 313 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 314 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 315 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 316 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 317 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 318 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 319 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 320 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 321 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 322 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 323 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 324 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 325 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 326 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 327 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 328 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 329 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 330 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 331 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 332 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 333 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 334 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 335 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 336 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n"
     ]
    }
   ],
   "source": [
    "lstmnet_start_time = time.time()\n",
    "train_and_validate(epochs, device, lstmnet, train_loader, criterion, lstmnet_optimizer, lstmnet_train_loss_array)\n",
    "lstmnet_end_time = time.time()\n",
    "lstmnet_total_training_time = lstmnet_end_time - lstmnet_start_time\n",
    "lstmnet_avg_training_time_per_epoch = lstmnet_total_training_time / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb145f-639a-426d-9b36-ffd6500259b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 1 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 2 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 3 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n",
      "Epoch no.: 4 | Training Loss: 1.170299892425537 | Validation Loss: 1.228244459629059\n"
     ]
    }
   ],
   "source": [
    "customnet_start_time = time.time()\n",
    "train_and_validate(epochs, device, customnet, train_loader, criterion, customnet_optimizer, customnet_train_loss_array)\n",
    "customnet_end_time = time.time()\n",
    "customnet_total_training_time = customnet_end_time - customnet_start_time\n",
    "customnet_avg_training_time_per_epoch = customnet_total_training_time / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bedf4-f1fc-42fd-9d20-8eca910c23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet_predictions = lstmnet(X_test.to(device)).cpu().detach().numpy()\n",
    "customnet_predictions = customnet(X_test.to(device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daaf7ef-2332-4a1a-8243-1515da878b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL REPORT PRINT\n",
    "print()\n",
    "print()\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Total Parameters:     LSTMNet: {total_lstmnet_parameters}  |  CustomNet: {total_customnet_parameters}\")\n",
    "print()\n",
    "print(f\"Total Training Time:     LSTMNet: {lstmnet_total_training_time}s  |  CustomNet: {customnet_total_training_time}s\")\n",
    "print()\n",
    "print(f\"Average Training Time Per Epoch:     LSTMNet: {lstmnet_avg_training_time_per_epoch}s  |  CustomNet: {customnet_avg_training_time_per_epoch}s\")\n",
    "\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04092862-d9b3-4298-b05d-0394fedcde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_comparison(epochs, lstmnet_train_loss_array, customnet_train_loss_array, \"LSTM Training Loss\", \"CustomNet Training Loss\", \"Training Loss Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ba5a6-74a9-4d3f-a71d-69004c325d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_predictions(Y_test, lstmnet_predictions, customnet_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5df72b-e9e6-497b-a1a1-21ebce31bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mse_loss(Y_test, lstmnet_predictions, customnet_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422367d-1848-4fed-85b2-44f16821f934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btp",
   "language": "python",
   "name": "btp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
