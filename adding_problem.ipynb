{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a323578-ebbe-4431-b0a1-2d97e32e8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary dependancies\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import import_ipynb\n",
    "from models import LSTMNet, CustomNet\n",
    "from helpers import train_and_validate, count_parameters, plot_loss_comparison, plot_test_predictions, compare_mse_loss\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46efb6aa-c2a3-4119-9057-d28f8b891853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use NVIDIA Geforce GTX 1650\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22c8c8f7-2eea-45ae-b319-f30e7bb53cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 100\n",
    "input_size = 2\n",
    "hidden_size = 100\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "momentum = 0.8\n",
    "use_nesterov = True\n",
    "max_norm = 1.0\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad7bbe0e-ca99-4321-8893-1970a90e7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create the adding problem dataset\n",
    "def adding_problem_generator(N, seq_len=8, high=1):\n",
    "    \"\"\" A data generator for adding problem.\n",
    "\n",
    "    A single entry has a 2D vector with two rows of the same length. \n",
    "    The first row has random numbers, and the second row contains a \n",
    "    binary mask with ones at only two positions. The label for this \n",
    "    entry is the sum of the numbers from the first row where the mask \n",
    "    is one.\n",
    "\n",
    "     input          label\n",
    "     -----          -----\n",
    "    1 4 5 3  ----->   9 (4 + 5)\n",
    "    0 1 1 0\n",
    "\n",
    "    N: the number of the entries.\n",
    "    eq_len: the length of a single sequence.\n",
    "    p: the probability of 1 in generated mask\n",
    "    high: the random data is sampled from a [0, high] uniform distribution.\n",
    "    return: (X, Y), X the data, Y the label.\n",
    "    \"\"\"\n",
    "    X_num = np.random.uniform(low=0, high=high, size=(N, seq_len, 1))\n",
    "    X_mask = np.zeros((N, seq_len, 1))\n",
    "    Y = np.ones((N, 1))\n",
    "    for i in range(N):\n",
    "        # Default uniform distribution on position sampling\n",
    "        positions = np.random.choice(seq_len, size=2, replace=False)\n",
    "        X_mask[i, positions] = 1\n",
    "        Y[i, 0] = np.sum(X_num[i, positions])\n",
    "    X = np.append(X_num, X_mask, axis=2)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7bfe027-7a26-44de-93e5-b011d76eb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the train and test datasets\n",
    "X_train, Y_train = adding_problem_generator(10000, 8, 1)\n",
    "X_val, Y_val = adding_problem_generator(1000, 8, 1)\n",
    "X_test, Y_test = adding_problem_generator(100, 8, 1)\n",
    "X_train = torch.tensor(X_train).float()\n",
    "Y_train = torch.tensor(Y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "Y_val = torch.tensor(Y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "Y_test = torch.tensor(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c2a45ae-a843-4ab5-a175-090bead38034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 8, 2]) torch.Size([10000, 1])\n",
      "torch.Size([1000, 8, 2]) torch.Size([1000, 1])\n",
      "torch.Size([100, 8, 2]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1052d544-1046-44b5-acbf-a84bfc3ea792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddingProblemDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "\n",
    "train_dataset = AddingProblemDataset(X_train, Y_train)\n",
    "val_dataset = AddingProblemDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69d5ee41-b7c6-4f0b-841b-9c35d32718ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c475247-49cd-4706-b8b5-6e44458cbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet = LSTMNet(input_size, hidden_size, num_classes).to(device)\n",
    "customnet= CustomNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c11c79dc-eddc-426e-a9de-acc4bc1044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lstmnet_parameters = count_parameters(lstmnet)\n",
    "total_customnet_parameters = count_parameters(customnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d6b79c9-23f7-47aa-bcc1-8ee903cd680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "lstmnet_optimizer = torch.optim.Adam(lstmnet.parameters(), lr = learning_rate)\n",
    "customnet_optimizer = torch.optim.Adam(customnet.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93e640cd-3539-4c4c-8670-83bd93558e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet_train_loss_array = []\n",
    "lstmnet_val_loss_array = []\n",
    "customnet_train_loss_array = []\n",
    "customnet_val_loss_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e15f56d1-a1a1-44b4-8da2-1de2d929f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 1 | Training Loss: 0.7077418802678586 | Validation Loss: 0.2397512137889862\n",
      "Epoch no.: 2 | Training Loss: 0.20147685214877128 | Validation Loss: 0.1750206708908081\n",
      "Epoch no.: 3 | Training Loss: 0.1787444432079792 | Validation Loss: 0.16289855986833573\n",
      "Epoch no.: 4 | Training Loss: 0.17031691469252108 | Validation Loss: 0.15555420219898225\n",
      "Epoch no.: 5 | Training Loss: 0.16510420970618725 | Validation Loss: 0.15112572610378266\n",
      "Epoch no.: 6 | Training Loss: 0.16207718662917614 | Validation Loss: 0.14807993173599243\n",
      "Epoch no.: 7 | Training Loss: 0.15970669627189638 | Validation Loss: 0.14597581699490547\n",
      "Epoch no.: 8 | Training Loss: 0.1574725940078497 | Validation Loss: 0.1439147762954235\n",
      "Epoch no.: 9 | Training Loss: 0.15485644340515137 | Validation Loss: 0.14304085001349448\n",
      "Epoch no.: 10 | Training Loss: 0.15226540878415107 | Validation Loss: 0.14041440486907958\n",
      "Epoch no.: 11 | Training Loss: 0.1503749131411314 | Validation Loss: 0.1390171617269516\n",
      "Epoch no.: 12 | Training Loss: 0.14912426277995108 | Validation Loss: 0.1383963018655777\n",
      "Epoch no.: 13 | Training Loss: 0.14826876662671565 | Validation Loss: 0.13808755502104758\n",
      "Epoch no.: 14 | Training Loss: 0.1481198211759329 | Validation Loss: 0.1381198339164257\n",
      "Epoch no.: 15 | Training Loss: 0.14810857638716698 | Validation Loss: 0.13812961652874947\n",
      "Epoch no.: 16 | Training Loss: 0.1480700508505106 | Validation Loss: 0.13819050639867783\n",
      "Epoch no.: 17 | Training Loss: 0.14816730722784996 | Validation Loss: 0.13814244866371156\n",
      "Epoch no.: 18 | Training Loss: 0.1480736568570137 | Validation Loss: 0.13815979063510894\n",
      "Epoch no.: 19 | Training Loss: 0.14794237554073333 | Validation Loss: 0.13805162832140921\n",
      "Epoch no.: 20 | Training Loss: 0.14799609504640102 | Validation Loss: 0.1382933586835861\n",
      "Epoch no.: 21 | Training Loss: 0.14814404040575027 | Validation Loss: 0.1381027065217495\n",
      "Epoch no.: 22 | Training Loss: 0.14794760704040527 | Validation Loss: 0.13847143352031707\n",
      "Epoch no.: 23 | Training Loss: 0.14817615486681462 | Validation Loss: 0.13880573436617852\n",
      "Epoch no.: 24 | Training Loss: 0.14801765188574792 | Validation Loss: 0.13831187412142754\n",
      "Epoch no.: 25 | Training Loss: 0.147996811196208 | Validation Loss: 0.1381672888994217\n",
      "Epoch no.: 26 | Training Loss: 0.1479951460659504 | Validation Loss: 0.13964103460311889\n",
      "Epoch no.: 27 | Training Loss: 0.14796402849256993 | Validation Loss: 0.1391426518559456\n",
      "Epoch no.: 28 | Training Loss: 0.14806877940893173 | Validation Loss: 0.13844597861170768\n",
      "Epoch no.: 29 | Training Loss: 0.1478638757020235 | Validation Loss: 0.13823842480778695\n",
      "Epoch no.: 30 | Training Loss: 0.14806205056607724 | Validation Loss: 0.13845702409744262\n",
      "Epoch no.: 31 | Training Loss: 0.14804393731057644 | Validation Loss: 0.1381561763584614\n",
      "Epoch no.: 32 | Training Loss: 0.14788983307778836 | Validation Loss: 0.13812657296657563\n",
      "Epoch no.: 33 | Training Loss: 0.1483801981806755 | Validation Loss: 0.13850655257701874\n",
      "Epoch no.: 34 | Training Loss: 0.1480893575400114 | Validation Loss: 0.13805216327309608\n",
      "Epoch no.: 35 | Training Loss: 0.14807766251266002 | Validation Loss: 0.13821024373173713\n",
      "Epoch no.: 36 | Training Loss: 0.1478828126937151 | Validation Loss: 0.13915791884064674\n",
      "Epoch no.: 37 | Training Loss: 0.14839308999478817 | Validation Loss: 0.13876517340540886\n",
      "Epoch no.: 38 | Training Loss: 0.14798674754798413 | Validation Loss: 0.13807632476091386\n",
      "Epoch no.: 39 | Training Loss: 0.14794029019773006 | Validation Loss: 0.1392919935286045\n",
      "Epoch no.: 40 | Training Loss: 0.14823301695287228 | Validation Loss: 0.138633069396019\n",
      "Epoch no.: 41 | Training Loss: 0.14805909350514412 | Validation Loss: 0.138279240578413\n",
      "Epoch no.: 42 | Training Loss: 0.14781723149120807 | Validation Loss: 0.13876345306634902\n",
      "Epoch no.: 43 | Training Loss: 0.1481218398362398 | Validation Loss: 0.1386083707213402\n",
      "Epoch no.: 44 | Training Loss: 0.14794512622058392 | Validation Loss: 0.1393900714814663\n",
      "Epoch no.: 45 | Training Loss: 0.14783341199159622 | Validation Loss: 0.13889658972620963\n",
      "Epoch no.: 46 | Training Loss: 0.14793318405747413 | Validation Loss: 0.13895605280995368\n",
      "Epoch no.: 47 | Training Loss: 0.14811255916953087 | Validation Loss: 0.1385903388261795\n",
      "Epoch no.: 48 | Training Loss: 0.14798376850783826 | Validation Loss: 0.1380930721759796\n",
      "Epoch no.: 49 | Training Loss: 0.14795997828245164 | Validation Loss: 0.13812921792268754\n",
      "Epoch no.: 50 | Training Loss: 0.14789076693356037 | Validation Loss: 0.138056830316782\n",
      "Epoch no.: 51 | Training Loss: 0.14806589253246785 | Validation Loss: 0.13812603205442428\n",
      "Epoch no.: 52 | Training Loss: 0.14803723856806755 | Validation Loss: 0.1387112133204937\n",
      "Epoch no.: 53 | Training Loss: 0.1480658283084631 | Validation Loss: 0.1384544774889946\n",
      "Epoch no.: 54 | Training Loss: 0.14820029392838477 | Validation Loss: 0.13848159983754157\n",
      "Epoch no.: 55 | Training Loss: 0.14816299438476563 | Validation Loss: 0.13818417116999626\n",
      "Epoch no.: 56 | Training Loss: 0.1481730864197016 | Validation Loss: 0.13822143152356148\n",
      "Epoch no.: 57 | Training Loss: 0.1481124810874462 | Validation Loss: 0.1382593482732773\n",
      "Epoch no.: 58 | Training Loss: 0.14796674363315104 | Validation Loss: 0.13870142847299577\n",
      "Epoch no.: 59 | Training Loss: 0.1479028594493866 | Validation Loss: 0.13814907819032668\n",
      "Epoch no.: 60 | Training Loss: 0.14805588372051715 | Validation Loss: 0.13809093907475473\n",
      "Epoch no.: 61 | Training Loss: 0.14804586574435233 | Validation Loss: 0.13868014886975288\n",
      "Epoch no.: 62 | Training Loss: 0.147919914573431 | Validation Loss: 0.13817456513643264\n",
      "Epoch no.: 63 | Training Loss: 0.14800220258533955 | Validation Loss: 0.1380944885313511\n",
      "Epoch no.: 64 | Training Loss: 0.14789463967084884 | Validation Loss: 0.13936635106801987\n",
      "Epoch no.: 65 | Training Loss: 0.14821840107440948 | Validation Loss: 0.13828023970127107\n",
      "Epoch no.: 66 | Training Loss: 0.1479075051099062 | Validation Loss: 0.13803817257285117\n",
      "Epoch no.: 67 | Training Loss: 0.1480027724802494 | Validation Loss: 0.1380836248397827\n",
      "Epoch no.: 68 | Training Loss: 0.1479222572594881 | Validation Loss: 0.1382375441491604\n",
      "Epoch no.: 69 | Training Loss: 0.14786666259169579 | Validation Loss: 0.13863539546728135\n",
      "Epoch no.: 70 | Training Loss: 0.147935791015625 | Validation Loss: 0.13855496048927307\n",
      "Epoch no.: 71 | Training Loss: 0.14811471790075303 | Validation Loss: 0.13818076327443124\n",
      "Epoch no.: 72 | Training Loss: 0.14807466916739942 | Validation Loss: 0.13804014697670935\n",
      "Epoch no.: 73 | Training Loss: 0.1478707890957594 | Validation Loss: 0.13814467564225197\n",
      "Epoch no.: 74 | Training Loss: 0.14792305715382098 | Validation Loss: 0.13838921785354613\n",
      "Epoch no.: 75 | Training Loss: 0.14800628915429115 | Validation Loss: 0.13810643330216407\n",
      "Epoch no.: 76 | Training Loss: 0.14806935906410218 | Validation Loss: 0.13805265724658966\n",
      "Epoch no.: 77 | Training Loss: 0.1479511297494173 | Validation Loss: 0.13810963928699493\n",
      "Epoch no.: 78 | Training Loss: 0.1481553766131401 | Validation Loss: 0.13823964446783066\n",
      "Epoch no.: 79 | Training Loss: 0.14795943014323712 | Validation Loss: 0.13895246386528015\n",
      "Epoch no.: 80 | Training Loss: 0.1481321297585964 | Validation Loss: 0.13851505741477013\n",
      "Epoch no.: 81 | Training Loss: 0.1480752781778574 | Validation Loss: 0.1383800119161606\n",
      "Epoch no.: 82 | Training Loss: 0.1479828504472971 | Validation Loss: 0.13840383887290955\n",
      "Epoch no.: 83 | Training Loss: 0.14787093304097654 | Validation Loss: 0.14104334712028505\n",
      "Epoch no.: 84 | Training Loss: 0.14798231333494186 | Validation Loss: 0.1385091006755829\n",
      "Epoch no.: 85 | Training Loss: 0.1478402030467987 | Validation Loss: 0.13808562830090523\n",
      "Epoch no.: 86 | Training Loss: 0.148041722625494 | Validation Loss: 0.13815926164388656\n",
      "Epoch no.: 87 | Training Loss: 0.14809872947633265 | Validation Loss: 0.139061788469553\n",
      "Epoch no.: 88 | Training Loss: 0.14827069133520127 | Validation Loss: 0.13815966844558716\n",
      "Epoch no.: 89 | Training Loss: 0.1477967744320631 | Validation Loss: 0.13822524696588517\n",
      "Epoch no.: 90 | Training Loss: 0.1478577832132578 | Validation Loss: 0.13859958648681642\n",
      "Epoch no.: 91 | Training Loss: 0.14830877915024757 | Validation Loss: 0.1381894662976265\n",
      "Epoch no.: 92 | Training Loss: 0.14802598632872105 | Validation Loss: 0.13810513615608216\n",
      "Epoch no.: 93 | Training Loss: 0.14790641456842424 | Validation Loss: 0.13804857358336448\n",
      "Epoch no.: 94 | Training Loss: 0.1479305475205183 | Validation Loss: 0.1381827637553215\n",
      "Epoch no.: 95 | Training Loss: 0.14821336828172207 | Validation Loss: 0.1386920243501663\n",
      "Epoch no.: 96 | Training Loss: 0.14789431005716325 | Validation Loss: 0.1382206566631794\n",
      "Epoch no.: 97 | Training Loss: 0.14788967087864877 | Validation Loss: 0.13838600367307663\n",
      "Epoch no.: 98 | Training Loss: 0.14805045880377293 | Validation Loss: 0.13809634670615195\n",
      "Epoch no.: 99 | Training Loss: 0.14808950945734978 | Validation Loss: 0.13863475993275642\n",
      "Epoch no.: 100 | Training Loss: 0.1478716777265072 | Validation Loss: 0.13858550563454627\n",
      "Epoch no.: 101 | Training Loss: 0.148125933483243 | Validation Loss: 0.13818083330988884\n",
      "Epoch no.: 102 | Training Loss: 0.14813291803002357 | Validation Loss: 0.13897750154137611\n",
      "Epoch no.: 103 | Training Loss: 0.14788663402199745 | Validation Loss: 0.13800931796431543\n",
      "Epoch no.: 104 | Training Loss: 0.14775961756706238 | Validation Loss: 0.13912162259221078\n",
      "Epoch no.: 105 | Training Loss: 0.14801975078880786 | Validation Loss: 0.1380452647805214\n",
      "Epoch no.: 106 | Training Loss: 0.14795235350728034 | Validation Loss: 0.13896023482084274\n",
      "Epoch no.: 107 | Training Loss: 0.14801460176706313 | Validation Loss: 0.1381100408732891\n",
      "Epoch no.: 108 | Training Loss: 0.1481566032022238 | Validation Loss: 0.13870775029063226\n",
      "Epoch no.: 109 | Training Loss: 0.14795234628021717 | Validation Loss: 0.13945075497031212\n",
      "Epoch no.: 110 | Training Loss: 0.14800810344517232 | Validation Loss: 0.1391913928091526\n",
      "Epoch no.: 111 | Training Loss: 0.14797746054828168 | Validation Loss: 0.13836919218301774\n",
      "Epoch no.: 112 | Training Loss: 0.14787317372858524 | Validation Loss: 0.13810396268963815\n",
      "Epoch no.: 113 | Training Loss: 0.14808741487562657 | Validation Loss: 0.13841482177376746\n",
      "Epoch no.: 114 | Training Loss: 0.14797706492245197 | Validation Loss: 0.13871913850307466\n",
      "Epoch no.: 115 | Training Loss: 0.14798317648470402 | Validation Loss: 0.1380942679941654\n",
      "Epoch no.: 116 | Training Loss: 0.14798217073082923 | Validation Loss: 0.13833030313253403\n",
      "Epoch no.: 117 | Training Loss: 0.1481192809343338 | Validation Loss: 0.13804989233613013\n",
      "Epoch no.: 118 | Training Loss: 0.14804997391998767 | Validation Loss: 0.13824368491768838\n",
      "Epoch no.: 119 | Training Loss: 0.14792341992259025 | Validation Loss: 0.13846348598599434\n",
      "Epoch no.: 120 | Training Loss: 0.14803623288869858 | Validation Loss: 0.13949977681040765\n",
      "Epoch no.: 121 | Training Loss: 0.1481257527321577 | Validation Loss: 0.13856046944856643\n",
      "Epoch no.: 122 | Training Loss: 0.14804017215967177 | Validation Loss: 0.13802815303206445\n",
      "Epoch no.: 123 | Training Loss: 0.14816568203270436 | Validation Loss: 0.1382730208337307\n",
      "Epoch no.: 124 | Training Loss: 0.14785554476082324 | Validation Loss: 0.13828296586871147\n",
      "Epoch no.: 125 | Training Loss: 0.1480072010308504 | Validation Loss: 0.13906847164034844\n",
      "Epoch no.: 126 | Training Loss: 0.1480195600539446 | Validation Loss: 0.13871160000562668\n",
      "Epoch no.: 127 | Training Loss: 0.14801662683486938 | Validation Loss: 0.13802726194262505\n",
      "Epoch no.: 128 | Training Loss: 0.14791146486997606 | Validation Loss: 0.13808402344584464\n",
      "Epoch no.: 129 | Training Loss: 0.14792411647737025 | Validation Loss: 0.13912168145179749\n",
      "Epoch no.: 130 | Training Loss: 0.14791696973145008 | Validation Loss: 0.13947929963469505\n",
      "Epoch no.: 131 | Training Loss: 0.14786021128296853 | Validation Loss: 0.1390820063650608\n",
      "Epoch no.: 132 | Training Loss: 0.14787958234548568 | Validation Loss: 0.1390046164393425\n",
      "Epoch no.: 133 | Training Loss: 0.14793812617659569 | Validation Loss: 0.13802068531513215\n",
      "Epoch no.: 134 | Training Loss: 0.14794118024408817 | Validation Loss: 0.13800354674458504\n",
      "Epoch no.: 135 | Training Loss: 0.1477766005694866 | Validation Loss: 0.13812209963798522\n",
      "Epoch no.: 136 | Training Loss: 0.1478713108599186 | Validation Loss: 0.1393997073173523\n",
      "Epoch no.: 137 | Training Loss: 0.14792573928833008 | Validation Loss: 0.13802162632346154\n",
      "Epoch no.: 138 | Training Loss: 0.1479176301509142 | Validation Loss: 0.13817875385284423\n",
      "Epoch no.: 139 | Training Loss: 0.14786361515522004 | Validation Loss: 0.1382739908993244\n",
      "Epoch no.: 140 | Training Loss: 0.14777735970914363 | Validation Loss: 0.1381655290722847\n",
      "Epoch no.: 141 | Training Loss: 0.14792580619454385 | Validation Loss: 0.13802661523222923\n",
      "Epoch no.: 142 | Training Loss: 0.14779301770031453 | Validation Loss: 0.13816271722316742\n",
      "Epoch no.: 143 | Training Loss: 0.14787749260663985 | Validation Loss: 0.1382691890001297\n",
      "Epoch no.: 144 | Training Loss: 0.14801929712295533 | Validation Loss: 0.13830660954117774\n",
      "Epoch no.: 145 | Training Loss: 0.14809587202966212 | Validation Loss: 0.13831880912184716\n",
      "Epoch no.: 146 | Training Loss: 0.14819277442991732 | Validation Loss: 0.13839786723256112\n",
      "Epoch no.: 147 | Training Loss: 0.14792469382286072 | Validation Loss: 0.1385701760649681\n",
      "Epoch no.: 148 | Training Loss: 0.14790405124425887 | Validation Loss: 0.1384672626852989\n",
      "Epoch no.: 149 | Training Loss: 0.14785450242459774 | Validation Loss: 0.1382218509912491\n",
      "Epoch no.: 150 | Training Loss: 0.1479966702312231 | Validation Loss: 0.13994478881359101\n",
      "Epoch no.: 151 | Training Loss: 0.14785076841711997 | Validation Loss: 0.13800739049911498\n",
      "Epoch no.: 152 | Training Loss: 0.1478137119859457 | Validation Loss: 0.13845400735735894\n",
      "Epoch no.: 153 | Training Loss: 0.1478712933510542 | Validation Loss: 0.13805028945207595\n",
      "Epoch no.: 154 | Training Loss: 0.1478868180513382 | Validation Loss: 0.13804188147187232\n",
      "Epoch no.: 155 | Training Loss: 0.14810796231031417 | Validation Loss: 0.13848128095269202\n",
      "Epoch no.: 156 | Training Loss: 0.14783389389514923 | Validation Loss: 0.1382978819310665\n",
      "Epoch no.: 157 | Training Loss: 0.14796951159834862 | Validation Loss: 0.13937730565667153\n",
      "Epoch no.: 158 | Training Loss: 0.14800454989075662 | Validation Loss: 0.13864486142992974\n",
      "Epoch no.: 159 | Training Loss: 0.1479510872066021 | Validation Loss: 0.13818061575293542\n",
      "Epoch no.: 160 | Training Loss: 0.14790433682501317 | Validation Loss: 0.13837179988622667\n",
      "Epoch no.: 161 | Training Loss: 0.1478409832715988 | Validation Loss: 0.13808260560035707\n",
      "Epoch no.: 162 | Training Loss: 0.14781043022871018 | Validation Loss: 0.138104347884655\n",
      "Epoch no.: 163 | Training Loss: 0.14796139642596245 | Validation Loss: 0.13816074877977372\n",
      "Epoch no.: 164 | Training Loss: 0.14791425928473473 | Validation Loss: 0.13799180388450621\n",
      "Epoch no.: 165 | Training Loss: 0.1477866567671299 | Validation Loss: 0.13816269338130951\n",
      "Epoch no.: 166 | Training Loss: 0.1479526960104704 | Validation Loss: 0.13822807371616364\n",
      "Epoch no.: 167 | Training Loss: 0.14792289167642594 | Validation Loss: 0.138067127764225\n",
      "Epoch no.: 168 | Training Loss: 0.147691495642066 | Validation Loss: 0.1382107727229595\n",
      "Epoch no.: 169 | Training Loss: 0.14807286337018014 | Validation Loss: 0.13812990859150887\n",
      "Epoch no.: 170 | Training Loss: 0.14781757861375808 | Validation Loss: 0.13813812285661697\n",
      "Epoch no.: 171 | Training Loss: 0.1477704395353794 | Validation Loss: 0.1380031354725361\n",
      "Epoch no.: 172 | Training Loss: 0.14794365219771863 | Validation Loss: 0.13802943453192712\n",
      "Epoch no.: 173 | Training Loss: 0.14799983061850072 | Validation Loss: 0.13829009756445884\n",
      "Epoch no.: 174 | Training Loss: 0.14824406050145625 | Validation Loss: 0.13919894397258759\n",
      "Epoch no.: 175 | Training Loss: 0.1480701855570078 | Validation Loss: 0.13804449439048766\n",
      "Epoch no.: 176 | Training Loss: 0.14781153291463853 | Validation Loss: 0.13827713951468468\n",
      "Epoch no.: 177 | Training Loss: 0.1479292396456003 | Validation Loss: 0.13915398418903352\n",
      "Epoch no.: 178 | Training Loss: 0.14787031903862954 | Validation Loss: 0.1384095512330532\n",
      "Epoch no.: 179 | Training Loss: 0.1479493337124586 | Validation Loss: 0.13836122006177903\n",
      "Epoch no.: 180 | Training Loss: 0.1479404067248106 | Validation Loss: 0.1380035810172558\n",
      "Epoch no.: 181 | Training Loss: 0.14778012335300444 | Validation Loss: 0.13801679611206055\n",
      "Epoch no.: 182 | Training Loss: 0.14802058167755605 | Validation Loss: 0.13834077417850493\n",
      "Epoch no.: 183 | Training Loss: 0.14792855367064475 | Validation Loss: 0.1385419636964798\n",
      "Epoch no.: 184 | Training Loss: 0.1478303474187851 | Validation Loss: 0.13808697536587716\n",
      "Epoch no.: 185 | Training Loss: 0.148017498254776 | Validation Loss: 0.1383080631494522\n",
      "Epoch no.: 186 | Training Loss: 0.1479704263061285 | Validation Loss: 0.13837817907333375\n",
      "Epoch no.: 187 | Training Loss: 0.14789361700415612 | Validation Loss: 0.13845880329608917\n",
      "Epoch no.: 188 | Training Loss: 0.14803115278482437 | Validation Loss: 0.138315898925066\n",
      "Epoch no.: 189 | Training Loss: 0.14799107424914837 | Validation Loss: 0.13799410313367844\n",
      "Epoch no.: 190 | Training Loss: 0.1480283685028553 | Validation Loss: 0.13881253451108932\n",
      "Epoch no.: 191 | Training Loss: 0.14782975904643536 | Validation Loss: 0.13801566436886786\n",
      "Epoch no.: 192 | Training Loss: 0.14801462821662426 | Validation Loss: 0.13819765150547028\n",
      "Epoch no.: 193 | Training Loss: 0.14779881723225116 | Validation Loss: 0.13870727121829987\n",
      "Epoch no.: 194 | Training Loss: 0.14788458295166493 | Validation Loss: 0.13821117728948593\n",
      "Epoch no.: 195 | Training Loss: 0.14776573352515698 | Validation Loss: 0.13800909966230393\n",
      "Epoch no.: 196 | Training Loss: 0.14811283156275748 | Validation Loss: 0.13947983011603354\n",
      "Epoch no.: 197 | Training Loss: 0.1480523620545864 | Validation Loss: 0.1382055677473545\n",
      "Epoch no.: 198 | Training Loss: 0.1479593154042959 | Validation Loss: 0.13803459480404853\n",
      "Epoch no.: 199 | Training Loss: 0.14785121247172356 | Validation Loss: 0.1382809706032276\n",
      "Epoch no.: 200 | Training Loss: 0.14777495071291924 | Validation Loss: 0.13800541087985038\n",
      "Epoch no.: 201 | Training Loss: 0.14796227410435678 | Validation Loss: 0.13899496644735337\n",
      "Epoch no.: 202 | Training Loss: 0.14792970463633537 | Validation Loss: 0.13799905478954316\n",
      "Epoch no.: 203 | Training Loss: 0.1480034451186657 | Validation Loss: 0.13813147842884063\n",
      "Epoch no.: 204 | Training Loss: 0.14778632931411267 | Validation Loss: 0.13868206664919852\n",
      "Epoch no.: 205 | Training Loss: 0.14812807619571686 | Validation Loss: 0.13807236179709434\n",
      "Epoch no.: 206 | Training Loss: 0.14791421316564082 | Validation Loss: 0.13817240968346595\n",
      "Epoch no.: 207 | Training Loss: 0.14791702039539814 | Validation Loss: 0.13852365911006928\n",
      "Epoch no.: 208 | Training Loss: 0.14794799782335757 | Validation Loss: 0.13842556700110437\n",
      "Epoch no.: 209 | Training Loss: 0.1477959331870079 | Validation Loss: 0.13801618292927742\n",
      "Epoch no.: 210 | Training Loss: 0.1477463484555483 | Validation Loss: 0.1390976645052433\n",
      "Epoch no.: 211 | Training Loss: 0.14779630295932292 | Validation Loss: 0.1380997531116009\n",
      "Epoch no.: 212 | Training Loss: 0.1479387702047825 | Validation Loss: 0.13819308057427407\n",
      "Epoch no.: 213 | Training Loss: 0.14795952923595906 | Validation Loss: 0.1380114108324051\n",
      "Epoch no.: 214 | Training Loss: 0.14780212961137296 | Validation Loss: 0.13862724527716636\n",
      "Epoch no.: 215 | Training Loss: 0.1478785727918148 | Validation Loss: 0.13802075907588005\n",
      "Epoch no.: 216 | Training Loss: 0.1477562689781189 | Validation Loss: 0.13812510967254638\n",
      "Epoch no.: 217 | Training Loss: 0.1480293145030737 | Validation Loss: 0.1380125842988491\n",
      "Epoch no.: 218 | Training Loss: 0.1477646490931511 | Validation Loss: 0.138017375767231\n",
      "Epoch no.: 219 | Training Loss: 0.14810390666127204 | Validation Loss: 0.138675969094038\n",
      "Epoch no.: 220 | Training Loss: 0.14787408478558065 | Validation Loss: 0.1387725591659546\n",
      "Epoch no.: 221 | Training Loss: 0.14783297806978227 | Validation Loss: 0.13824500739574433\n",
      "Epoch no.: 222 | Training Loss: 0.14787508934736251 | Validation Loss: 0.13800855055451394\n",
      "Epoch no.: 223 | Training Loss: 0.14806181102991103 | Validation Loss: 0.1395733319222927\n",
      "Epoch no.: 224 | Training Loss: 0.1478794138133526 | Validation Loss: 0.13875126466155052\n",
      "Epoch no.: 225 | Training Loss: 0.14801552437245846 | Validation Loss: 0.13854162767529488\n",
      "Epoch no.: 226 | Training Loss: 0.14802505500614643 | Validation Loss: 0.13808783292770385\n",
      "Epoch no.: 227 | Training Loss: 0.14785921514034273 | Validation Loss: 0.13799506276845933\n",
      "Epoch no.: 228 | Training Loss: 0.14790203459560872 | Validation Loss: 0.1381630778312683\n",
      "Epoch no.: 229 | Training Loss: 0.14795760869979857 | Validation Loss: 0.13809148892760276\n",
      "Epoch no.: 230 | Training Loss: 0.14814016051590442 | Validation Loss: 0.1380230665206909\n",
      "Epoch no.: 231 | Training Loss: 0.1477860599011183 | Validation Loss: 0.1380701757967472\n",
      "Epoch no.: 232 | Training Loss: 0.14784042060375213 | Validation Loss: 0.1382691226899624\n",
      "Epoch no.: 233 | Training Loss: 0.14790653184056282 | Validation Loss: 0.1379949264228344\n",
      "Epoch no.: 234 | Training Loss: 0.1480005320161581 | Validation Loss: 0.13803466334939002\n",
      "Epoch no.: 235 | Training Loss: 0.1478029827028513 | Validation Loss: 0.13798811584711074\n",
      "Epoch no.: 236 | Training Loss: 0.147841582223773 | Validation Loss: 0.13866006955504417\n",
      "Epoch no.: 237 | Training Loss: 0.14784173682332039 | Validation Loss: 0.13805255219340323\n",
      "Epoch no.: 238 | Training Loss: 0.1477306839823723 | Validation Loss: 0.13801044672727586\n",
      "Epoch no.: 239 | Training Loss: 0.14786198757588864 | Validation Loss: 0.13829811662435532\n",
      "Epoch no.: 240 | Training Loss: 0.14787328891456128 | Validation Loss: 0.13816343545913695\n",
      "Epoch no.: 241 | Training Loss: 0.14785352863371373 | Validation Loss: 0.13871334940195085\n",
      "Epoch no.: 242 | Training Loss: 0.14787292554974557 | Validation Loss: 0.13805024549365044\n",
      "Epoch no.: 243 | Training Loss: 0.14815507113933563 | Validation Loss: 0.13809880316257478\n",
      "Epoch no.: 244 | Training Loss: 0.14777762010693551 | Validation Loss: 0.1380717433989048\n",
      "Epoch no.: 245 | Training Loss: 0.1478647068142891 | Validation Loss: 0.13855901584029198\n",
      "Epoch no.: 246 | Training Loss: 0.14774432547390462 | Validation Loss: 0.13818562105298043\n",
      "Epoch no.: 247 | Training Loss: 0.14777900740504266 | Validation Loss: 0.13804158866405486\n",
      "Epoch no.: 248 | Training Loss: 0.14788762882351875 | Validation Loss: 0.13811203241348266\n",
      "Epoch no.: 249 | Training Loss: 0.14792790777981282 | Validation Loss: 0.1380305513739586\n",
      "Epoch no.: 250 | Training Loss: 0.14793297477066517 | Validation Loss: 0.13808194026350976\n",
      "Epoch no.: 251 | Training Loss: 0.14775325499475003 | Validation Loss: 0.13841417953372\n",
      "Epoch no.: 252 | Training Loss: 0.14788947254419327 | Validation Loss: 0.13804220110177995\n",
      "Epoch no.: 253 | Training Loss: 0.14818378545343877 | Validation Loss: 0.13810917288064956\n",
      "Epoch no.: 254 | Training Loss: 0.14784119218587877 | Validation Loss: 0.13803438991308212\n",
      "Epoch no.: 255 | Training Loss: 0.14787402413785458 | Validation Loss: 0.1382901966571808\n",
      "Epoch no.: 256 | Training Loss: 0.1479920057207346 | Validation Loss: 0.13873150646686555\n",
      "Epoch no.: 257 | Training Loss: 0.14780308969318867 | Validation Loss: 0.1382470555603504\n",
      "Epoch no.: 258 | Training Loss: 0.14804051503539084 | Validation Loss: 0.13807916194200515\n",
      "Epoch no.: 259 | Training Loss: 0.14796469561755657 | Validation Loss: 0.13827420696616172\n",
      "Epoch no.: 260 | Training Loss: 0.1479111362248659 | Validation Loss: 0.13811767622828483\n",
      "Epoch no.: 261 | Training Loss: 0.14782680600881576 | Validation Loss: 0.13811347186565398\n",
      "Epoch no.: 262 | Training Loss: 0.14773712620139123 | Validation Loss: 0.13800252750515937\n",
      "Epoch no.: 263 | Training Loss: 0.1478737312555313 | Validation Loss: 0.13798520416021348\n",
      "Epoch no.: 264 | Training Loss: 0.14797249980270863 | Validation Loss: 0.13810376226902008\n",
      "Epoch no.: 265 | Training Loss: 0.14791064478456975 | Validation Loss: 0.13799311593174934\n",
      "Epoch no.: 266 | Training Loss: 0.14791111558675765 | Validation Loss: 0.13817370906472207\n",
      "Epoch no.: 267 | Training Loss: 0.14780431307852268 | Validation Loss: 0.13818740248680114\n",
      "Epoch no.: 268 | Training Loss: 0.14783078655600548 | Validation Loss: 0.13991000801324843\n",
      "Epoch no.: 269 | Training Loss: 0.14837093703448773 | Validation Loss: 0.1382995903491974\n",
      "Epoch no.: 270 | Training Loss: 0.14796953178942204 | Validation Loss: 0.13811868205666541\n",
      "Epoch no.: 271 | Training Loss: 0.14795275166630745 | Validation Loss: 0.13807516172528267\n",
      "Epoch no.: 272 | Training Loss: 0.14789220526814462 | Validation Loss: 0.13801616281270981\n",
      "Epoch no.: 273 | Training Loss: 0.14801384828984737 | Validation Loss: 0.1380186289548874\n",
      "Epoch no.: 274 | Training Loss: 0.1478163806349039 | Validation Loss: 0.1379778653383255\n",
      "Epoch no.: 275 | Training Loss: 0.147979798540473 | Validation Loss: 0.1387622207403183\n",
      "Epoch no.: 276 | Training Loss: 0.1478086929023266 | Validation Loss: 0.13802378326654435\n",
      "Epoch no.: 277 | Training Loss: 0.14786878779530524 | Validation Loss: 0.1381002575159073\n",
      "Epoch no.: 278 | Training Loss: 0.1477722977846861 | Validation Loss: 0.13809118568897247\n",
      "Epoch no.: 279 | Training Loss: 0.14780225850641726 | Validation Loss: 0.13831361010670662\n",
      "Epoch no.: 280 | Training Loss: 0.14797141931951047 | Validation Loss: 0.13812554255127907\n",
      "Epoch no.: 281 | Training Loss: 0.14781673826277256 | Validation Loss: 0.1384969510138035\n",
      "Epoch no.: 282 | Training Loss: 0.1479591089487076 | Validation Loss: 0.13842891231179238\n",
      "Epoch no.: 283 | Training Loss: 0.147900717779994 | Validation Loss: 0.13825441151857376\n",
      "Epoch no.: 284 | Training Loss: 0.1479935558140278 | Validation Loss: 0.14052105322480202\n",
      "Epoch no.: 285 | Training Loss: 0.14803110510110856 | Validation Loss: 0.1382999114692211\n",
      "Epoch no.: 286 | Training Loss: 0.14783974662423133 | Validation Loss: 0.13897404819726944\n",
      "Epoch no.: 287 | Training Loss: 0.1478394263982773 | Validation Loss: 0.13853562250733376\n",
      "Epoch no.: 288 | Training Loss: 0.14777407057583333 | Validation Loss: 0.13848940804600715\n",
      "Epoch no.: 289 | Training Loss: 0.14785704255104065 | Validation Loss: 0.13911991864442824\n",
      "Epoch no.: 290 | Training Loss: 0.1479189360141754 | Validation Loss: 0.1382011093199253\n",
      "Epoch no.: 291 | Training Loss: 0.14806452691555022 | Validation Loss: 0.1380262330174446\n",
      "Epoch no.: 292 | Training Loss: 0.147888008877635 | Validation Loss: 0.1379779189825058\n",
      "Epoch no.: 293 | Training Loss: 0.14792226895689964 | Validation Loss: 0.13852810859680176\n",
      "Epoch no.: 294 | Training Loss: 0.14785167813301087 | Validation Loss: 0.13883821666240692\n",
      "Epoch no.: 295 | Training Loss: 0.14784810848534108 | Validation Loss: 0.1381819598376751\n",
      "Epoch no.: 296 | Training Loss: 0.1477147813141346 | Validation Loss: 0.13827794194221496\n",
      "Epoch no.: 297 | Training Loss: 0.14786951288580893 | Validation Loss: 0.13920221328735352\n",
      "Epoch no.: 298 | Training Loss: 0.14798932172358037 | Validation Loss: 0.13796135410666466\n",
      "Epoch no.: 299 | Training Loss: 0.14783018067479134 | Validation Loss: 0.138694866001606\n",
      "Epoch no.: 300 | Training Loss: 0.14795871078968048 | Validation Loss: 0.13809100165963173\n",
      "Epoch no.: 301 | Training Loss: 0.1479929106682539 | Validation Loss: 0.1383571520447731\n",
      "Epoch no.: 302 | Training Loss: 0.14787498533725738 | Validation Loss: 0.13900236263871193\n",
      "Epoch no.: 303 | Training Loss: 0.1477858654409647 | Validation Loss: 0.1380327485501766\n",
      "Epoch no.: 304 | Training Loss: 0.14787680938839912 | Validation Loss: 0.13797961920499802\n",
      "Epoch no.: 305 | Training Loss: 0.14775688730180264 | Validation Loss: 0.13822850510478019\n",
      "Epoch no.: 306 | Training Loss: 0.14780213303864 | Validation Loss: 0.13800522089004516\n",
      "Epoch no.: 307 | Training Loss: 0.14778328008949757 | Validation Loss: 0.13952661231160163\n",
      "Epoch no.: 308 | Training Loss: 0.14796121016144753 | Validation Loss: 0.13822562247514725\n",
      "Epoch no.: 309 | Training Loss: 0.1479363325238228 | Validation Loss: 0.1380868747830391\n",
      "Epoch no.: 310 | Training Loss: 0.147978515625 | Validation Loss: 0.13806767016649246\n",
      "Epoch no.: 311 | Training Loss: 0.1478272257745266 | Validation Loss: 0.13914776593446732\n",
      "Epoch no.: 312 | Training Loss: 0.14789085693657397 | Validation Loss: 0.1383676901459694\n",
      "Epoch no.: 313 | Training Loss: 0.1477308339625597 | Validation Loss: 0.13820157572627068\n",
      "Epoch no.: 314 | Training Loss: 0.1477971276640892 | Validation Loss: 0.13805839270353318\n",
      "Epoch no.: 315 | Training Loss: 0.1479013479501009 | Validation Loss: 0.1380785159766674\n",
      "Epoch no.: 316 | Training Loss: 0.14779358640313148 | Validation Loss: 0.1380459673702717\n",
      "Epoch no.: 317 | Training Loss: 0.147830560952425 | Validation Loss: 0.13796201050281526\n",
      "Epoch no.: 318 | Training Loss: 0.14788423240184784 | Validation Loss: 0.13799364417791365\n",
      "Epoch no.: 319 | Training Loss: 0.1478771023452282 | Validation Loss: 0.13797371983528137\n",
      "Epoch no.: 320 | Training Loss: 0.1479553747177124 | Validation Loss: 0.13842924013733865\n",
      "Epoch no.: 321 | Training Loss: 0.1478716267645359 | Validation Loss: 0.13826558217406273\n",
      "Epoch no.: 322 | Training Loss: 0.14782926984131337 | Validation Loss: 0.1380492217838764\n",
      "Epoch no.: 323 | Training Loss: 0.14787155203521252 | Validation Loss: 0.13797752633690835\n",
      "Epoch no.: 324 | Training Loss: 0.14769733287394046 | Validation Loss: 0.13811530247330667\n",
      "Epoch no.: 325 | Training Loss: 0.14784416057169436 | Validation Loss: 0.13807265907526017\n",
      "Epoch no.: 326 | Training Loss: 0.14793974466621876 | Validation Loss: 0.13876309394836425\n",
      "Epoch no.: 327 | Training Loss: 0.14802507527172565 | Validation Loss: 0.13848682194948198\n",
      "Epoch no.: 328 | Training Loss: 0.1477650797367096 | Validation Loss: 0.13855312019586563\n",
      "Epoch no.: 329 | Training Loss: 0.14793408215045928 | Validation Loss: 0.1383148804306984\n",
      "Epoch no.: 330 | Training Loss: 0.1477799315750599 | Validation Loss: 0.13821007907390595\n",
      "Epoch no.: 331 | Training Loss: 0.14788296803832054 | Validation Loss: 0.13797641098499297\n",
      "Epoch no.: 332 | Training Loss: 0.14804155595600604 | Validation Loss: 0.13830069601535797\n",
      "Epoch no.: 333 | Training Loss: 0.14808530762791633 | Validation Loss: 0.13825586587190627\n",
      "Epoch no.: 334 | Training Loss: 0.14806479506194592 | Validation Loss: 0.13815083727240562\n",
      "Epoch no.: 335 | Training Loss: 0.14781401433050634 | Validation Loss: 0.13825006633996964\n",
      "Epoch no.: 336 | Training Loss: 0.14816927544772626 | Validation Loss: 0.13868352249264718\n",
      "Epoch no.: 337 | Training Loss: 0.14789686135947705 | Validation Loss: 0.13822493627667426\n",
      "Epoch no.: 338 | Training Loss: 0.14792779974639417 | Validation Loss: 0.13833242431282997\n",
      "Epoch no.: 339 | Training Loss: 0.14773554153740406 | Validation Loss: 0.13826706036925315\n",
      "Epoch no.: 340 | Training Loss: 0.14808702282607555 | Validation Loss: 0.13930648639798165\n",
      "Epoch no.: 341 | Training Loss: 0.1477632909268141 | Validation Loss: 0.13865528777241706\n",
      "Epoch no.: 342 | Training Loss: 0.14803394891321658 | Validation Loss: 0.1391328401863575\n",
      "Epoch no.: 343 | Training Loss: 0.1477656849473715 | Validation Loss: 0.13824363499879838\n",
      "Epoch no.: 344 | Training Loss: 0.14776840209960937 | Validation Loss: 0.13812175020575523\n",
      "Epoch no.: 345 | Training Loss: 0.14779338464140893 | Validation Loss: 0.13830109760165216\n",
      "Epoch no.: 346 | Training Loss: 0.1478921899944544 | Validation Loss: 0.1381080061197281\n",
      "Epoch no.: 347 | Training Loss: 0.14786464311182498 | Validation Loss: 0.13824024572968482\n",
      "Epoch no.: 348 | Training Loss: 0.14778681933879853 | Validation Loss: 0.13803842440247535\n",
      "Epoch no.: 349 | Training Loss: 0.1476964647322893 | Validation Loss: 0.13888608962297438\n",
      "Epoch no.: 350 | Training Loss: 0.14786502979695798 | Validation Loss: 0.1381629891693592\n",
      "Epoch no.: 351 | Training Loss: 0.1480162774771452 | Validation Loss: 0.13801775202155114\n",
      "Epoch no.: 352 | Training Loss: 0.14808461919426918 | Validation Loss: 0.13851002231240273\n",
      "Epoch no.: 353 | Training Loss: 0.1480165685713291 | Validation Loss: 0.1388602413237095\n",
      "Epoch no.: 354 | Training Loss: 0.14800362028181552 | Validation Loss: 0.13798595294356347\n",
      "Epoch no.: 355 | Training Loss: 0.1478835242986679 | Validation Loss: 0.13820996806025504\n",
      "Epoch no.: 356 | Training Loss: 0.1478767527639866 | Validation Loss: 0.13897507712244989\n",
      "Epoch no.: 357 | Training Loss: 0.14816050358116628 | Validation Loss: 0.1383233241736889\n",
      "Epoch no.: 358 | Training Loss: 0.1478729198873043 | Validation Loss: 0.13798108473420143\n",
      "Epoch no.: 359 | Training Loss: 0.1477265986800194 | Validation Loss: 0.13811026215553285\n",
      "Epoch no.: 360 | Training Loss: 0.1478198503702879 | Validation Loss: 0.1393435664474964\n",
      "Epoch no.: 361 | Training Loss: 0.14791089497506618 | Validation Loss: 0.1380983129143715\n",
      "Epoch no.: 362 | Training Loss: 0.1476863556355238 | Validation Loss: 0.13804941922426223\n",
      "Epoch no.: 363 | Training Loss: 0.14790176272392272 | Validation Loss: 0.13966112583875656\n",
      "Epoch no.: 364 | Training Loss: 0.14800127238035202 | Validation Loss: 0.1381470590829849\n",
      "Epoch no.: 365 | Training Loss: 0.14778719007968902 | Validation Loss: 0.1381292425096035\n",
      "Epoch no.: 366 | Training Loss: 0.14784808099269867 | Validation Loss: 0.13798038884997368\n",
      "Epoch no.: 367 | Training Loss: 0.1479099114239216 | Validation Loss: 0.13803577050566673\n",
      "Epoch no.: 368 | Training Loss: 0.14790673315525055 | Validation Loss: 0.1382353238761425\n",
      "Epoch no.: 369 | Training Loss: 0.1478970693796873 | Validation Loss: 0.1383824311196804\n",
      "Epoch no.: 370 | Training Loss: 0.14789127819240094 | Validation Loss: 0.13816707879304885\n",
      "Epoch no.: 371 | Training Loss: 0.14779820688068868 | Validation Loss: 0.1379876635968685\n",
      "Epoch no.: 372 | Training Loss: 0.1478582037240267 | Validation Loss: 0.13820184245705605\n",
      "Epoch no.: 373 | Training Loss: 0.147845828384161 | Validation Loss: 0.1379846841096878\n",
      "Epoch no.: 374 | Training Loss: 0.147881014123559 | Validation Loss: 0.13812474757432938\n",
      "Epoch no.: 375 | Training Loss: 0.14783894494175912 | Validation Loss: 0.13800201416015626\n",
      "Epoch no.: 376 | Training Loss: 0.14774271823465823 | Validation Loss: 0.13817972540855408\n",
      "Epoch no.: 377 | Training Loss: 0.1480320966243744 | Validation Loss: 0.13833127692341804\n",
      "Epoch no.: 378 | Training Loss: 0.14771830558776855 | Validation Loss: 0.13906577602028847\n",
      "Epoch no.: 379 | Training Loss: 0.1480245766043663 | Validation Loss: 0.13802537843585014\n",
      "Epoch no.: 380 | Training Loss: 0.14788002774119377 | Validation Loss: 0.138007303327322\n",
      "Epoch no.: 381 | Training Loss: 0.14780215345323086 | Validation Loss: 0.13798052072525024\n",
      "Epoch no.: 382 | Training Loss: 0.14789793729782105 | Validation Loss: 0.13909839391708373\n",
      "Epoch no.: 383 | Training Loss: 0.1477280129492283 | Validation Loss: 0.13806686028838158\n",
      "Epoch no.: 384 | Training Loss: 0.14783519588410854 | Validation Loss: 0.13823001459240913\n",
      "Epoch no.: 385 | Training Loss: 0.1478050258755684 | Validation Loss: 0.13836863189935683\n",
      "Epoch no.: 386 | Training Loss: 0.14785982124507427 | Validation Loss: 0.13804633542895317\n",
      "Epoch no.: 387 | Training Loss: 0.14789558500051497 | Validation Loss: 0.13807388693094252\n",
      "Epoch no.: 388 | Training Loss: 0.14776920445263386 | Validation Loss: 0.13839001432061196\n",
      "Epoch no.: 389 | Training Loss: 0.14782247073948385 | Validation Loss: 0.1380160465836525\n",
      "Epoch no.: 390 | Training Loss: 0.1478617772459984 | Validation Loss: 0.1380988612771034\n",
      "Epoch no.: 391 | Training Loss: 0.1479282171279192 | Validation Loss: 0.13868973776698112\n",
      "Epoch no.: 392 | Training Loss: 0.1478206545114517 | Validation Loss: 0.13805462270975113\n",
      "Epoch no.: 393 | Training Loss: 0.14786861307919025 | Validation Loss: 0.13816331326961517\n",
      "Epoch no.: 394 | Training Loss: 0.1477901842445135 | Validation Loss: 0.1379821553826332\n",
      "Epoch no.: 395 | Training Loss: 0.14792846962809564 | Validation Loss: 0.13837981447577477\n",
      "Epoch no.: 396 | Training Loss: 0.14773797906935215 | Validation Loss: 0.13816227093338967\n",
      "Epoch no.: 397 | Training Loss: 0.14795783579349517 | Validation Loss: 0.13805189058184625\n",
      "Epoch no.: 398 | Training Loss: 0.14791189312934874 | Validation Loss: 0.1389880135655403\n",
      "Epoch no.: 399 | Training Loss: 0.14782208397984506 | Validation Loss: 0.13881690427660942\n",
      "Epoch no.: 400 | Training Loss: 0.1478067810833454 | Validation Loss: 0.13807944357395172\n",
      "Epoch no.: 401 | Training Loss: 0.1478413225710392 | Validation Loss: 0.1384093590080738\n",
      "Epoch no.: 402 | Training Loss: 0.14769743762910367 | Validation Loss: 0.13821456879377364\n",
      "Epoch no.: 403 | Training Loss: 0.1478931800276041 | Validation Loss: 0.1379903644323349\n",
      "Epoch no.: 404 | Training Loss: 0.1481202831864357 | Validation Loss: 0.13798245638608933\n",
      "Epoch no.: 405 | Training Loss: 0.1476880482584238 | Validation Loss: 0.13832749500870706\n",
      "Epoch no.: 406 | Training Loss: 0.1478611245751381 | Validation Loss: 0.13849040493369102\n",
      "Epoch no.: 407 | Training Loss: 0.14793320141732694 | Validation Loss: 0.13795569092035292\n",
      "Epoch no.: 408 | Training Loss: 0.14789161950349808 | Validation Loss: 0.1379656933248043\n",
      "Epoch no.: 409 | Training Loss: 0.14792621918022633 | Validation Loss: 0.1379889130592346\n",
      "Epoch no.: 410 | Training Loss: 0.14788816943764688 | Validation Loss: 0.1379726767539978\n",
      "Epoch no.: 411 | Training Loss: 0.14777674458920956 | Validation Loss: 0.1381067730486393\n",
      "Epoch no.: 412 | Training Loss: 0.14785071656107904 | Validation Loss: 0.13796131759881974\n",
      "Epoch no.: 413 | Training Loss: 0.14790901601314543 | Validation Loss: 0.13804521709680556\n",
      "Epoch no.: 414 | Training Loss: 0.14783731184899807 | Validation Loss: 0.13860355615615844\n",
      "Epoch no.: 415 | Training Loss: 0.14777567595243454 | Validation Loss: 0.1384042590856552\n",
      "Epoch no.: 416 | Training Loss: 0.14778285168111324 | Validation Loss: 0.13812963366508485\n",
      "Epoch no.: 417 | Training Loss: 0.14809335455298422 | Validation Loss: 0.13871967792510986\n",
      "Epoch no.: 418 | Training Loss: 0.1479566166549921 | Validation Loss: 0.13833058178424834\n",
      "Epoch no.: 419 | Training Loss: 0.14785286620259286 | Validation Loss: 0.1380723848938942\n",
      "Epoch no.: 420 | Training Loss: 0.1479908813536167 | Validation Loss: 0.13854215666651726\n",
      "Epoch no.: 421 | Training Loss: 0.14773800410330296 | Validation Loss: 0.1381298430263996\n",
      "Epoch no.: 422 | Training Loss: 0.14783271603286266 | Validation Loss: 0.137970656901598\n",
      "Epoch no.: 423 | Training Loss: 0.1477278858423233 | Validation Loss: 0.1381714440882206\n",
      "Epoch no.: 424 | Training Loss: 0.1477192724496126 | Validation Loss: 0.13796289190649985\n",
      "Epoch no.: 425 | Training Loss: 0.14808858379721643 | Validation Loss: 0.13795289769768715\n",
      "Epoch no.: 426 | Training Loss: 0.14789991028606891 | Validation Loss: 0.13796906247735025\n",
      "Epoch no.: 427 | Training Loss: 0.14775320753455162 | Validation Loss: 0.13840121105313302\n",
      "Epoch no.: 428 | Training Loss: 0.14799459733068943 | Validation Loss: 0.13796867057681084\n",
      "Epoch no.: 429 | Training Loss: 0.147750309035182 | Validation Loss: 0.13849296644330025\n",
      "Epoch no.: 430 | Training Loss: 0.14779385216534138 | Validation Loss: 0.13823585584759712\n",
      "Epoch no.: 431 | Training Loss: 0.14787850491702556 | Validation Loss: 0.1379668392241001\n",
      "Epoch no.: 432 | Training Loss: 0.1477980125695467 | Validation Loss: 0.13801233172416688\n",
      "Epoch no.: 433 | Training Loss: 0.14789100632071495 | Validation Loss: 0.13836478143930436\n",
      "Epoch no.: 434 | Training Loss: 0.1481023219972849 | Validation Loss: 0.13795813024044037\n",
      "Epoch no.: 435 | Training Loss: 0.14788394279778003 | Validation Loss: 0.13806014582514764\n",
      "Epoch no.: 436 | Training Loss: 0.1478439588099718 | Validation Loss: 0.1382188007235527\n",
      "Epoch no.: 437 | Training Loss: 0.1478592125326395 | Validation Loss: 0.13824013844132424\n",
      "Epoch no.: 438 | Training Loss: 0.14774756461381913 | Validation Loss: 0.13804584443569184\n",
      "Epoch no.: 439 | Training Loss: 0.14796264462172984 | Validation Loss: 0.13811109513044356\n",
      "Epoch no.: 440 | Training Loss: 0.1479063092172146 | Validation Loss: 0.13808287382125856\n",
      "Epoch no.: 441 | Training Loss: 0.14780577018857002 | Validation Loss: 0.1380638912320137\n",
      "Epoch no.: 442 | Training Loss: 0.1477993718534708 | Validation Loss: 0.13805293589830397\n",
      "Epoch no.: 443 | Training Loss: 0.1478270348161459 | Validation Loss: 0.13845413252711297\n",
      "Epoch no.: 444 | Training Loss: 0.14793527089059352 | Validation Loss: 0.1382925644516945\n",
      "Epoch no.: 445 | Training Loss: 0.1476007379591465 | Validation Loss: 0.13823699951171875\n",
      "Epoch no.: 446 | Training Loss: 0.14776711151003838 | Validation Loss: 0.1379626326262951\n",
      "Epoch no.: 447 | Training Loss: 0.14795369811356068 | Validation Loss: 0.13796993643045424\n",
      "Epoch no.: 448 | Training Loss: 0.1478805410116911 | Validation Loss: 0.1381034530699253\n",
      "Epoch no.: 449 | Training Loss: 0.147745842859149 | Validation Loss: 0.13797505274415017\n",
      "Epoch no.: 450 | Training Loss: 0.14774136923253536 | Validation Loss: 0.13813607543706893\n",
      "Epoch no.: 451 | Training Loss: 0.14773807875812053 | Validation Loss: 0.13858720287680626\n",
      "Epoch no.: 452 | Training Loss: 0.14781190343201162 | Validation Loss: 0.1384781062602997\n",
      "Epoch no.: 453 | Training Loss: 0.1482178633660078 | Validation Loss: 0.13816031739115714\n",
      "Epoch no.: 454 | Training Loss: 0.14780936427414418 | Validation Loss: 0.13804693073034285\n",
      "Epoch no.: 455 | Training Loss: 0.1476887983828783 | Validation Loss: 0.13801832050085067\n",
      "Epoch no.: 456 | Training Loss: 0.1479162472486496 | Validation Loss: 0.1379589930176735\n",
      "Epoch no.: 457 | Training Loss: 0.14782540388405324 | Validation Loss: 0.1380338378250599\n",
      "Epoch no.: 458 | Training Loss: 0.14772857390344143 | Validation Loss: 0.13797089830040932\n",
      "Epoch no.: 459 | Training Loss: 0.1476336731761694 | Validation Loss: 0.13820190355181694\n",
      "Epoch no.: 460 | Training Loss: 0.14795017033815383 | Validation Loss: 0.13802148401737213\n",
      "Epoch no.: 461 | Training Loss: 0.14769163176417352 | Validation Loss: 0.1382259614765644\n",
      "Epoch no.: 462 | Training Loss: 0.14783641576766968 | Validation Loss: 0.13844108656048776\n",
      "Epoch no.: 463 | Training Loss: 0.14787566654384135 | Validation Loss: 0.1380547359585762\n",
      "Epoch no.: 464 | Training Loss: 0.14784841261804105 | Validation Loss: 0.1384636104106903\n",
      "Epoch no.: 465 | Training Loss: 0.14778201207518576 | Validation Loss: 0.1379908837378025\n",
      "Epoch no.: 466 | Training Loss: 0.14779693394899368 | Validation Loss: 0.13799086064100266\n",
      "Epoch no.: 467 | Training Loss: 0.14784377343952657 | Validation Loss: 0.13803633451461791\n",
      "Epoch no.: 468 | Training Loss: 0.14798234552145004 | Validation Loss: 0.13886384591460227\n",
      "Epoch no.: 469 | Training Loss: 0.14777846254408358 | Validation Loss: 0.1379900634288788\n",
      "Epoch no.: 470 | Training Loss: 0.14779359869658948 | Validation Loss: 0.1380189634859562\n",
      "Epoch no.: 471 | Training Loss: 0.14774674884974956 | Validation Loss: 0.13811023086309432\n",
      "Epoch no.: 472 | Training Loss: 0.14791170321404934 | Validation Loss: 0.13825618252158164\n",
      "Epoch no.: 473 | Training Loss: 0.14774467669427394 | Validation Loss: 0.13801943212747575\n",
      "Epoch no.: 474 | Training Loss: 0.14790509700775145 | Validation Loss: 0.13797298073768616\n",
      "Epoch no.: 475 | Training Loss: 0.14784430883824826 | Validation Loss: 0.13897699266672134\n",
      "Epoch no.: 476 | Training Loss: 0.14779946006834507 | Validation Loss: 0.13797183185815812\n",
      "Epoch no.: 477 | Training Loss: 0.1478643310815096 | Validation Loss: 0.13802231028676032\n",
      "Epoch no.: 478 | Training Loss: 0.14786056756973268 | Validation Loss: 0.13823169618844985\n",
      "Epoch no.: 479 | Training Loss: 0.1479478120058775 | Validation Loss: 0.1380835860967636\n",
      "Epoch no.: 480 | Training Loss: 0.14771736517548562 | Validation Loss: 0.13943402394652366\n",
      "Epoch no.: 481 | Training Loss: 0.14783306680619718 | Validation Loss: 0.13805968314409256\n",
      "Epoch no.: 482 | Training Loss: 0.14788660496473313 | Validation Loss: 0.13814759105443955\n",
      "Epoch no.: 483 | Training Loss: 0.14784284725785254 | Validation Loss: 0.13806170970201492\n",
      "Epoch no.: 484 | Training Loss: 0.14786282338202 | Validation Loss: 0.13848771825432776\n",
      "Epoch no.: 485 | Training Loss: 0.14793142452836036 | Validation Loss: 0.13809595257043839\n",
      "Epoch no.: 486 | Training Loss: 0.1477305942028761 | Validation Loss: 0.13827528282999993\n",
      "Epoch no.: 487 | Training Loss: 0.14776521682739258 | Validation Loss: 0.13918665871024133\n",
      "Epoch no.: 488 | Training Loss: 0.1476753509044647 | Validation Loss: 0.1384742461144924\n",
      "Epoch no.: 489 | Training Loss: 0.1479673534631729 | Validation Loss: 0.13830109536647797\n",
      "Epoch no.: 490 | Training Loss: 0.1476676758378744 | Validation Loss: 0.13796777576208114\n",
      "Epoch no.: 491 | Training Loss: 0.14771920278668405 | Validation Loss: 0.13803933933377266\n",
      "Epoch no.: 492 | Training Loss: 0.1478704047948122 | Validation Loss: 0.1384084552526474\n",
      "Epoch no.: 493 | Training Loss: 0.14784291103482247 | Validation Loss: 0.13798813745379448\n",
      "Epoch no.: 494 | Training Loss: 0.14785626739263535 | Validation Loss: 0.13817874267697333\n",
      "Epoch no.: 495 | Training Loss: 0.1477980864048004 | Validation Loss: 0.13814225420355797\n",
      "Epoch no.: 496 | Training Loss: 0.14774971306324006 | Validation Loss: 0.13800592496991157\n",
      "Epoch no.: 497 | Training Loss: 0.14789555534720422 | Validation Loss: 0.1379705049097538\n",
      "Epoch no.: 498 | Training Loss: 0.1477059391885996 | Validation Loss: 0.1385826788842678\n",
      "Epoch no.: 499 | Training Loss: 0.14783888526260852 | Validation Loss: 0.13799093365669252\n",
      "Epoch no.: 500 | Training Loss: 0.14777137003839017 | Validation Loss: 0.13831861391663552\n",
      "Epoch no.: 501 | Training Loss: 0.14780642963945867 | Validation Loss: 0.13806207552552224\n",
      "Epoch no.: 502 | Training Loss: 0.1477946174144745 | Validation Loss: 0.13796351477503777\n",
      "Epoch no.: 503 | Training Loss: 0.14773022316396237 | Validation Loss: 0.137957401573658\n",
      "Epoch no.: 504 | Training Loss: 0.1478231478482485 | Validation Loss: 0.13813364133238792\n",
      "Epoch no.: 505 | Training Loss: 0.1478562708199024 | Validation Loss: 0.13827940300107003\n",
      "Epoch no.: 506 | Training Loss: 0.1478507674485445 | Validation Loss: 0.13805105909705162\n",
      "Epoch no.: 507 | Training Loss: 0.14790787398815156 | Validation Loss: 0.13802197501063346\n",
      "Epoch no.: 508 | Training Loss: 0.14777127906680107 | Validation Loss: 0.13847087547183037\n",
      "Epoch no.: 509 | Training Loss: 0.1477987015247345 | Validation Loss: 0.13857163712382317\n",
      "Epoch no.: 510 | Training Loss: 0.14776522152125834 | Validation Loss: 0.138093239068985\n",
      "Epoch no.: 511 | Training Loss: 0.14773707129061223 | Validation Loss: 0.1382896237075329\n",
      "Epoch no.: 512 | Training Loss: 0.14773606166243552 | Validation Loss: 0.13823567554354668\n",
      "Epoch no.: 513 | Training Loss: 0.14785344801843167 | Validation Loss: 0.13798975497484206\n",
      "Epoch no.: 514 | Training Loss: 0.14778186790645123 | Validation Loss: 0.13797445148229598\n",
      "Epoch no.: 515 | Training Loss: 0.14786848954856396 | Validation Loss: 0.13794603273272515\n",
      "Epoch no.: 516 | Training Loss: 0.14792418219149112 | Validation Loss: 0.13800562992691995\n",
      "Epoch no.: 517 | Training Loss: 0.147749802544713 | Validation Loss: 0.13796658292412758\n",
      "Epoch no.: 518 | Training Loss: 0.14778032176196576 | Validation Loss: 0.1381831608712673\n",
      "Epoch no.: 519 | Training Loss: 0.1480308923125267 | Validation Loss: 0.13814908191561698\n",
      "Epoch no.: 520 | Training Loss: 0.14767000921070575 | Validation Loss: 0.13852281197905542\n",
      "Epoch no.: 521 | Training Loss: 0.14770780704915523 | Validation Loss: 0.13815147280693055\n",
      "Epoch no.: 522 | Training Loss: 0.14795467033982276 | Validation Loss: 0.138474802672863\n",
      "Epoch no.: 523 | Training Loss: 0.14784143663942814 | Validation Loss: 0.1381930463016033\n",
      "Epoch no.: 524 | Training Loss: 0.14778958596289157 | Validation Loss: 0.13802709355950354\n",
      "Epoch no.: 525 | Training Loss: 0.14794196486473082 | Validation Loss: 0.1387961134314537\n",
      "Epoch no.: 526 | Training Loss: 0.14773630023002624 | Validation Loss: 0.1380486734211445\n",
      "Epoch no.: 527 | Training Loss: 0.1476884875446558 | Validation Loss: 0.1382317066192627\n",
      "Epoch no.: 528 | Training Loss: 0.147802562341094 | Validation Loss: 0.13847897052764893\n",
      "Epoch no.: 529 | Training Loss: 0.14774565167725087 | Validation Loss: 0.13825505077838898\n",
      "Epoch no.: 530 | Training Loss: 0.14786197789013386 | Validation Loss: 0.13834547251462936\n",
      "Epoch no.: 531 | Training Loss: 0.1477602231502533 | Validation Loss: 0.1381770744919777\n",
      "Epoch no.: 532 | Training Loss: 0.14786453798413277 | Validation Loss: 0.13809676021337508\n",
      "Epoch no.: 533 | Training Loss: 0.14775358356535434 | Validation Loss: 0.13858306258916855\n",
      "Epoch no.: 534 | Training Loss: 0.14787849150598048 | Validation Loss: 0.13806448355317116\n",
      "Epoch no.: 535 | Training Loss: 0.1477184746414423 | Validation Loss: 0.13886113837361336\n",
      "Epoch no.: 536 | Training Loss: 0.14781868360936642 | Validation Loss: 0.1387379549443722\n",
      "Epoch no.: 537 | Training Loss: 0.14794371627271174 | Validation Loss: 0.13815307915210723\n",
      "Epoch no.: 538 | Training Loss: 0.1478450358659029 | Validation Loss: 0.13805904760956764\n",
      "Epoch no.: 539 | Training Loss: 0.14771392792463303 | Validation Loss: 0.13804200291633606\n",
      "Epoch no.: 540 | Training Loss: 0.147816109880805 | Validation Loss: 0.13798320442438125\n",
      "Epoch no.: 541 | Training Loss: 0.14782011412084103 | Validation Loss: 0.1381290502846241\n",
      "Epoch no.: 542 | Training Loss: 0.14784230716526509 | Validation Loss: 0.1379636637866497\n",
      "Epoch no.: 543 | Training Loss: 0.14794931218028068 | Validation Loss: 0.13864456862211227\n",
      "Epoch no.: 544 | Training Loss: 0.14782113946974276 | Validation Loss: 0.13796718195080757\n",
      "Epoch no.: 545 | Training Loss: 0.14773997500538827 | Validation Loss: 0.1381082035601139\n",
      "Epoch no.: 546 | Training Loss: 0.14771161943674088 | Validation Loss: 0.13816231414675711\n",
      "Epoch no.: 547 | Training Loss: 0.14785975858569145 | Validation Loss: 0.13856289088726043\n",
      "Epoch no.: 548 | Training Loss: 0.14772539846599103 | Validation Loss: 0.13802223578095435\n",
      "Epoch no.: 549 | Training Loss: 0.14781130395829678 | Validation Loss: 0.13862262517213822\n",
      "Epoch no.: 550 | Training Loss: 0.14772058814764022 | Validation Loss: 0.13797277882695197\n",
      "Epoch no.: 551 | Training Loss: 0.1476576478034258 | Validation Loss: 0.13800573125481605\n",
      "Epoch no.: 552 | Training Loss: 0.147817018404603 | Validation Loss: 0.13833302706480027\n",
      "Epoch no.: 553 | Training Loss: 0.14783564753830433 | Validation Loss: 0.13845251351594925\n",
      "Epoch no.: 554 | Training Loss: 0.1477096624672413 | Validation Loss: 0.13950600549578668\n",
      "Epoch no.: 555 | Training Loss: 0.14781949557363988 | Validation Loss: 0.1380816951394081\n",
      "Epoch no.: 556 | Training Loss: 0.14788396745920182 | Validation Loss: 0.13805704563856125\n",
      "Epoch no.: 557 | Training Loss: 0.14790036834776402 | Validation Loss: 0.1379869244992733\n",
      "Epoch no.: 558 | Training Loss: 0.1477227069437504 | Validation Loss: 0.13798466846346855\n",
      "Epoch no.: 559 | Training Loss: 0.14784851163625717 | Validation Loss: 0.13829628601670266\n",
      "Epoch no.: 560 | Training Loss: 0.14768680848181248 | Validation Loss: 0.13797732889652253\n",
      "Epoch no.: 561 | Training Loss: 0.14770622670650482 | Validation Loss: 0.13814875334501267\n",
      "Epoch no.: 562 | Training Loss: 0.14787821032106876 | Validation Loss: 0.13826836496591569\n",
      "Epoch no.: 563 | Training Loss: 0.14790270157158375 | Validation Loss: 0.1384054847061634\n",
      "Epoch no.: 564 | Training Loss: 0.1481160843372345 | Validation Loss: 0.13795770481228828\n",
      "Epoch no.: 565 | Training Loss: 0.1478403628617525 | Validation Loss: 0.13806348890066147\n",
      "Epoch no.: 566 | Training Loss: 0.14781446509063245 | Validation Loss: 0.13895455598831177\n",
      "Epoch no.: 567 | Training Loss: 0.14780823893845083 | Validation Loss: 0.13804325163364412\n",
      "Epoch no.: 568 | Training Loss: 0.14780908048152924 | Validation Loss: 0.13819383531808854\n",
      "Epoch no.: 569 | Training Loss: 0.14783215418457984 | Validation Loss: 0.13886693567037584\n",
      "Epoch no.: 570 | Training Loss: 0.14796097122132779 | Validation Loss: 0.13993203938007354\n",
      "Epoch no.: 571 | Training Loss: 0.14785706162452697 | Validation Loss: 0.13831049352884292\n",
      "Epoch no.: 572 | Training Loss: 0.14769043661653997 | Validation Loss: 0.13810367062687873\n",
      "Epoch no.: 573 | Training Loss: 0.14781997181475162 | Validation Loss: 0.1380436547100544\n",
      "Epoch no.: 574 | Training Loss: 0.1479693878442049 | Validation Loss: 0.13796067833900452\n",
      "Epoch no.: 575 | Training Loss: 0.14768310993909836 | Validation Loss: 0.13798123747110366\n",
      "Epoch no.: 576 | Training Loss: 0.14800838246941567 | Validation Loss: 0.13918927684426308\n",
      "Epoch no.: 577 | Training Loss: 0.14781870648264886 | Validation Loss: 0.1381903663277626\n",
      "Epoch no.: 578 | Training Loss: 0.14786645144224167 | Validation Loss: 0.1379611648619175\n",
      "Epoch no.: 579 | Training Loss: 0.14783158510923386 | Validation Loss: 0.13802278116345407\n",
      "Epoch no.: 580 | Training Loss: 0.14792543083429335 | Validation Loss: 0.13869438990950583\n",
      "Epoch no.: 581 | Training Loss: 0.14791641056537627 | Validation Loss: 0.1379844903945923\n",
      "Epoch no.: 582 | Training Loss: 0.14776308536529542 | Validation Loss: 0.13885483592748643\n",
      "Epoch no.: 583 | Training Loss: 0.1477490108460188 | Validation Loss: 0.13838522732257844\n",
      "Epoch no.: 584 | Training Loss: 0.14797027587890624 | Validation Loss: 0.1381496161222458\n",
      "Epoch no.: 585 | Training Loss: 0.14807668663561344 | Validation Loss: 0.13809509724378585\n",
      "Epoch no.: 586 | Training Loss: 0.14781496331095695 | Validation Loss: 0.13806778118014335\n",
      "Epoch no.: 587 | Training Loss: 0.14781944945454598 | Validation Loss: 0.13861350566148758\n",
      "Epoch no.: 588 | Training Loss: 0.14776538386940957 | Validation Loss: 0.13810737133026124\n",
      "Epoch no.: 589 | Training Loss: 0.14790534988045692 | Validation Loss: 0.13797815889120102\n",
      "Epoch no.: 590 | Training Loss: 0.14781695768237113 | Validation Loss: 0.13863336741924287\n",
      "Epoch no.: 591 | Training Loss: 0.14806926906108855 | Validation Loss: 0.13797479271888732\n",
      "Epoch no.: 592 | Training Loss: 0.14772530652582647 | Validation Loss: 0.13797122463583947\n",
      "Epoch no.: 593 | Training Loss: 0.14769030332565308 | Validation Loss: 0.1382480077445507\n",
      "Epoch no.: 594 | Training Loss: 0.14781088821589947 | Validation Loss: 0.13819233775138856\n",
      "Epoch no.: 595 | Training Loss: 0.1477934554219246 | Validation Loss: 0.1381254330277443\n",
      "Epoch no.: 596 | Training Loss: 0.14776509828865528 | Validation Loss: 0.13807217478752137\n",
      "Epoch no.: 597 | Training Loss: 0.14779001854360105 | Validation Loss: 0.13832331001758574\n",
      "Epoch no.: 598 | Training Loss: 0.1477125132083893 | Validation Loss: 0.13823892623186113\n",
      "Epoch no.: 599 | Training Loss: 0.14794343940913676 | Validation Loss: 0.13805764093995093\n",
      "Epoch no.: 600 | Training Loss: 0.14783998891711236 | Validation Loss: 0.13808980360627174\n",
      "Epoch no.: 601 | Training Loss: 0.14774209715425968 | Validation Loss: 0.13846577033400537\n",
      "Epoch no.: 602 | Training Loss: 0.14780272275209427 | Validation Loss: 0.13798805475234985\n",
      "Epoch no.: 603 | Training Loss: 0.1477246519178152 | Validation Loss: 0.13807169049978257\n",
      "Epoch no.: 604 | Training Loss: 0.14770961619913578 | Validation Loss: 0.13818079903721808\n",
      "Epoch no.: 605 | Training Loss: 0.1479089146107435 | Validation Loss: 0.13935916274785995\n",
      "Epoch no.: 606 | Training Loss: 0.14803293764591216 | Validation Loss: 0.13823501542210578\n",
      "Epoch no.: 607 | Training Loss: 0.1477565412968397 | Validation Loss: 0.13818957954645156\n",
      "Epoch no.: 608 | Training Loss: 0.14777308873832226 | Validation Loss: 0.1379629723727703\n",
      "Epoch no.: 609 | Training Loss: 0.14785070054233074 | Validation Loss: 0.13856999799609185\n",
      "Epoch no.: 610 | Training Loss: 0.1477256001532078 | Validation Loss: 0.13831793144345284\n",
      "Epoch no.: 611 | Training Loss: 0.1477348493784666 | Validation Loss: 0.13819001913070678\n",
      "Epoch no.: 612 | Training Loss: 0.14781260438263416 | Validation Loss: 0.1381199724972248\n",
      "Epoch no.: 613 | Training Loss: 0.14775085493922233 | Validation Loss: 0.1380613625049591\n",
      "Epoch no.: 614 | Training Loss: 0.14781505577266216 | Validation Loss: 0.1379915200173855\n",
      "Epoch no.: 615 | Training Loss: 0.14776459358632565 | Validation Loss: 0.1379658468067646\n",
      "Epoch no.: 616 | Training Loss: 0.14785505704581736 | Validation Loss: 0.13817484974861144\n",
      "Epoch no.: 617 | Training Loss: 0.1478022462874651 | Validation Loss: 0.13804488778114318\n",
      "Epoch no.: 618 | Training Loss: 0.14777533225715161 | Validation Loss: 0.13825577720999718\n",
      "Epoch no.: 619 | Training Loss: 0.14775551296770573 | Validation Loss: 0.13841500431299208\n",
      "Epoch no.: 620 | Training Loss: 0.14801065124571322 | Validation Loss: 0.13795575276017188\n",
      "Epoch no.: 621 | Training Loss: 0.14782608047127724 | Validation Loss: 0.13811247050762177\n",
      "Epoch no.: 622 | Training Loss: 0.14773477852344513 | Validation Loss: 0.13799396008253098\n",
      "Epoch no.: 623 | Training Loss: 0.14777369931340217 | Validation Loss: 0.138550066947937\n",
      "Epoch no.: 624 | Training Loss: 0.148026354983449 | Validation Loss: 0.13805893138051034\n",
      "Epoch no.: 625 | Training Loss: 0.14789558410644532 | Validation Loss: 0.14025538116693498\n",
      "Epoch no.: 626 | Training Loss: 0.1479778052866459 | Validation Loss: 0.13818345814943314\n",
      "Epoch no.: 627 | Training Loss: 0.14784406341612338 | Validation Loss: 0.13803110793232917\n",
      "Epoch no.: 628 | Training Loss: 0.14790347151458264 | Validation Loss: 0.13802115246653557\n",
      "Epoch no.: 629 | Training Loss: 0.14796051286160947 | Validation Loss: 0.13979114219546318\n",
      "Epoch no.: 630 | Training Loss: 0.14781136065721512 | Validation Loss: 0.1381099469959736\n",
      "Epoch no.: 631 | Training Loss: 0.14787228167057037 | Validation Loss: 0.13874283954501151\n",
      "Epoch no.: 632 | Training Loss: 0.14778091311454772 | Validation Loss: 0.13825848922133446\n",
      "Epoch no.: 633 | Training Loss: 0.14774013079702855 | Validation Loss: 0.13795674666762353\n",
      "Epoch no.: 634 | Training Loss: 0.14781188741326332 | Validation Loss: 0.1379573218524456\n",
      "Epoch no.: 635 | Training Loss: 0.14782913468778133 | Validation Loss: 0.13846739158034324\n",
      "Epoch no.: 636 | Training Loss: 0.14781181268393995 | Validation Loss: 0.13831350728869438\n",
      "Epoch no.: 637 | Training Loss: 0.14767430767416953 | Validation Loss: 0.13798385933041574\n",
      "Epoch no.: 638 | Training Loss: 0.14778199024498462 | Validation Loss: 0.13800933435559273\n",
      "Epoch no.: 639 | Training Loss: 0.14787198901176452 | Validation Loss: 0.1388135403394699\n",
      "Epoch no.: 640 | Training Loss: 0.14771608665585517 | Validation Loss: 0.1385613724589348\n",
      "Epoch no.: 641 | Training Loss: 0.14778318926692008 | Validation Loss: 0.138182320445776\n",
      "Epoch no.: 642 | Training Loss: 0.1478223344683647 | Validation Loss: 0.13920503482222557\n",
      "Epoch no.: 643 | Training Loss: 0.14783487610518933 | Validation Loss: 0.13873202875256538\n",
      "Epoch no.: 644 | Training Loss: 0.14777939528226852 | Validation Loss: 0.13854371383786201\n",
      "Epoch no.: 645 | Training Loss: 0.14773738861083985 | Validation Loss: 0.1379581168293953\n",
      "Epoch no.: 646 | Training Loss: 0.14774264350533486 | Validation Loss: 0.13802684396505355\n",
      "Epoch no.: 647 | Training Loss: 0.14770240053534509 | Validation Loss: 0.13803013190627098\n",
      "Epoch no.: 648 | Training Loss: 0.14763571985065937 | Validation Loss: 0.13864656388759614\n",
      "Epoch no.: 649 | Training Loss: 0.14788063138723373 | Validation Loss: 0.13810896575450898\n",
      "Epoch no.: 650 | Training Loss: 0.14776049360632895 | Validation Loss: 0.1379939779639244\n",
      "Epoch no.: 651 | Training Loss: 0.14781264550983905 | Validation Loss: 0.1380664251744747\n",
      "Epoch no.: 652 | Training Loss: 0.14778050422668457 | Validation Loss: 0.13868191316723824\n",
      "Epoch no.: 653 | Training Loss: 0.14780157297849655 | Validation Loss: 0.13797674030065538\n",
      "Epoch no.: 654 | Training Loss: 0.14785222746431828 | Validation Loss: 0.13849861025810242\n",
      "Epoch no.: 655 | Training Loss: 0.14782738357782363 | Validation Loss: 0.1384667530655861\n",
      "Epoch no.: 656 | Training Loss: 0.147988638356328 | Validation Loss: 0.13796915411949157\n",
      "Epoch no.: 657 | Training Loss: 0.14776628158986568 | Validation Loss: 0.1381235182285309\n",
      "Epoch no.: 658 | Training Loss: 0.14786045260727407 | Validation Loss: 0.13794725239276887\n",
      "Epoch no.: 659 | Training Loss: 0.14768923930823802 | Validation Loss: 0.13795243948698044\n",
      "Epoch no.: 660 | Training Loss: 0.14779545225203036 | Validation Loss: 0.1380861409008503\n",
      "Epoch no.: 661 | Training Loss: 0.1478230009227991 | Validation Loss: 0.13898541405797005\n",
      "Epoch no.: 662 | Training Loss: 0.14775221198797225 | Validation Loss: 0.13822147995233536\n",
      "Epoch no.: 663 | Training Loss: 0.1477455100417137 | Validation Loss: 0.13807384595274924\n",
      "Epoch no.: 664 | Training Loss: 0.14791067965328694 | Validation Loss: 0.13851072490215302\n",
      "Epoch no.: 665 | Training Loss: 0.1477757403254509 | Validation Loss: 0.13819572925567628\n",
      "Epoch no.: 666 | Training Loss: 0.1476906929165125 | Validation Loss: 0.1381380960345268\n",
      "Epoch no.: 667 | Training Loss: 0.14775207936763762 | Validation Loss: 0.13833667635917662\n",
      "Epoch no.: 668 | Training Loss: 0.1478503678739071 | Validation Loss: 0.13810618668794633\n",
      "Epoch no.: 669 | Training Loss: 0.14777421370148658 | Validation Loss: 0.1379944786429405\n",
      "Epoch no.: 670 | Training Loss: 0.1478203009814024 | Validation Loss: 0.13808926418423653\n",
      "Epoch no.: 671 | Training Loss: 0.14790086343884468 | Validation Loss: 0.1379988044500351\n",
      "Epoch no.: 672 | Training Loss: 0.14777957782149315 | Validation Loss: 0.1379690870642662\n",
      "Epoch no.: 673 | Training Loss: 0.14801551066339017 | Validation Loss: 0.138176342099905\n",
      "Epoch no.: 674 | Training Loss: 0.14776470482349396 | Validation Loss: 0.13844180330634118\n",
      "Epoch no.: 675 | Training Loss: 0.14775705389678478 | Validation Loss: 0.13806431591510773\n",
      "Epoch no.: 676 | Training Loss: 0.14777190044522284 | Validation Loss: 0.13804899156093597\n",
      "Epoch no.: 677 | Training Loss: 0.1477835364639759 | Validation Loss: 0.13813919946551323\n",
      "Epoch no.: 678 | Training Loss: 0.14780533216893674 | Validation Loss: 0.13828590735793114\n",
      "Epoch no.: 679 | Training Loss: 0.1478104907274246 | Validation Loss: 0.13821262642741203\n",
      "Epoch no.: 680 | Training Loss: 0.14790172629058362 | Validation Loss: 0.13802658542990684\n",
      "Epoch no.: 681 | Training Loss: 0.14780554309487343 | Validation Loss: 0.1387610539793968\n",
      "Epoch no.: 682 | Training Loss: 0.1478815820068121 | Validation Loss: 0.13830840736627578\n",
      "Epoch no.: 683 | Training Loss: 0.14776302076876163 | Validation Loss: 0.13805147558450698\n",
      "Epoch no.: 684 | Training Loss: 0.14779034890234471 | Validation Loss: 0.13821458965539932\n",
      "Epoch no.: 685 | Training Loss: 0.1476896370947361 | Validation Loss: 0.13819113075733186\n",
      "Epoch no.: 686 | Training Loss: 0.14776989668607712 | Validation Loss: 0.13831535950303078\n",
      "Epoch no.: 687 | Training Loss: 0.14789621248841286 | Validation Loss: 0.13819143921136856\n",
      "Epoch no.: 688 | Training Loss: 0.14770460046827794 | Validation Loss: 0.13799264430999755\n",
      "Epoch no.: 689 | Training Loss: 0.14790981583297252 | Validation Loss: 0.13798512369394303\n",
      "Epoch no.: 690 | Training Loss: 0.1478381120413542 | Validation Loss: 0.13799262493848802\n",
      "Epoch no.: 691 | Training Loss: 0.14777175202965737 | Validation Loss: 0.13801255226135253\n",
      "Epoch no.: 692 | Training Loss: 0.1476879161596298 | Validation Loss: 0.13838979825377465\n",
      "Epoch no.: 693 | Training Loss: 0.14799575939774512 | Validation Loss: 0.13824847489595413\n",
      "Epoch no.: 694 | Training Loss: 0.14779525719583034 | Validation Loss: 0.13815351724624633\n",
      "Epoch no.: 695 | Training Loss: 0.14768693573772906 | Validation Loss: 0.13806502297520637\n",
      "Epoch no.: 696 | Training Loss: 0.147911479100585 | Validation Loss: 0.13812987953424455\n",
      "Epoch no.: 697 | Training Loss: 0.14781480386853219 | Validation Loss: 0.13817030042409897\n",
      "Epoch no.: 698 | Training Loss: 0.147742780148983 | Validation Loss: 0.13796599209308624\n",
      "Epoch no.: 699 | Training Loss: 0.14775219261646272 | Validation Loss: 0.1379941523075104\n",
      "Epoch no.: 700 | Training Loss: 0.14776711978018284 | Validation Loss: 0.13811435997486116\n",
      "Epoch no.: 701 | Training Loss: 0.14772694267332553 | Validation Loss: 0.13834666833281517\n",
      "Epoch no.: 702 | Training Loss: 0.14777866043150426 | Validation Loss: 0.1380154624581337\n",
      "Epoch no.: 703 | Training Loss: 0.14764753751456738 | Validation Loss: 0.13795435950160026\n",
      "Epoch no.: 704 | Training Loss: 0.14778079107403755 | Validation Loss: 0.1381359077990055\n",
      "Epoch no.: 705 | Training Loss: 0.1478381607681513 | Validation Loss: 0.13874072283506395\n",
      "Epoch no.: 706 | Training Loss: 0.14777151137590408 | Validation Loss: 0.13802914172410966\n",
      "Epoch no.: 707 | Training Loss: 0.1478189826756716 | Validation Loss: 0.13811070919036866\n",
      "Epoch no.: 708 | Training Loss: 0.14780641168355943 | Validation Loss: 0.13799265697598456\n",
      "Epoch no.: 709 | Training Loss: 0.14771809376776218 | Validation Loss: 0.13797529861330987\n",
      "Epoch no.: 710 | Training Loss: 0.14790753602981568 | Validation Loss: 0.13808635696768762\n",
      "Epoch no.: 711 | Training Loss: 0.14774681247770785 | Validation Loss: 0.13821059241890907\n",
      "Epoch no.: 712 | Training Loss: 0.14769663505256175 | Validation Loss: 0.13834720924496652\n",
      "Epoch no.: 713 | Training Loss: 0.147761015817523 | Validation Loss: 0.13832867220044137\n",
      "Epoch no.: 714 | Training Loss: 0.14773924618959428 | Validation Loss: 0.1391161747276783\n",
      "Epoch no.: 715 | Training Loss: 0.14785280920565128 | Validation Loss: 0.13806281089782715\n",
      "Epoch no.: 716 | Training Loss: 0.1477902690321207 | Validation Loss: 0.1382785089313984\n",
      "Epoch no.: 717 | Training Loss: 0.14771799623966217 | Validation Loss: 0.13796278163790704\n",
      "Epoch no.: 718 | Training Loss: 0.14775892220437525 | Validation Loss: 0.13914640694856645\n",
      "Epoch no.: 719 | Training Loss: 0.14802855253219604 | Validation Loss: 0.13834951519966127\n",
      "Epoch no.: 720 | Training Loss: 0.14777312949299812 | Validation Loss: 0.13800171688199042\n",
      "Epoch no.: 721 | Training Loss: 0.1478779484331608 | Validation Loss: 0.1379690319299698\n",
      "Epoch no.: 722 | Training Loss: 0.1479378415644169 | Validation Loss: 0.1380392827093601\n",
      "Epoch no.: 723 | Training Loss: 0.14770859472453593 | Validation Loss: 0.1386840485036373\n",
      "Epoch no.: 724 | Training Loss: 0.14785141080617906 | Validation Loss: 0.13812668472528458\n",
      "Epoch no.: 725 | Training Loss: 0.1476977375149727 | Validation Loss: 0.13818009197711945\n",
      "Epoch no.: 726 | Training Loss: 0.14789550177752972 | Validation Loss: 0.13933232352137565\n",
      "Epoch no.: 727 | Training Loss: 0.14781746685504912 | Validation Loss: 0.13797358870506288\n",
      "Epoch no.: 728 | Training Loss: 0.14786038905382157 | Validation Loss: 0.1380281016230583\n",
      "Epoch no.: 729 | Training Loss: 0.1477390019595623 | Validation Loss: 0.1380346342921257\n",
      "Epoch no.: 730 | Training Loss: 0.14772414200007916 | Validation Loss: 0.1384139157831669\n",
      "Epoch no.: 731 | Training Loss: 0.14796478532254695 | Validation Loss: 0.13828982561826705\n",
      "Epoch no.: 732 | Training Loss: 0.14783889517188073 | Validation Loss: 0.1384452059864998\n",
      "Epoch no.: 733 | Training Loss: 0.1477747665345669 | Validation Loss: 0.13817354366183282\n",
      "Epoch no.: 734 | Training Loss: 0.1478353749215603 | Validation Loss: 0.13849989846348762\n",
      "Epoch no.: 735 | Training Loss: 0.14785269513726235 | Validation Loss: 0.1382665976881981\n",
      "Epoch no.: 736 | Training Loss: 0.14773129284381867 | Validation Loss: 0.13802525475621225\n",
      "Epoch no.: 737 | Training Loss: 0.1477259486913681 | Validation Loss: 0.13854638636112213\n",
      "Epoch no.: 738 | Training Loss: 0.14770276091992854 | Validation Loss: 0.13814727216959\n",
      "Epoch no.: 739 | Training Loss: 0.1478579418361187 | Validation Loss: 0.13811852857470514\n",
      "Epoch no.: 740 | Training Loss: 0.1476711269468069 | Validation Loss: 0.13808137997984887\n",
      "Epoch no.: 741 | Training Loss: 0.14771121107041835 | Validation Loss: 0.13797102868556976\n",
      "Epoch no.: 742 | Training Loss: 0.1477263715863228 | Validation Loss: 0.13802040591835976\n",
      "Epoch no.: 743 | Training Loss: 0.1478161884099245 | Validation Loss: 0.13829018622636796\n",
      "Epoch no.: 744 | Training Loss: 0.14778171576559543 | Validation Loss: 0.1380348525941372\n",
      "Epoch no.: 745 | Training Loss: 0.14768234640359879 | Validation Loss: 0.138067127764225\n",
      "Epoch no.: 746 | Training Loss: 0.14771193198859692 | Validation Loss: 0.13798787742853164\n",
      "Epoch no.: 747 | Training Loss: 0.14784719213843345 | Validation Loss: 0.13825448900461196\n",
      "Epoch no.: 748 | Training Loss: 0.14769224025309086 | Validation Loss: 0.1379515066742897\n",
      "Epoch no.: 749 | Training Loss: 0.14778396405279637 | Validation Loss: 0.13803548365831375\n",
      "Epoch no.: 750 | Training Loss: 0.14793578997254372 | Validation Loss: 0.13802670314908028\n",
      "Epoch no.: 751 | Training Loss: 0.14810772828757762 | Validation Loss: 0.13824139833450316\n",
      "Epoch no.: 752 | Training Loss: 0.14765120096504689 | Validation Loss: 0.13880274519324304\n",
      "Epoch no.: 753 | Training Loss: 0.14775283634662628 | Validation Loss: 0.13814071342349052\n",
      "Epoch no.: 754 | Training Loss: 0.14782298251986503 | Validation Loss: 0.1381479874253273\n",
      "Epoch no.: 755 | Training Loss: 0.147825800254941 | Validation Loss: 0.1380028583109379\n",
      "Epoch no.: 756 | Training Loss: 0.1477039685100317 | Validation Loss: 0.13841125294566153\n",
      "Epoch no.: 757 | Training Loss: 0.1476773104071617 | Validation Loss: 0.13805328235030173\n",
      "Epoch no.: 758 | Training Loss: 0.14766128316521646 | Validation Loss: 0.13859010487794876\n",
      "Epoch no.: 759 | Training Loss: 0.14779628917574883 | Validation Loss: 0.13799324706196786\n",
      "Epoch no.: 760 | Training Loss: 0.14789474852383136 | Validation Loss: 0.1380927838385105\n",
      "Epoch no.: 761 | Training Loss: 0.14775418438017368 | Validation Loss: 0.13810116648674012\n",
      "Epoch no.: 762 | Training Loss: 0.14770917616784573 | Validation Loss: 0.13799535855650902\n",
      "Epoch no.: 763 | Training Loss: 0.14777842044830322 | Validation Loss: 0.13807053864002228\n",
      "Epoch no.: 764 | Training Loss: 0.14782461747527123 | Validation Loss: 0.1386688992381096\n",
      "Epoch no.: 765 | Training Loss: 0.14777994059026242 | Validation Loss: 0.13821660801768304\n",
      "Epoch no.: 766 | Training Loss: 0.14776668332517148 | Validation Loss: 0.13816073313355445\n",
      "Epoch no.: 767 | Training Loss: 0.1477343004196882 | Validation Loss: 0.13826716616749762\n",
      "Epoch no.: 768 | Training Loss: 0.14790638387203217 | Validation Loss: 0.13902958258986473\n",
      "Epoch no.: 769 | Training Loss: 0.14801630534231663 | Validation Loss: 0.13829479292035102\n",
      "Epoch no.: 770 | Training Loss: 0.14774690821766853 | Validation Loss: 0.1390018008649349\n",
      "Epoch no.: 771 | Training Loss: 0.14780292451381682 | Validation Loss: 0.1384521134197712\n",
      "Epoch no.: 772 | Training Loss: 0.14776584059000014 | Validation Loss: 0.13807409405708312\n",
      "Epoch no.: 773 | Training Loss: 0.14781150095164775 | Validation Loss: 0.13796401023864746\n",
      "Epoch no.: 774 | Training Loss: 0.1478052119165659 | Validation Loss: 0.1380541853606701\n",
      "Epoch no.: 775 | Training Loss: 0.14780426271259783 | Validation Loss: 0.13808736354112625\n",
      "Epoch no.: 776 | Training Loss: 0.14772947199642658 | Validation Loss: 0.1379824124276638\n",
      "Epoch no.: 777 | Training Loss: 0.1477672056108713 | Validation Loss: 0.13887286558747292\n",
      "Epoch no.: 778 | Training Loss: 0.14776269868016242 | Validation Loss: 0.1389392986893654\n",
      "Epoch no.: 779 | Training Loss: 0.1477104365080595 | Validation Loss: 0.13824671879410744\n",
      "Epoch no.: 780 | Training Loss: 0.1478298033773899 | Validation Loss: 0.13810007274150848\n",
      "Epoch no.: 781 | Training Loss: 0.14779666803777217 | Validation Loss: 0.1380435049533844\n",
      "Epoch no.: 782 | Training Loss: 0.14783439628779887 | Validation Loss: 0.13934422358870507\n",
      "Epoch no.: 783 | Training Loss: 0.1477041841298342 | Validation Loss: 0.13869585320353509\n",
      "Epoch no.: 784 | Training Loss: 0.14781548000872136 | Validation Loss: 0.13916461765766144\n",
      "Epoch no.: 785 | Training Loss: 0.14773055002093316 | Validation Loss: 0.13810672089457512\n",
      "Epoch no.: 786 | Training Loss: 0.14784799881279467 | Validation Loss: 0.13877173364162446\n",
      "Epoch no.: 787 | Training Loss: 0.1480288728326559 | Validation Loss: 0.13819410800933837\n",
      "Epoch no.: 788 | Training Loss: 0.14768864199519158 | Validation Loss: 0.13834543004631997\n",
      "Epoch no.: 789 | Training Loss: 0.14785592168569564 | Validation Loss: 0.13832206949591636\n",
      "Epoch no.: 790 | Training Loss: 0.1477267322689295 | Validation Loss: 0.13818228915333747\n",
      "Epoch no.: 791 | Training Loss: 0.14780741915106774 | Validation Loss: 0.1379513829946518\n",
      "Epoch no.: 792 | Training Loss: 0.14798749178647996 | Validation Loss: 0.139057969301939\n",
      "Epoch no.: 793 | Training Loss: 0.1476536401361227 | Validation Loss: 0.13797335848212242\n",
      "Epoch no.: 794 | Training Loss: 0.14776211097836495 | Validation Loss: 0.13818478658795358\n",
      "Epoch no.: 795 | Training Loss: 0.14780992574989796 | Validation Loss: 0.13831599727272986\n",
      "Epoch no.: 796 | Training Loss: 0.14769646637141703 | Validation Loss: 0.13794219642877578\n",
      "Epoch no.: 797 | Training Loss: 0.14777340106666087 | Validation Loss: 0.1384674869477749\n",
      "Epoch no.: 798 | Training Loss: 0.14773782290518284 | Validation Loss: 0.1381775751709938\n",
      "Epoch no.: 799 | Training Loss: 0.14779677003622055 | Validation Loss: 0.1384898379445076\n",
      "Epoch no.: 800 | Training Loss: 0.14773956067860128 | Validation Loss: 0.13848182037472725\n",
      "Epoch no.: 801 | Training Loss: 0.14783623419702052 | Validation Loss: 0.1381429836153984\n",
      "Epoch no.: 802 | Training Loss: 0.14767102807760238 | Validation Loss: 0.13840800076723098\n",
      "Epoch no.: 803 | Training Loss: 0.14771893471479416 | Validation Loss: 0.13798331022262572\n",
      "Epoch no.: 804 | Training Loss: 0.14777811855077744 | Validation Loss: 0.13815699890255928\n",
      "Epoch no.: 805 | Training Loss: 0.14765592634677888 | Validation Loss: 0.1386764407157898\n",
      "Epoch no.: 806 | Training Loss: 0.1479004254192114 | Validation Loss: 0.13831583186984062\n",
      "Epoch no.: 807 | Training Loss: 0.14773984886705877 | Validation Loss: 0.13800507932901382\n",
      "Epoch no.: 808 | Training Loss: 0.14776492297649382 | Validation Loss: 0.13820973336696624\n",
      "Epoch no.: 809 | Training Loss: 0.14771174982190133 | Validation Loss: 0.13819413632154465\n",
      "Epoch no.: 810 | Training Loss: 0.1478764047473669 | Validation Loss: 0.1379564329981804\n",
      "Epoch no.: 811 | Training Loss: 0.14767444230616092 | Validation Loss: 0.1384670041501522\n",
      "Epoch no.: 812 | Training Loss: 0.1477338892221451 | Validation Loss: 0.13799375519156457\n",
      "Epoch no.: 813 | Training Loss: 0.1477927689254284 | Validation Loss: 0.1383854515850544\n",
      "Epoch no.: 814 | Training Loss: 0.14782032907009124 | Validation Loss: 0.13801628798246385\n",
      "Epoch no.: 815 | Training Loss: 0.1479051187634468 | Validation Loss: 0.13798628747463226\n",
      "Epoch no.: 816 | Training Loss: 0.14783746607601642 | Validation Loss: 0.13796896561980249\n",
      "Epoch no.: 817 | Training Loss: 0.14766879089176654 | Validation Loss: 0.13813106268644332\n",
      "Epoch no.: 818 | Training Loss: 0.14764638148248196 | Validation Loss: 0.14008987620472907\n",
      "Epoch no.: 819 | Training Loss: 0.14765632048249244 | Validation Loss: 0.13849014341831206\n",
      "Epoch no.: 820 | Training Loss: 0.14781121708452702 | Validation Loss: 0.13816600367426873\n",
      "Epoch no.: 821 | Training Loss: 0.14774917148053646 | Validation Loss: 0.13811601921916009\n",
      "Epoch no.: 822 | Training Loss: 0.14771730355918408 | Validation Loss: 0.1379576250910759\n",
      "Epoch no.: 823 | Training Loss: 0.14778077721595764 | Validation Loss: 0.1381327487528324\n",
      "Epoch no.: 824 | Training Loss: 0.14770280495285987 | Validation Loss: 0.13800999224185945\n",
      "Epoch no.: 825 | Training Loss: 0.1476335058361292 | Validation Loss: 0.13829301446676254\n",
      "Epoch no.: 826 | Training Loss: 0.14776964031159878 | Validation Loss: 0.13844178020954132\n",
      "Epoch no.: 827 | Training Loss: 0.14777544483542443 | Validation Loss: 0.1379656620323658\n",
      "Epoch no.: 828 | Training Loss: 0.14781711310148238 | Validation Loss: 0.13803451955318452\n",
      "Epoch no.: 829 | Training Loss: 0.14776988618075848 | Validation Loss: 0.13802170157432556\n",
      "Epoch no.: 830 | Training Loss: 0.14763643831014633 | Validation Loss: 0.13795488327741623\n",
      "Epoch no.: 831 | Training Loss: 0.14780576765537262 | Validation Loss: 0.13800387978553771\n",
      "Epoch no.: 832 | Training Loss: 0.14771603591740132 | Validation Loss: 0.1379589132964611\n",
      "Epoch no.: 833 | Training Loss: 0.1477368625998497 | Validation Loss: 0.13809753209352493\n",
      "Epoch no.: 834 | Training Loss: 0.1477748380601406 | Validation Loss: 0.13882973715662955\n",
      "Epoch no.: 835 | Training Loss: 0.14764581695199014 | Validation Loss: 0.13839019313454629\n",
      "Epoch no.: 836 | Training Loss: 0.14775940239429475 | Validation Loss: 0.13835797235369682\n",
      "Epoch no.: 837 | Training Loss: 0.14779603406786918 | Validation Loss: 0.1382616266608238\n",
      "Epoch no.: 838 | Training Loss: 0.1478065310418606 | Validation Loss: 0.13796056881546975\n",
      "Epoch no.: 839 | Training Loss: 0.14794780142605304 | Validation Loss: 0.13813642784953117\n",
      "Epoch no.: 840 | Training Loss: 0.14774166971445082 | Validation Loss: 0.13802777081727982\n",
      "Epoch no.: 841 | Training Loss: 0.14771570734679698 | Validation Loss: 0.13796332851052284\n",
      "Epoch no.: 842 | Training Loss: 0.1478007260710001 | Validation Loss: 0.13815131187438964\n",
      "Epoch no.: 843 | Training Loss: 0.14794731736183167 | Validation Loss: 0.1379767820239067\n",
      "Epoch no.: 844 | Training Loss: 0.14768826998770238 | Validation Loss: 0.13803744614124297\n",
      "Epoch no.: 845 | Training Loss: 0.14770242623984814 | Validation Loss: 0.13797487169504166\n",
      "Epoch no.: 846 | Training Loss: 0.1478529342263937 | Validation Loss: 0.13807826787233352\n",
      "Epoch no.: 847 | Training Loss: 0.14772753939032554 | Validation Loss: 0.13849759101867676\n",
      "Epoch no.: 848 | Training Loss: 0.1477915857732296 | Validation Loss: 0.1381226360797882\n",
      "Epoch no.: 849 | Training Loss: 0.1479024276137352 | Validation Loss: 0.13804176077246666\n",
      "Epoch no.: 850 | Training Loss: 0.1477370858937502 | Validation Loss: 0.13799724280834197\n",
      "Epoch no.: 851 | Training Loss: 0.14780082292854785 | Validation Loss: 0.13812655434012414\n",
      "Epoch no.: 852 | Training Loss: 0.14773191057145596 | Validation Loss: 0.13802566677331923\n",
      "Epoch no.: 853 | Training Loss: 0.14778292164206505 | Validation Loss: 0.13819993510842324\n",
      "Epoch no.: 854 | Training Loss: 0.14797087006270884 | Validation Loss: 0.13840198740363122\n",
      "Epoch no.: 855 | Training Loss: 0.14770454689860343 | Validation Loss: 0.13873727321624757\n",
      "Epoch no.: 856 | Training Loss: 0.14777128368616105 | Validation Loss: 0.13819054737687111\n",
      "Epoch no.: 857 | Training Loss: 0.1477366343140602 | Validation Loss: 0.13797878921031953\n",
      "Epoch no.: 858 | Training Loss: 0.14779037550091745 | Validation Loss: 0.13913552984595298\n",
      "Epoch no.: 859 | Training Loss: 0.14780359797179699 | Validation Loss: 0.13809592872858048\n",
      "Epoch no.: 860 | Training Loss: 0.14778609193861483 | Validation Loss: 0.13800767362117766\n",
      "Epoch no.: 861 | Training Loss: 0.14774619482457638 | Validation Loss: 0.13841464817523957\n",
      "Epoch no.: 862 | Training Loss: 0.14775789298117162 | Validation Loss: 0.13864941000938416\n",
      "Epoch no.: 863 | Training Loss: 0.1479431316256523 | Validation Loss: 0.13833119124174117\n",
      "Epoch no.: 864 | Training Loss: 0.14776535600423812 | Validation Loss: 0.13796650916337966\n",
      "Epoch no.: 865 | Training Loss: 0.14773056603968143 | Validation Loss: 0.13796904310584068\n",
      "Epoch no.: 866 | Training Loss: 0.1477903988212347 | Validation Loss: 0.13892291560769082\n",
      "Epoch no.: 867 | Training Loss: 0.1476883287727833 | Validation Loss: 0.13801426514983178\n",
      "Epoch no.: 868 | Training Loss: 0.1476273562014103 | Validation Loss: 0.13798350244760513\n",
      "Epoch no.: 869 | Training Loss: 0.14781706690788268 | Validation Loss: 0.13799777179956435\n",
      "Epoch no.: 870 | Training Loss: 0.14765900529921056 | Validation Loss: 0.1380363255739212\n",
      "Epoch no.: 871 | Training Loss: 0.14777593217790128 | Validation Loss: 0.1379472829401493\n",
      "Epoch no.: 872 | Training Loss: 0.14781964413821697 | Validation Loss: 0.13808602914214135\n",
      "Epoch no.: 873 | Training Loss: 0.1477129039913416 | Validation Loss: 0.13832529112696648\n",
      "Epoch no.: 874 | Training Loss: 0.14772509969770908 | Validation Loss: 0.13801643624901772\n",
      "Epoch no.: 875 | Training Loss: 0.1477500692009926 | Validation Loss: 0.13815078437328338\n",
      "Epoch no.: 876 | Training Loss: 0.14771724201738834 | Validation Loss: 0.13810400739312173\n",
      "Epoch no.: 877 | Training Loss: 0.14774577535688876 | Validation Loss: 0.13799014687538147\n",
      "Epoch no.: 878 | Training Loss: 0.14776501424610614 | Validation Loss: 0.13923479840159417\n",
      "Epoch no.: 879 | Training Loss: 0.1477151594310999 | Validation Loss: 0.1384752355515957\n",
      "Epoch no.: 880 | Training Loss: 0.14777756750583648 | Validation Loss: 0.138116105645895\n",
      "Epoch no.: 881 | Training Loss: 0.14770086348056793 | Validation Loss: 0.13799688816070557\n",
      "Epoch no.: 882 | Training Loss: 0.14788644589483738 | Validation Loss: 0.13833024650812148\n",
      "Epoch no.: 883 | Training Loss: 0.14770730569958687 | Validation Loss: 0.13806932121515275\n",
      "Epoch no.: 884 | Training Loss: 0.1479079932719469 | Validation Loss: 0.13846941590309142\n",
      "Epoch no.: 885 | Training Loss: 0.14789934128522872 | Validation Loss: 0.13812218829989434\n",
      "Epoch no.: 886 | Training Loss: 0.14770269818603993 | Validation Loss: 0.13825179189443587\n",
      "Epoch no.: 887 | Training Loss: 0.1477345894277096 | Validation Loss: 0.13814667537808417\n",
      "Epoch no.: 888 | Training Loss: 0.14788409627974033 | Validation Loss: 0.13868674114346505\n",
      "Epoch no.: 889 | Training Loss: 0.1479377430677414 | Validation Loss: 0.13805817812681198\n",
      "Epoch no.: 890 | Training Loss: 0.14771958865225315 | Validation Loss: 0.13842710703611374\n",
      "Epoch no.: 891 | Training Loss: 0.1477945839613676 | Validation Loss: 0.13854246065020562\n",
      "Epoch no.: 892 | Training Loss: 0.14771497912704945 | Validation Loss: 0.13794379979372023\n",
      "Epoch no.: 893 | Training Loss: 0.14773341581225397 | Validation Loss: 0.13802927359938622\n",
      "Epoch no.: 894 | Training Loss: 0.14772129647433757 | Validation Loss: 0.1384408250451088\n",
      "Epoch no.: 895 | Training Loss: 0.1477148215472698 | Validation Loss: 0.13832715451717376\n",
      "Epoch no.: 896 | Training Loss: 0.14774840727448463 | Validation Loss: 0.13810385763645172\n",
      "Epoch no.: 897 | Training Loss: 0.147791363671422 | Validation Loss: 0.1383096843957901\n",
      "Epoch no.: 898 | Training Loss: 0.14768314234912394 | Validation Loss: 0.13798872083425523\n",
      "Epoch no.: 899 | Training Loss: 0.14794292867183687 | Validation Loss: 0.1381740912795067\n",
      "Epoch no.: 900 | Training Loss: 0.14772074148058892 | Validation Loss: 0.13815195634961128\n",
      "Epoch no.: 901 | Training Loss: 0.14773402370512487 | Validation Loss: 0.1379904106259346\n",
      "Epoch no.: 902 | Training Loss: 0.14782533407211304 | Validation Loss: 0.13849415630102158\n",
      "Epoch no.: 903 | Training Loss: 0.14774574883282185 | Validation Loss: 0.13801447078585624\n",
      "Epoch no.: 904 | Training Loss: 0.14772131703794003 | Validation Loss: 0.13844363689422606\n",
      "Epoch no.: 905 | Training Loss: 0.14757433742284776 | Validation Loss: 0.13836369514465333\n",
      "Epoch no.: 906 | Training Loss: 0.14784597150981427 | Validation Loss: 0.1380302257835865\n",
      "Epoch no.: 907 | Training Loss: 0.147789159566164 | Validation Loss: 0.13888985216617583\n",
      "Epoch no.: 908 | Training Loss: 0.14779479816555977 | Validation Loss: 0.13817653059959412\n",
      "Epoch no.: 909 | Training Loss: 0.14773321323096752 | Validation Loss: 0.1381452575325966\n",
      "Epoch no.: 910 | Training Loss: 0.14776984430849552 | Validation Loss: 0.13845939710736274\n",
      "Epoch no.: 911 | Training Loss: 0.14773015148937702 | Validation Loss: 0.1381258562207222\n",
      "Epoch no.: 912 | Training Loss: 0.1476626391708851 | Validation Loss: 0.13837298527359962\n",
      "Epoch no.: 913 | Training Loss: 0.14774684652686118 | Validation Loss: 0.1381673149764538\n",
      "Epoch no.: 914 | Training Loss: 0.14783475436270238 | Validation Loss: 0.13821375072002412\n",
      "Epoch no.: 915 | Training Loss: 0.14777675934135914 | Validation Loss: 0.13863477036356925\n",
      "Epoch no.: 916 | Training Loss: 0.14778326503932476 | Validation Loss: 0.13886305689811707\n",
      "Epoch no.: 917 | Training Loss: 0.1478267364948988 | Validation Loss: 0.13826312348246575\n",
      "Epoch no.: 918 | Training Loss: 0.1478473324328661 | Validation Loss: 0.13799915686249734\n",
      "Epoch no.: 919 | Training Loss: 0.14772589467465877 | Validation Loss: 0.13807331547141075\n",
      "Epoch no.: 920 | Training Loss: 0.14775005742907524 | Validation Loss: 0.13805911988019942\n",
      "Epoch no.: 921 | Training Loss: 0.1477223065495491 | Validation Loss: 0.13819699883460998\n",
      "Epoch no.: 922 | Training Loss: 0.14772878721356392 | Validation Loss: 0.13805629536509514\n",
      "Epoch no.: 923 | Training Loss: 0.1477706441283226 | Validation Loss: 0.13811935260891914\n",
      "Epoch no.: 924 | Training Loss: 0.14768030762672424 | Validation Loss: 0.13797652944922448\n",
      "Epoch no.: 925 | Training Loss: 0.14775356635451317 | Validation Loss: 0.1386737823486328\n",
      "Epoch no.: 926 | Training Loss: 0.14767730832099915 | Validation Loss: 0.13818711042404175\n",
      "Epoch no.: 927 | Training Loss: 0.1476679827272892 | Validation Loss: 0.13800106793642045\n",
      "Epoch no.: 928 | Training Loss: 0.14767162345349788 | Validation Loss: 0.1380302391946316\n",
      "Epoch no.: 929 | Training Loss: 0.14785327203571796 | Validation Loss: 0.13842099383473397\n",
      "Epoch no.: 930 | Training Loss: 0.14774539411067963 | Validation Loss: 0.1382512405514717\n",
      "Epoch no.: 931 | Training Loss: 0.14771310582756997 | Validation Loss: 0.13834252208471298\n",
      "Epoch no.: 932 | Training Loss: 0.14769629560410977 | Validation Loss: 0.13827425837516785\n",
      "Epoch no.: 933 | Training Loss: 0.1477388161420822 | Validation Loss: 0.138417749106884\n",
      "Epoch no.: 934 | Training Loss: 0.147733016833663 | Validation Loss: 0.1380895234644413\n",
      "Epoch no.: 935 | Training Loss: 0.14765994124114512 | Validation Loss: 0.13882382586598396\n",
      "Epoch no.: 936 | Training Loss: 0.14784016564488411 | Validation Loss: 0.138020870834589\n",
      "Epoch no.: 937 | Training Loss: 0.14775107994675638 | Validation Loss: 0.1383240230381489\n",
      "Epoch no.: 938 | Training Loss: 0.14767909690737724 | Validation Loss: 0.1379546843469143\n",
      "Epoch no.: 939 | Training Loss: 0.147703430429101 | Validation Loss: 0.13817313984036445\n",
      "Epoch no.: 940 | Training Loss: 0.14797261394560338 | Validation Loss: 0.13822220489382744\n",
      "Epoch no.: 941 | Training Loss: 0.1476551154255867 | Validation Loss: 0.13806708604097367\n",
      "Epoch no.: 942 | Training Loss: 0.14786299593746663 | Validation Loss: 0.13836255371570588\n",
      "Epoch no.: 943 | Training Loss: 0.1476634120941162 | Validation Loss: 0.1383502535521984\n",
      "Epoch no.: 944 | Training Loss: 0.14767178378999232 | Validation Loss: 0.1379734084010124\n",
      "Epoch no.: 945 | Training Loss: 0.1477458167821169 | Validation Loss: 0.13908937498927115\n",
      "Epoch no.: 946 | Training Loss: 0.147786813005805 | Validation Loss: 0.13816444724798202\n",
      "Epoch no.: 947 | Training Loss: 0.14773759700357914 | Validation Loss: 0.13817303031682968\n",
      "Epoch no.: 948 | Training Loss: 0.1476630099862814 | Validation Loss: 0.1390625774860382\n",
      "Epoch no.: 949 | Training Loss: 0.14781580328941346 | Validation Loss: 0.13870637640357017\n",
      "Epoch no.: 950 | Training Loss: 0.14786296837031843 | Validation Loss: 0.13849485144019127\n",
      "Epoch no.: 951 | Training Loss: 0.14771831706166266 | Validation Loss: 0.13813432008028032\n",
      "Epoch no.: 952 | Training Loss: 0.147734886854887 | Validation Loss: 0.138216782361269\n",
      "Epoch no.: 953 | Training Loss: 0.147870754301548 | Validation Loss: 0.1380317196249962\n",
      "Epoch no.: 954 | Training Loss: 0.14786118313670157 | Validation Loss: 0.13797198608517647\n",
      "Epoch no.: 955 | Training Loss: 0.1477756145596504 | Validation Loss: 0.13810754269361497\n",
      "Epoch no.: 956 | Training Loss: 0.14778060622513295 | Validation Loss: 0.1386807754635811\n",
      "Epoch no.: 957 | Training Loss: 0.14766325183212758 | Validation Loss: 0.13830744177103044\n",
      "Epoch no.: 958 | Training Loss: 0.14793216601014136 | Validation Loss: 0.13813867121934892\n",
      "Epoch no.: 959 | Training Loss: 0.1477752784639597 | Validation Loss: 0.13799962326884269\n",
      "Epoch no.: 960 | Training Loss: 0.1477383130788803 | Validation Loss: 0.13795980140566827\n",
      "Epoch no.: 961 | Training Loss: 0.1477018854767084 | Validation Loss: 0.13803140670061112\n",
      "Epoch no.: 962 | Training Loss: 0.14772084034979344 | Validation Loss: 0.13846612125635147\n",
      "Epoch no.: 963 | Training Loss: 0.1476915355026722 | Validation Loss: 0.13816042244434357\n",
      "Epoch no.: 964 | Training Loss: 0.1477940075099468 | Validation Loss: 0.13810112178325654\n",
      "Epoch no.: 965 | Training Loss: 0.14777241989970208 | Validation Loss: 0.13842958882451056\n",
      "Epoch no.: 966 | Training Loss: 0.14776071697473525 | Validation Loss: 0.13841453120112418\n",
      "Epoch no.: 967 | Training Loss: 0.14779463790357114 | Validation Loss: 0.1381385311484337\n",
      "Epoch no.: 968 | Training Loss: 0.1477329221367836 | Validation Loss: 0.137993723154068\n",
      "Epoch no.: 969 | Training Loss: 0.14770313255488873 | Validation Loss: 0.13807214051485062\n",
      "Epoch no.: 970 | Training Loss: 0.14778064839541913 | Validation Loss: 0.13800884112715722\n",
      "Epoch no.: 971 | Training Loss: 0.14780842915177345 | Validation Loss: 0.13839440643787385\n",
      "Epoch no.: 972 | Training Loss: 0.1478275530785322 | Validation Loss: 0.1379849210381508\n",
      "Epoch no.: 973 | Training Loss: 0.14762578800320625 | Validation Loss: 0.13813803642988204\n",
      "Epoch no.: 974 | Training Loss: 0.14774494826793672 | Validation Loss: 0.13833287134766578\n",
      "Epoch no.: 975 | Training Loss: 0.14772106997668744 | Validation Loss: 0.13795721158385277\n",
      "Epoch no.: 976 | Training Loss: 0.1477177492529154 | Validation Loss: 0.13796782046556472\n",
      "Epoch no.: 977 | Training Loss: 0.14762941271066665 | Validation Loss: 0.13845053613185881\n",
      "Epoch no.: 978 | Training Loss: 0.14783694714307785 | Validation Loss: 0.13798843771219255\n",
      "Epoch no.: 979 | Training Loss: 0.14772185109555722 | Validation Loss: 0.13807313814759253\n",
      "Epoch no.: 980 | Training Loss: 0.14768589399755 | Validation Loss: 0.138361906260252\n",
      "Epoch no.: 981 | Training Loss: 0.1477561604231596 | Validation Loss: 0.13816845789551735\n",
      "Epoch no.: 982 | Training Loss: 0.14790043398737907 | Validation Loss: 0.1379642330110073\n",
      "Epoch no.: 983 | Training Loss: 0.14767778009176255 | Validation Loss: 0.1379566691815853\n",
      "Epoch no.: 984 | Training Loss: 0.14767693772912024 | Validation Loss: 0.13832131624221802\n",
      "Epoch no.: 985 | Training Loss: 0.14772723712027072 | Validation Loss: 0.13850039020180702\n",
      "Epoch no.: 986 | Training Loss: 0.14772854410111905 | Validation Loss: 0.13840804398059844\n",
      "Epoch no.: 987 | Training Loss: 0.14781548254191876 | Validation Loss: 0.1379891127347946\n",
      "Epoch no.: 988 | Training Loss: 0.14765522360801697 | Validation Loss: 0.13800610229372978\n",
      "Epoch no.: 989 | Training Loss: 0.14770571619272232 | Validation Loss: 0.13809639662504197\n",
      "Epoch no.: 990 | Training Loss: 0.14778783313930036 | Validation Loss: 0.1381763346493244\n",
      "Epoch no.: 991 | Training Loss: 0.1477280130982399 | Validation Loss: 0.1384894385933876\n",
      "Epoch no.: 992 | Training Loss: 0.14776500545442103 | Validation Loss: 0.13812851905822754\n",
      "Epoch no.: 993 | Training Loss: 0.14786417588591574 | Validation Loss: 0.1381254456937313\n",
      "Epoch no.: 994 | Training Loss: 0.14778055287897587 | Validation Loss: 0.13804492652416228\n",
      "Epoch no.: 995 | Training Loss: 0.1477775952965021 | Validation Loss: 0.13795215114951134\n",
      "Epoch no.: 996 | Training Loss: 0.1477519279718399 | Validation Loss: 0.13823374211788178\n",
      "Epoch no.: 997 | Training Loss: 0.1477438785880804 | Validation Loss: 0.13849431350827218\n",
      "Epoch no.: 998 | Training Loss: 0.1479643912613392 | Validation Loss: 0.13812355175614358\n",
      "Epoch no.: 999 | Training Loss: 0.14772421583533288 | Validation Loss: 0.13802491649985313\n",
      "Epoch no.: 1000 | Training Loss: 0.14782964825630188 | Validation Loss: 0.13795081228017808\n"
     ]
    }
   ],
   "source": [
    "lstmnet_start_time = time.time()\n",
    "train_and_validate(epochs, device, lstmnet, train_loader, val_loader, criterion, lstmnet_optimizer, lstmnet_train_loss_array, lstmnet_val_loss_array)\n",
    "lstmnet_end_time = time.time()\n",
    "lstmnet_total_training_time = lstmnet_end_time - lstmnet_start_time\n",
    "lstmnet_avg_training_time_per_epoch = lstmnet_total_training_time / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb145f-639a-426d-9b36-ffd6500259b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no.: 1 | Training Loss: 0.5832641530036926 | Validation Loss: 0.3358241647481918\n",
      "Epoch no.: 2 | Training Loss: 0.3178152504563332 | Validation Loss: 0.30462291538715364\n",
      "Epoch no.: 3 | Training Loss: 0.28964419081807136 | Validation Loss: 0.2714419111609459\n",
      "Epoch no.: 4 | Training Loss: 0.25578171372413633 | Validation Loss: 0.2324933871626854\n",
      "Epoch no.: 5 | Training Loss: 0.222101067006588 | Validation Loss: 0.20115545839071275\n",
      "Epoch no.: 6 | Training Loss: 0.19876268535852432 | Validation Loss: 0.18321079015731812\n",
      "Epoch no.: 7 | Training Loss: 0.18663244307041169 | Validation Loss: 0.1737976387143135\n",
      "Epoch no.: 8 | Training Loss: 0.17889287784695626 | Validation Loss: 0.16673085689544678\n",
      "Epoch no.: 9 | Training Loss: 0.17249054580926895 | Validation Loss: 0.16090157330036164\n",
      "Epoch no.: 10 | Training Loss: 0.1675539018213749 | Validation Loss: 0.15585823208093644\n",
      "Epoch no.: 11 | Training Loss: 0.16364275246858598 | Validation Loss: 0.15246070325374603\n",
      "Epoch no.: 12 | Training Loss: 0.16065698496997358 | Validation Loss: 0.15044721364974975\n",
      "Epoch no.: 13 | Training Loss: 0.15879375889897346 | Validation Loss: 0.14776179641485215\n",
      "Epoch no.: 14 | Training Loss: 0.1573442180454731 | Validation Loss: 0.14637218564748763\n",
      "Epoch no.: 15 | Training Loss: 0.15600126400589942 | Validation Loss: 0.1455719366669655\n",
      "Epoch no.: 16 | Training Loss: 0.15493770368397236 | Validation Loss: 0.14436710700392724\n",
      "Epoch no.: 17 | Training Loss: 0.15436553098261357 | Validation Loss: 0.14454096779227257\n",
      "Epoch no.: 18 | Training Loss: 0.15356977716088294 | Validation Loss: 0.14293909817934036\n",
      "Epoch no.: 19 | Training Loss: 0.15286297656595707 | Validation Loss: 0.14289259612560273\n",
      "Epoch no.: 20 | Training Loss: 0.15235844641923904 | Validation Loss: 0.14166297465562822\n",
      "Epoch no.: 21 | Training Loss: 0.15196976572275162 | Validation Loss: 0.14175090044736863\n",
      "Epoch no.: 22 | Training Loss: 0.1515365518629551 | Validation Loss: 0.14226410463452338\n",
      "Epoch no.: 23 | Training Loss: 0.15121569387614728 | Validation Loss: 0.14057195112109183\n",
      "Epoch no.: 24 | Training Loss: 0.1510521486401558 | Validation Loss: 0.14065819308161737\n",
      "Epoch no.: 25 | Training Loss: 0.1508771526068449 | Validation Loss: 0.14002496674656867\n",
      "Epoch no.: 26 | Training Loss: 0.15042270556092263 | Validation Loss: 0.13980869203805923\n",
      "Epoch no.: 27 | Training Loss: 0.15018572114408016 | Validation Loss: 0.14082780480384827\n",
      "Epoch no.: 28 | Training Loss: 0.15003649167716504 | Validation Loss: 0.1392991177737713\n",
      "Epoch no.: 29 | Training Loss: 0.14981795705854892 | Validation Loss: 0.14101543948054313\n",
      "Epoch no.: 30 | Training Loss: 0.14972551472485066 | Validation Loss: 0.1397077701985836\n",
      "Epoch no.: 31 | Training Loss: 0.1494556936621666 | Validation Loss: 0.13913730755448342\n",
      "Epoch no.: 32 | Training Loss: 0.1494051230698824 | Validation Loss: 0.1387623131275177\n",
      "Epoch no.: 33 | Training Loss: 0.1492227479815483 | Validation Loss: 0.1386904872953892\n",
      "Epoch no.: 34 | Training Loss: 0.1492083927989006 | Validation Loss: 0.13989422395825385\n",
      "Epoch no.: 35 | Training Loss: 0.14901522874832154 | Validation Loss: 0.13847876638174056\n",
      "Epoch no.: 36 | Training Loss: 0.14894360408186913 | Validation Loss: 0.13903888761997224\n",
      "Epoch no.: 37 | Training Loss: 0.14879575185477734 | Validation Loss: 0.13849605098366738\n",
      "Epoch no.: 38 | Training Loss: 0.1486949183046818 | Validation Loss: 0.13843896687030793\n",
      "Epoch no.: 39 | Training Loss: 0.14872102670371531 | Validation Loss: 0.1392788276076317\n",
      "Epoch no.: 40 | Training Loss: 0.14854760698974132 | Validation Loss: 0.13998219147324562\n",
      "Epoch no.: 41 | Training Loss: 0.14854683466255664 | Validation Loss: 0.13865363523364066\n",
      "Epoch no.: 42 | Training Loss: 0.1485191919654608 | Validation Loss: 0.13810622245073317\n",
      "Epoch no.: 43 | Training Loss: 0.148526506498456 | Validation Loss: 0.1379151463508606\n",
      "Epoch no.: 44 | Training Loss: 0.14844046548008918 | Validation Loss: 0.13828063607215882\n",
      "Epoch no.: 45 | Training Loss: 0.1482743615657091 | Validation Loss: 0.13834257498383523\n",
      "Epoch no.: 46 | Training Loss: 0.1484128701686859 | Validation Loss: 0.1381600059568882\n",
      "Epoch no.: 47 | Training Loss: 0.1482133310288191 | Validation Loss: 0.1383105218410492\n",
      "Epoch no.: 48 | Training Loss: 0.1483120660483837 | Validation Loss: 0.13800174221396447\n",
      "Epoch no.: 49 | Training Loss: 0.14818879589438438 | Validation Loss: 0.13854453712701797\n",
      "Epoch no.: 50 | Training Loss: 0.14827358447015285 | Validation Loss: 0.13841433376073836\n",
      "Epoch no.: 51 | Training Loss: 0.14822822086513043 | Validation Loss: 0.13785239905118943\n",
      "Epoch no.: 52 | Training Loss: 0.14819624297320844 | Validation Loss: 0.13844723179936408\n",
      "Epoch no.: 53 | Training Loss: 0.14814323715865613 | Validation Loss: 0.13784863725304602\n",
      "Epoch no.: 54 | Training Loss: 0.14807847484946252 | Validation Loss: 0.13795885145664216\n",
      "Epoch no.: 55 | Training Loss: 0.14815887115895748 | Validation Loss: 0.13779882788658143\n",
      "Epoch no.: 56 | Training Loss: 0.14807818174362183 | Validation Loss: 0.13782832473516465\n",
      "Epoch no.: 57 | Training Loss: 0.1483261113613844 | Validation Loss: 0.13794956281781195\n",
      "Epoch no.: 58 | Training Loss: 0.14806245341897012 | Validation Loss: 0.13779870867729188\n",
      "Epoch no.: 59 | Training Loss: 0.14808207884430885 | Validation Loss: 0.13766987174749373\n",
      "Epoch no.: 60 | Training Loss: 0.1481643284112215 | Validation Loss: 0.13829461708664895\n",
      "Epoch no.: 61 | Training Loss: 0.14805908769369125 | Validation Loss: 0.13778403922915458\n",
      "Epoch no.: 62 | Training Loss: 0.14800155572593213 | Validation Loss: 0.13774753212928773\n",
      "Epoch no.: 63 | Training Loss: 0.14791938714683056 | Validation Loss: 0.13816738948225976\n",
      "Epoch no.: 64 | Training Loss: 0.14803231343626977 | Validation Loss: 0.13766828775405884\n",
      "Epoch no.: 65 | Training Loss: 0.14794061042368412 | Validation Loss: 0.13801379054784774\n",
      "Epoch no.: 66 | Training Loss: 0.14802611172199248 | Validation Loss: 0.1377798467874527\n",
      "Epoch no.: 67 | Training Loss: 0.1479087868332863 | Validation Loss: 0.13799750059843063\n",
      "Epoch no.: 68 | Training Loss: 0.14789717338979244 | Validation Loss: 0.13758441507816316\n",
      "Epoch no.: 69 | Training Loss: 0.14788298048079013 | Validation Loss: 0.137970632314682\n",
      "Epoch no.: 70 | Training Loss: 0.14788180634379386 | Validation Loss: 0.13804429471492768\n",
      "Epoch no.: 71 | Training Loss: 0.14790549896657468 | Validation Loss: 0.1376112848520279\n",
      "Epoch no.: 72 | Training Loss: 0.14782125525176526 | Validation Loss: 0.1386605553328991\n",
      "Epoch no.: 73 | Training Loss: 0.14799370519816876 | Validation Loss: 0.1378407098352909\n",
      "Epoch no.: 74 | Training Loss: 0.14807134188711643 | Validation Loss: 0.13763313367962837\n",
      "Epoch no.: 75 | Training Loss: 0.1478762970864773 | Validation Loss: 0.13794420436024665\n",
      "Epoch no.: 76 | Training Loss: 0.1478530563414097 | Validation Loss: 0.1376216858625412\n",
      "Epoch no.: 77 | Training Loss: 0.1480029982328415 | Validation Loss: 0.1377919301390648\n",
      "Epoch no.: 78 | Training Loss: 0.14790623746812342 | Validation Loss: 0.13811365216970445\n",
      "Epoch no.: 79 | Training Loss: 0.1477206039428711 | Validation Loss: 0.13758602738380432\n",
      "Epoch no.: 80 | Training Loss: 0.14774771824479102 | Validation Loss: 0.13774922341108323\n",
      "Epoch no.: 81 | Training Loss: 0.14779539324343205 | Validation Loss: 0.13808759599924086\n",
      "Epoch no.: 82 | Training Loss: 0.14783791705965996 | Validation Loss: 0.13768966421484946\n",
      "Epoch no.: 83 | Training Loss: 0.1478983871638775 | Validation Loss: 0.13811371400952338\n",
      "Epoch no.: 84 | Training Loss: 0.14779959857463837 | Validation Loss: 0.13783165588974952\n",
      "Epoch no.: 85 | Training Loss: 0.14781727008521556 | Validation Loss: 0.1379853218793869\n",
      "Epoch no.: 86 | Training Loss: 0.14777762107551098 | Validation Loss: 0.13762711361050606\n",
      "Epoch no.: 87 | Training Loss: 0.14791482850909232 | Validation Loss: 0.1378500558435917\n",
      "Epoch no.: 88 | Training Loss: 0.1478260273486376 | Validation Loss: 0.137811853736639\n",
      "Epoch no.: 89 | Training Loss: 0.14788170583546162 | Validation Loss: 0.13782468363642691\n",
      "Epoch no.: 90 | Training Loss: 0.1479264198243618 | Validation Loss: 0.13797041177749633\n",
      "Epoch no.: 91 | Training Loss: 0.14780049838125706 | Validation Loss: 0.13815210983157158\n",
      "Epoch no.: 92 | Training Loss: 0.14777507573366166 | Validation Loss: 0.13826760053634643\n",
      "Epoch no.: 93 | Training Loss: 0.14781919054687023 | Validation Loss: 0.13795030266046523\n",
      "Epoch no.: 94 | Training Loss: 0.1478229983150959 | Validation Loss: 0.13778234422206878\n",
      "Epoch no.: 95 | Training Loss: 0.14773315131664277 | Validation Loss: 0.1379297509789467\n",
      "Epoch no.: 96 | Training Loss: 0.14773658834397793 | Validation Loss: 0.13793160691857337\n",
      "Epoch no.: 97 | Training Loss: 0.14776619248092174 | Validation Loss: 0.13764195516705513\n",
      "Epoch no.: 98 | Training Loss: 0.14785463899374007 | Validation Loss: 0.1379461444914341\n",
      "Epoch no.: 99 | Training Loss: 0.14788383930921556 | Validation Loss: 0.137612384557724\n",
      "Epoch no.: 100 | Training Loss: 0.14772522032260896 | Validation Loss: 0.13772593587636947\n",
      "Epoch no.: 101 | Training Loss: 0.14775764741003514 | Validation Loss: 0.13797704949975015\n",
      "Epoch no.: 102 | Training Loss: 0.14770318858325482 | Validation Loss: 0.13770804107189177\n",
      "Epoch no.: 103 | Training Loss: 0.1477829623967409 | Validation Loss: 0.13759826198220254\n",
      "Epoch no.: 104 | Training Loss: 0.1478870300203562 | Validation Loss: 0.13772065341472625\n",
      "Epoch no.: 105 | Training Loss: 0.14769807271659374 | Validation Loss: 0.13767028972506523\n",
      "Epoch no.: 106 | Training Loss: 0.1477569168061018 | Validation Loss: 0.1377046823501587\n",
      "Epoch no.: 107 | Training Loss: 0.14781729839742183 | Validation Loss: 0.13799119666218757\n",
      "Epoch no.: 108 | Training Loss: 0.147761457413435 | Validation Loss: 0.137854515761137\n",
      "Epoch no.: 109 | Training Loss: 0.1477105562388897 | Validation Loss: 0.1381804823875427\n",
      "Epoch no.: 110 | Training Loss: 0.14771690361201764 | Validation Loss: 0.13771472573280336\n",
      "Epoch no.: 111 | Training Loss: 0.147654801979661 | Validation Loss: 0.13778808563947678\n",
      "Epoch no.: 112 | Training Loss: 0.1477457343041897 | Validation Loss: 0.13772891238331794\n",
      "Epoch no.: 113 | Training Loss: 0.14773553341627121 | Validation Loss: 0.1381797932088375\n",
      "Epoch no.: 114 | Training Loss: 0.14768892996013164 | Validation Loss: 0.13771600052714347\n",
      "Epoch no.: 115 | Training Loss: 0.14774357415735723 | Validation Loss: 0.13773268088698387\n",
      "Epoch no.: 116 | Training Loss: 0.14772445015609265 | Validation Loss: 0.13773901239037514\n",
      "Epoch no.: 117 | Training Loss: 0.1477007780224085 | Validation Loss: 0.1377388872206211\n",
      "Epoch no.: 118 | Training Loss: 0.1478106515109539 | Validation Loss: 0.13766922578215599\n",
      "Epoch no.: 119 | Training Loss: 0.147710300385952 | Validation Loss: 0.13764680325984954\n",
      "Epoch no.: 120 | Training Loss: 0.14773721903562545 | Validation Loss: 0.13783345967531205\n",
      "Epoch no.: 121 | Training Loss: 0.1476867275685072 | Validation Loss: 0.1382812686264515\n",
      "Epoch no.: 122 | Training Loss: 0.1477337484806776 | Validation Loss: 0.13791978359222412\n",
      "Epoch no.: 123 | Training Loss: 0.14768369495868683 | Validation Loss: 0.13775458112359046\n",
      "Epoch no.: 124 | Training Loss: 0.14768604099750518 | Validation Loss: 0.1377203106880188\n",
      "Epoch no.: 125 | Training Loss: 0.14763227343559265 | Validation Loss: 0.1381130911409855\n",
      "Epoch no.: 126 | Training Loss: 0.14760315015912057 | Validation Loss: 0.13803681880235671\n",
      "Epoch no.: 127 | Training Loss: 0.14783398374915124 | Validation Loss: 0.1379961207509041\n",
      "Epoch no.: 128 | Training Loss: 0.14767443753778933 | Validation Loss: 0.13788037449121476\n",
      "Epoch no.: 129 | Training Loss: 0.1476629862189293 | Validation Loss: 0.13784370347857475\n",
      "Epoch no.: 130 | Training Loss: 0.14781600095331668 | Validation Loss: 0.13794220760464668\n",
      "Epoch no.: 131 | Training Loss: 0.1476479133963585 | Validation Loss: 0.1379344381392002\n",
      "Epoch no.: 132 | Training Loss: 0.14770572483539582 | Validation Loss: 0.13792240470647812\n",
      "Epoch no.: 133 | Training Loss: 0.14779167532920837 | Validation Loss: 0.1377207174897194\n",
      "Epoch no.: 134 | Training Loss: 0.1477242887020111 | Validation Loss: 0.13790227845311165\n",
      "Epoch no.: 135 | Training Loss: 0.1477776486426592 | Validation Loss: 0.13776332512497902\n",
      "Epoch no.: 136 | Training Loss: 0.14764296017587186 | Validation Loss: 0.13834968730807304\n",
      "Epoch no.: 137 | Training Loss: 0.14792839981615544 | Validation Loss: 0.13771425783634186\n",
      "Epoch no.: 138 | Training Loss: 0.14769226804375649 | Validation Loss: 0.13809973895549774\n",
      "Epoch no.: 139 | Training Loss: 0.1475968835502863 | Validation Loss: 0.1379556402564049\n",
      "Epoch no.: 140 | Training Loss: 0.14767591163516045 | Validation Loss: 0.13794334679841996\n",
      "Epoch no.: 141 | Training Loss: 0.14765673972666263 | Validation Loss: 0.1377887062728405\n",
      "Epoch no.: 142 | Training Loss: 0.14766076311469079 | Validation Loss: 0.13785307109355927\n",
      "Epoch no.: 143 | Training Loss: 0.14770136810839177 | Validation Loss: 0.13773616775870323\n",
      "Epoch no.: 144 | Training Loss: 0.14769398383796215 | Validation Loss: 0.1378909207880497\n",
      "Epoch no.: 145 | Training Loss: 0.14766721472144126 | Validation Loss: 0.13808012679219245\n",
      "Epoch no.: 146 | Training Loss: 0.14768699035048485 | Validation Loss: 0.13781023696064948\n",
      "Epoch no.: 147 | Training Loss: 0.14776034466922283 | Validation Loss: 0.13778652623295784\n",
      "Epoch no.: 148 | Training Loss: 0.14759701296687125 | Validation Loss: 0.13850450217723848\n",
      "Epoch no.: 149 | Training Loss: 0.14765348695218564 | Validation Loss: 0.13783449828624725\n",
      "Epoch no.: 150 | Training Loss: 0.14762987606227398 | Validation Loss: 0.1377720758318901\n",
      "Epoch no.: 151 | Training Loss: 0.1476846592128277 | Validation Loss: 0.1377902753651142\n",
      "Epoch no.: 152 | Training Loss: 0.14770099826157093 | Validation Loss: 0.13796335756778716\n",
      "Epoch no.: 153 | Training Loss: 0.14777358405292035 | Validation Loss: 0.13796602934598923\n",
      "Epoch no.: 154 | Training Loss: 0.14772645719349384 | Validation Loss: 0.13783332109451293\n",
      "Epoch no.: 155 | Training Loss: 0.1476335972547531 | Validation Loss: 0.13806387037038803\n",
      "Epoch no.: 156 | Training Loss: 0.14771048687398433 | Validation Loss: 0.13789195269346238\n",
      "Epoch no.: 157 | Training Loss: 0.147676964700222 | Validation Loss: 0.13783974274992944\n",
      "Epoch no.: 158 | Training Loss: 0.1478490387648344 | Validation Loss: 0.13780055120587348\n",
      "Epoch no.: 159 | Training Loss: 0.14766605786979198 | Validation Loss: 0.13827554434537886\n",
      "Epoch no.: 160 | Training Loss: 0.14768328927457333 | Validation Loss: 0.13812364414334297\n",
      "Epoch no.: 161 | Training Loss: 0.1477155654132366 | Validation Loss: 0.13804438039660455\n",
      "Epoch no.: 162 | Training Loss: 0.14768639467656614 | Validation Loss: 0.13775329515337945\n",
      "Epoch no.: 163 | Training Loss: 0.14767411552369594 | Validation Loss: 0.13852534070611\n",
      "Epoch no.: 164 | Training Loss: 0.14764705546200274 | Validation Loss: 0.13781041130423546\n",
      "Epoch no.: 165 | Training Loss: 0.14770673654973507 | Validation Loss: 0.1379826545715332\n",
      "Epoch no.: 166 | Training Loss: 0.14772208653390406 | Validation Loss: 0.13816256076097488\n",
      "Epoch no.: 167 | Training Loss: 0.14765549436211586 | Validation Loss: 0.13807329013943673\n",
      "Epoch no.: 168 | Training Loss: 0.14769941098988057 | Validation Loss: 0.1378156453371048\n",
      "Epoch no.: 169 | Training Loss: 0.14765514262020588 | Validation Loss: 0.13787927925586702\n",
      "Epoch no.: 170 | Training Loss: 0.14762687586247922 | Validation Loss: 0.1378442607820034\n",
      "Epoch no.: 171 | Training Loss: 0.1477428048849106 | Validation Loss: 0.13789958134293556\n",
      "Epoch no.: 172 | Training Loss: 0.14761085234582424 | Validation Loss: 0.13784770146012307\n",
      "Epoch no.: 173 | Training Loss: 0.14762854903936387 | Validation Loss: 0.13790970146656037\n",
      "Epoch no.: 174 | Training Loss: 0.14769068151712417 | Validation Loss: 0.1377764545381069\n",
      "Epoch no.: 175 | Training Loss: 0.14764489084482194 | Validation Loss: 0.13780312687158586\n",
      "Epoch no.: 176 | Training Loss: 0.14770763278007507 | Validation Loss: 0.13777417913079262\n",
      "Epoch no.: 177 | Training Loss: 0.14758977524936198 | Validation Loss: 0.13803525418043136\n",
      "Epoch no.: 178 | Training Loss: 0.14758980058133603 | Validation Loss: 0.13781097233295442\n",
      "Epoch no.: 179 | Training Loss: 0.14775852859020233 | Validation Loss: 0.13802413269877434\n",
      "Epoch no.: 180 | Training Loss: 0.14768238931894304 | Validation Loss: 0.13791056200861931\n",
      "Epoch no.: 181 | Training Loss: 0.1476630435138941 | Validation Loss: 0.13794361054897308\n",
      "Epoch no.: 182 | Training Loss: 0.1476482852548361 | Validation Loss: 0.13806403800845146\n",
      "Epoch no.: 183 | Training Loss: 0.14762531630694867 | Validation Loss: 0.1379838041961193\n",
      "Epoch no.: 184 | Training Loss: 0.14763059206306933 | Validation Loss: 0.13833256661891938\n",
      "Epoch no.: 185 | Training Loss: 0.14768752850592137 | Validation Loss: 0.1377936191856861\n",
      "Epoch no.: 186 | Training Loss: 0.14769189342856406 | Validation Loss: 0.1379195272922516\n",
      "Epoch no.: 187 | Training Loss: 0.1476750771701336 | Validation Loss: 0.1379328638315201\n",
      "Epoch no.: 188 | Training Loss: 0.1476806227862835 | Validation Loss: 0.13808310627937317\n",
      "Epoch no.: 189 | Training Loss: 0.14766983330249786 | Validation Loss: 0.13792606443166733\n",
      "Epoch no.: 190 | Training Loss: 0.14761571034789087 | Validation Loss: 0.1378766752779484\n",
      "Epoch no.: 191 | Training Loss: 0.14766371712088586 | Validation Loss: 0.13787990733981131\n",
      "Epoch no.: 192 | Training Loss: 0.1476532833278179 | Validation Loss: 0.13819639906287193\n",
      "Epoch no.: 193 | Training Loss: 0.1476695764809847 | Validation Loss: 0.13834574222564697\n",
      "Epoch no.: 194 | Training Loss: 0.14763577990233898 | Validation Loss: 0.13780684694647788\n",
      "Epoch no.: 195 | Training Loss: 0.14778813168406488 | Validation Loss: 0.13788136020302771\n",
      "Epoch no.: 196 | Training Loss: 0.14762361034750937 | Validation Loss: 0.13780842944979668\n",
      "Epoch no.: 197 | Training Loss: 0.1476288068294525 | Validation Loss: 0.1381966568529606\n",
      "Epoch no.: 198 | Training Loss: 0.1476878634095192 | Validation Loss: 0.13787275552749634\n",
      "Epoch no.: 199 | Training Loss: 0.14764034181833266 | Validation Loss: 0.137887042760849\n",
      "Epoch no.: 200 | Training Loss: 0.1476915754377842 | Validation Loss: 0.13780881091952324\n",
      "Epoch no.: 201 | Training Loss: 0.14772741444408893 | Validation Loss: 0.13809328079223632\n",
      "Epoch no.: 202 | Training Loss: 0.147594600841403 | Validation Loss: 0.13806945458054543\n",
      "Epoch no.: 203 | Training Loss: 0.14767328970134258 | Validation Loss: 0.13785345181822778\n",
      "Epoch no.: 204 | Training Loss: 0.14770534068346022 | Validation Loss: 0.13790893852710723\n",
      "Epoch no.: 205 | Training Loss: 0.1476338844001293 | Validation Loss: 0.138135889172554\n",
      "Epoch no.: 206 | Training Loss: 0.14768654264509679 | Validation Loss: 0.1378196232020855\n",
      "Epoch no.: 207 | Training Loss: 0.1476321765780449 | Validation Loss: 0.13800304308533667\n",
      "Epoch no.: 208 | Training Loss: 0.14757227018475533 | Validation Loss: 0.1387914724647999\n",
      "Epoch no.: 209 | Training Loss: 0.14771256662905216 | Validation Loss: 0.1379115901887417\n",
      "Epoch no.: 210 | Training Loss: 0.14773045539855956 | Validation Loss: 0.13780110105872154\n",
      "Epoch no.: 211 | Training Loss: 0.14757092542946337 | Validation Loss: 0.13790951669216156\n",
      "Epoch no.: 212 | Training Loss: 0.14772252440452577 | Validation Loss: 0.1380708023905754\n",
      "Epoch no.: 213 | Training Loss: 0.14764777816832064 | Validation Loss: 0.13802442848682403\n",
      "Epoch no.: 214 | Training Loss: 0.14761326044797898 | Validation Loss: 0.13788895681500435\n",
      "Epoch no.: 215 | Training Loss: 0.14761442437767983 | Validation Loss: 0.1380233995616436\n",
      "Epoch no.: 216 | Training Loss: 0.14763470023870467 | Validation Loss: 0.1378864750266075\n",
      "Epoch no.: 217 | Training Loss: 0.14762273393571376 | Validation Loss: 0.13800278156995774\n",
      "Epoch no.: 218 | Training Loss: 0.14763273157179355 | Validation Loss: 0.13781640380620958\n",
      "Epoch no.: 219 | Training Loss: 0.14760616958141326 | Validation Loss: 0.13782275319099427\n",
      "Epoch no.: 220 | Training Loss: 0.1476249797642231 | Validation Loss: 0.1381143495440483\n",
      "Epoch no.: 221 | Training Loss: 0.14758482359349728 | Validation Loss: 0.137854465842247\n",
      "Epoch no.: 222 | Training Loss: 0.14761824592947959 | Validation Loss: 0.1379023164510727\n",
      "Epoch no.: 223 | Training Loss: 0.14758932746946812 | Validation Loss: 0.1378607839345932\n",
      "Epoch no.: 224 | Training Loss: 0.1476208757609129 | Validation Loss: 0.1379222482442856\n",
      "Epoch no.: 225 | Training Loss: 0.14766859881579875 | Validation Loss: 0.13787910491228103\n",
      "Epoch no.: 226 | Training Loss: 0.1476013197749853 | Validation Loss: 0.13798464685678483\n",
      "Epoch no.: 227 | Training Loss: 0.14760291181504726 | Validation Loss: 0.1384862743318081\n",
      "Epoch no.: 228 | Training Loss: 0.14762431360781192 | Validation Loss: 0.1380031444132328\n",
      "Epoch no.: 229 | Training Loss: 0.14757809974253178 | Validation Loss: 0.13844963386654854\n",
      "Epoch no.: 230 | Training Loss: 0.1477214962244034 | Validation Loss: 0.13823525607585907\n",
      "Epoch no.: 231 | Training Loss: 0.14757639683783055 | Validation Loss: 0.13792125135660172\n",
      "Epoch no.: 232 | Training Loss: 0.14775132931768895 | Validation Loss: 0.137885170429945\n",
      "Epoch no.: 233 | Training Loss: 0.14771227672696113 | Validation Loss: 0.13788246884942054\n",
      "Epoch no.: 234 | Training Loss: 0.14761615589261054 | Validation Loss: 0.13790842294692993\n",
      "Epoch no.: 235 | Training Loss: 0.14771152287721634 | Validation Loss: 0.13802521154284478\n",
      "Epoch no.: 236 | Training Loss: 0.1476218519359827 | Validation Loss: 0.13795780688524245\n",
      "Epoch no.: 237 | Training Loss: 0.14768310137093066 | Validation Loss: 0.13805825337767602\n",
      "Epoch no.: 238 | Training Loss: 0.14765154637396335 | Validation Loss: 0.13817484229803084\n",
      "Epoch no.: 239 | Training Loss: 0.14767947770655154 | Validation Loss: 0.1380263775587082\n",
      "Epoch no.: 240 | Training Loss: 0.14762695498764514 | Validation Loss: 0.13790322840213776\n",
      "Epoch no.: 241 | Training Loss: 0.1476587474346161 | Validation Loss: 0.13789211213588715\n",
      "Epoch no.: 242 | Training Loss: 0.14760383039712907 | Validation Loss: 0.1379666045308113\n",
      "Epoch no.: 243 | Training Loss: 0.1475999543070793 | Validation Loss: 0.1378440648317337\n",
      "Epoch no.: 244 | Training Loss: 0.14757139801979066 | Validation Loss: 0.13791559115052224\n",
      "Epoch no.: 245 | Training Loss: 0.1475887494534254 | Validation Loss: 0.13782672062516213\n",
      "Epoch no.: 246 | Training Loss: 0.14760152019560338 | Validation Loss: 0.13816877752542495\n",
      "Epoch no.: 247 | Training Loss: 0.1477322519570589 | Validation Loss: 0.138180410861969\n",
      "Epoch no.: 248 | Training Loss: 0.14768870204687118 | Validation Loss: 0.13794225081801414\n",
      "Epoch no.: 249 | Training Loss: 0.14765467695891857 | Validation Loss: 0.13784521520137788\n",
      "Epoch no.: 250 | Training Loss: 0.14757872641086578 | Validation Loss: 0.1379103936254978\n",
      "Epoch no.: 251 | Training Loss: 0.14762608148157597 | Validation Loss: 0.1379980117082596\n",
      "Epoch no.: 252 | Training Loss: 0.14760352112352848 | Validation Loss: 0.13796219304203988\n",
      "Epoch no.: 253 | Training Loss: 0.14766987890005112 | Validation Loss: 0.13828180581331254\n",
      "Epoch no.: 254 | Training Loss: 0.14760802954435348 | Validation Loss: 0.13801193609833717\n",
      "Epoch no.: 255 | Training Loss: 0.1475985376536846 | Validation Loss: 0.13789956122636796\n",
      "Epoch no.: 256 | Training Loss: 0.1476366011053324 | Validation Loss: 0.13788785189390182\n",
      "Epoch no.: 257 | Training Loss: 0.1476260159164667 | Validation Loss: 0.13787854537367822\n",
      "Epoch no.: 258 | Training Loss: 0.14761974282562731 | Validation Loss: 0.13791993334889413\n",
      "Epoch no.: 259 | Training Loss: 0.1476169005036354 | Validation Loss: 0.1380162924528122\n",
      "Epoch no.: 260 | Training Loss: 0.1476538421958685 | Validation Loss: 0.1378363437950611\n",
      "Epoch no.: 261 | Training Loss: 0.147662690281868 | Validation Loss: 0.13796518221497536\n",
      "Epoch no.: 262 | Training Loss: 0.14761581398546697 | Validation Loss: 0.13794219195842744\n",
      "Epoch no.: 263 | Training Loss: 0.14769662976264952 | Validation Loss: 0.1381141796708107\n",
      "Epoch no.: 264 | Training Loss: 0.14761492036283017 | Validation Loss: 0.13790030851960183\n",
      "Epoch no.: 265 | Training Loss: 0.1475504309684038 | Validation Loss: 0.13797134682536125\n",
      "Epoch no.: 266 | Training Loss: 0.1476463933289051 | Validation Loss: 0.13791515454649925\n",
      "Epoch no.: 267 | Training Loss: 0.14764439061284065 | Validation Loss: 0.13795186653733255\n",
      "Epoch no.: 268 | Training Loss: 0.14765182681381703 | Validation Loss: 0.13792792707681656\n",
      "Epoch no.: 269 | Training Loss: 0.14762771055102347 | Validation Loss: 0.13798442333936692\n",
      "Epoch no.: 270 | Training Loss: 0.1476870010048151 | Validation Loss: 0.13836873769760133\n",
      "Epoch no.: 271 | Training Loss: 0.14760988295078278 | Validation Loss: 0.13815485760569574\n",
      "Epoch no.: 272 | Training Loss: 0.14758616320788862 | Validation Loss: 0.13785980865359307\n",
      "Epoch no.: 273 | Training Loss: 0.14761524096131326 | Validation Loss: 0.1383720561861992\n",
      "Epoch no.: 274 | Training Loss: 0.14760840810835363 | Validation Loss: 0.1378755584359169\n",
      "Epoch no.: 275 | Training Loss: 0.14755444772541523 | Validation Loss: 0.13865756765007972\n",
      "Epoch no.: 276 | Training Loss: 0.14762826330959797 | Validation Loss: 0.1379845231771469\n",
      "Epoch no.: 277 | Training Loss: 0.14756164878606795 | Validation Loss: 0.13821546211838723\n",
      "Epoch no.: 278 | Training Loss: 0.14759912520647048 | Validation Loss: 0.13838508799672128\n",
      "Epoch no.: 279 | Training Loss: 0.14763097338378428 | Validation Loss: 0.1383280150592327\n",
      "Epoch no.: 280 | Training Loss: 0.14773106515407564 | Validation Loss: 0.1386693999171257\n",
      "Epoch no.: 281 | Training Loss: 0.14763284482061864 | Validation Loss: 0.13786775693297387\n",
      "Epoch no.: 282 | Training Loss: 0.14757084734737874 | Validation Loss: 0.13792024478316306\n",
      "Epoch no.: 283 | Training Loss: 0.1476195027679205 | Validation Loss: 0.13832528293132781\n",
      "Epoch no.: 284 | Training Loss: 0.14762073382735252 | Validation Loss: 0.13783937096595764\n",
      "Epoch no.: 285 | Training Loss: 0.14764308765530587 | Validation Loss: 0.13810176998376847\n",
      "Epoch no.: 286 | Training Loss: 0.14764121294021607 | Validation Loss: 0.13798853307962416\n",
      "Epoch no.: 287 | Training Loss: 0.14766011118888855 | Validation Loss: 0.13828597068786622\n",
      "Epoch no.: 288 | Training Loss: 0.14768056996166706 | Validation Loss: 0.13799327239394188\n",
      "Epoch no.: 289 | Training Loss: 0.1476042578369379 | Validation Loss: 0.1382351353764534\n",
      "Epoch no.: 290 | Training Loss: 0.14765202216804027 | Validation Loss: 0.13799454271793365\n",
      "Epoch no.: 291 | Training Loss: 0.14762926094233988 | Validation Loss: 0.1379850685596466\n",
      "Epoch no.: 292 | Training Loss: 0.1476813192665577 | Validation Loss: 0.13798198848962784\n",
      "Epoch no.: 293 | Training Loss: 0.14759919993579387 | Validation Loss: 0.13869801312685012\n",
      "Epoch no.: 294 | Training Loss: 0.14764581494033335 | Validation Loss: 0.1383715897798538\n",
      "Epoch no.: 295 | Training Loss: 0.1476385462284088 | Validation Loss: 0.13821970969438552\n",
      "Epoch no.: 296 | Training Loss: 0.14761409506201745 | Validation Loss: 0.1380435675382614\n",
      "Epoch no.: 297 | Training Loss: 0.14769911125302315 | Validation Loss: 0.13792397752404212\n",
      "Epoch no.: 298 | Training Loss: 0.14762356340885163 | Validation Loss: 0.13790324702858925\n",
      "Epoch no.: 299 | Training Loss: 0.1476132284104824 | Validation Loss: 0.1379020057618618\n",
      "Epoch no.: 300 | Training Loss: 0.14759174637496472 | Validation Loss: 0.1379323571920395\n",
      "Epoch no.: 301 | Training Loss: 0.14758210472762584 | Validation Loss: 0.13806208074092866\n",
      "Epoch no.: 302 | Training Loss: 0.14759766101837157 | Validation Loss: 0.13822221457958223\n",
      "Epoch no.: 303 | Training Loss: 0.14766173116862774 | Validation Loss: 0.1380046598613262\n",
      "Epoch no.: 304 | Training Loss: 0.14757099226117135 | Validation Loss: 0.13803477361798286\n",
      "Epoch no.: 305 | Training Loss: 0.14758965834975243 | Validation Loss: 0.13840543925762178\n",
      "Epoch no.: 306 | Training Loss: 0.14760368898510934 | Validation Loss: 0.13793403506278992\n",
      "Epoch no.: 307 | Training Loss: 0.14757862202823163 | Validation Loss: 0.1379111610352993\n",
      "Epoch no.: 308 | Training Loss: 0.1475690183788538 | Validation Loss: 0.1380210466682911\n",
      "Epoch no.: 309 | Training Loss: 0.14764253862202167 | Validation Loss: 0.13805893063545227\n",
      "Epoch no.: 310 | Training Loss: 0.1475765348225832 | Validation Loss: 0.1382007196545601\n",
      "Epoch no.: 311 | Training Loss: 0.14758979938924313 | Validation Loss: 0.13807514756917955\n",
      "Epoch no.: 312 | Training Loss: 0.14755634799599648 | Validation Loss: 0.13806228563189507\n",
      "Epoch no.: 313 | Training Loss: 0.14767487026751042 | Validation Loss: 0.13821902871131897\n",
      "Epoch no.: 314 | Training Loss: 0.14755879804491998 | Validation Loss: 0.13807648196816444\n",
      "Epoch no.: 315 | Training Loss: 0.14763305857777595 | Validation Loss: 0.13814013674855233\n",
      "Epoch no.: 316 | Training Loss: 0.14757348448038102 | Validation Loss: 0.13805870115756988\n",
      "Epoch no.: 317 | Training Loss: 0.14759752973914148 | Validation Loss: 0.1379566289484501\n",
      "Epoch no.: 318 | Training Loss: 0.14762694299221038 | Validation Loss: 0.138152564316988\n",
      "Epoch no.: 319 | Training Loss: 0.14751393765211104 | Validation Loss: 0.13800907209515573\n",
      "Epoch no.: 320 | Training Loss: 0.14763056635856628 | Validation Loss: 0.13823844343423844\n",
      "Epoch no.: 321 | Training Loss: 0.14767856679856778 | Validation Loss: 0.1380348727107048\n",
      "Epoch no.: 322 | Training Loss: 0.1475617630779743 | Validation Loss: 0.13790831342339516\n",
      "Epoch no.: 323 | Training Loss: 0.1477150058746338 | Validation Loss: 0.13811909332871436\n",
      "Epoch no.: 324 | Training Loss: 0.14757776968181133 | Validation Loss: 0.1380239099264145\n",
      "Epoch no.: 325 | Training Loss: 0.1476070558279753 | Validation Loss: 0.1380081906914711\n",
      "Epoch no.: 326 | Training Loss: 0.14763298578560352 | Validation Loss: 0.1379389263689518\n",
      "Epoch no.: 327 | Training Loss: 0.1476552602648735 | Validation Loss: 0.13815635740756987\n",
      "Epoch no.: 328 | Training Loss: 0.1475699383020401 | Validation Loss: 0.13799901455640792\n",
      "Epoch no.: 329 | Training Loss: 0.14769434623420238 | Validation Loss: 0.1380218431353569\n",
      "Epoch no.: 330 | Training Loss: 0.1476016764342785 | Validation Loss: 0.13808322548866273\n",
      "Epoch no.: 331 | Training Loss: 0.14769022546708585 | Validation Loss: 0.1381044030189514\n",
      "Epoch no.: 332 | Training Loss: 0.14767711102962494 | Validation Loss: 0.13799214735627174\n",
      "Epoch no.: 333 | Training Loss: 0.14758580833673476 | Validation Loss: 0.1379820965230465\n",
      "Epoch no.: 334 | Training Loss: 0.14766093783080578 | Validation Loss: 0.1381274752318859\n",
      "Epoch no.: 335 | Training Loss: 0.14763233877718449 | Validation Loss: 0.1380501352250576\n",
      "Epoch no.: 336 | Training Loss: 0.14761449716985225 | Validation Loss: 0.1380010060966015\n",
      "Epoch no.: 337 | Training Loss: 0.14761558420956133 | Validation Loss: 0.13810331299901007\n",
      "Epoch no.: 338 | Training Loss: 0.1475115431845188 | Validation Loss: 0.13837494030594827\n",
      "Epoch no.: 339 | Training Loss: 0.1476220217347145 | Validation Loss: 0.13807401210069656\n",
      "Epoch no.: 340 | Training Loss: 0.14766455605626105 | Validation Loss: 0.1381584644317627\n",
      "Epoch no.: 341 | Training Loss: 0.14761774607002734 | Validation Loss: 0.13836349099874495\n",
      "Epoch no.: 342 | Training Loss: 0.14767247527837754 | Validation Loss: 0.1380437932908535\n",
      "Epoch no.: 343 | Training Loss: 0.14755623236298562 | Validation Loss: 0.1378824345767498\n",
      "Epoch no.: 344 | Training Loss: 0.14763862170279027 | Validation Loss: 0.13805708661675453\n",
      "Epoch no.: 345 | Training Loss: 0.14756058588624 | Validation Loss: 0.13807504028081893\n",
      "Epoch no.: 346 | Training Loss: 0.14760443195700645 | Validation Loss: 0.13797225505113603\n",
      "Epoch no.: 347 | Training Loss: 0.14758122935891152 | Validation Loss: 0.138192380964756\n",
      "Epoch no.: 348 | Training Loss: 0.14765883855521678 | Validation Loss: 0.13803873136639594\n",
      "Epoch no.: 349 | Training Loss: 0.14757344856858254 | Validation Loss: 0.13802867382764816\n",
      "Epoch no.: 350 | Training Loss: 0.14760233953595162 | Validation Loss: 0.1383717752993107\n",
      "Epoch no.: 351 | Training Loss: 0.14763656452298166 | Validation Loss: 0.13832390755414964\n",
      "Epoch no.: 352 | Training Loss: 0.14761735521256925 | Validation Loss: 0.13816251158714293\n",
      "Epoch no.: 353 | Training Loss: 0.14764659889042378 | Validation Loss: 0.13800131306052207\n",
      "Epoch no.: 354 | Training Loss: 0.14757196240127088 | Validation Loss: 0.13807077780365945\n",
      "Epoch no.: 355 | Training Loss: 0.14755736529827118 | Validation Loss: 0.13794315084815026\n",
      "Epoch no.: 356 | Training Loss: 0.14756669737398626 | Validation Loss: 0.13795291557908057\n",
      "Epoch no.: 357 | Training Loss: 0.1476233993470669 | Validation Loss: 0.1380260705947876\n",
      "Epoch no.: 358 | Training Loss: 0.1476471174508333 | Validation Loss: 0.13796880096197128\n",
      "Epoch no.: 359 | Training Loss: 0.14757699467241764 | Validation Loss: 0.1380768783390522\n",
      "Epoch no.: 360 | Training Loss: 0.14770023562014103 | Validation Loss: 0.1381789527833462\n",
      "Epoch no.: 361 | Training Loss: 0.1475374798476696 | Validation Loss: 0.13795167952775955\n",
      "Epoch no.: 362 | Training Loss: 0.14762842141091823 | Validation Loss: 0.13797635734081268\n",
      "Epoch no.: 363 | Training Loss: 0.14757632948458194 | Validation Loss: 0.13801307901740073\n",
      "Epoch no.: 364 | Training Loss: 0.1476228541880846 | Validation Loss: 0.13831673339009284\n",
      "Epoch no.: 365 | Training Loss: 0.1476604001224041 | Validation Loss: 0.13790822476148606\n",
      "Epoch no.: 366 | Training Loss: 0.1476178824901581 | Validation Loss: 0.13800919875502587\n",
      "Epoch no.: 367 | Training Loss: 0.1476446384936571 | Validation Loss: 0.13795590102672578\n",
      "Epoch no.: 368 | Training Loss: 0.14771618470549583 | Validation Loss: 0.138068850338459\n",
      "Epoch no.: 369 | Training Loss: 0.14756205946207046 | Validation Loss: 0.13787836879491805\n",
      "Epoch no.: 370 | Training Loss: 0.1476530631631613 | Validation Loss: 0.13821599036455154\n",
      "Epoch no.: 371 | Training Loss: 0.14758937887847423 | Validation Loss: 0.1381517618894577\n",
      "Epoch no.: 372 | Training Loss: 0.14764303274452686 | Validation Loss: 0.13803111687302588\n",
      "Epoch no.: 373 | Training Loss: 0.14763981483876706 | Validation Loss: 0.13792644590139388\n",
      "Epoch no.: 374 | Training Loss: 0.14758362069725992 | Validation Loss: 0.13814650103449821\n",
      "Epoch no.: 375 | Training Loss: 0.1475188909471035 | Validation Loss: 0.13806419670581818\n",
      "Epoch no.: 376 | Training Loss: 0.14759511843323708 | Validation Loss: 0.1381340853869915\n",
      "Epoch no.: 377 | Training Loss: 0.14764759585261344 | Validation Loss: 0.1382100075483322\n",
      "Epoch no.: 378 | Training Loss: 0.14764448009431363 | Validation Loss: 0.1383504405617714\n",
      "Epoch no.: 379 | Training Loss: 0.1475921979546547 | Validation Loss: 0.1379960611462593\n",
      "Epoch no.: 380 | Training Loss: 0.14761904545128346 | Validation Loss: 0.13812468945980072\n",
      "Epoch no.: 381 | Training Loss: 0.147595437169075 | Validation Loss: 0.13797886446118354\n",
      "Epoch no.: 382 | Training Loss: 0.14762178339064122 | Validation Loss: 0.1380213566124439\n",
      "Epoch no.: 383 | Training Loss: 0.1476109940558672 | Validation Loss: 0.13823391646146774\n",
      "Epoch no.: 384 | Training Loss: 0.14761803157627582 | Validation Loss: 0.13791481778025627\n",
      "Epoch no.: 385 | Training Loss: 0.14758464269340038 | Validation Loss: 0.13805607035756112\n",
      "Epoch no.: 386 | Training Loss: 0.14760470367968082 | Validation Loss: 0.13803725615143775\n",
      "Epoch no.: 387 | Training Loss: 0.14759352505207063 | Validation Loss: 0.13810976296663285\n",
      "Epoch no.: 388 | Training Loss: 0.14754207916557788 | Validation Loss: 0.13804108574986457\n",
      "Epoch no.: 389 | Training Loss: 0.1475470045953989 | Validation Loss: 0.1383039191365242\n",
      "Epoch no.: 390 | Training Loss: 0.1476017463952303 | Validation Loss: 0.1379957377910614\n",
      "Epoch no.: 391 | Training Loss: 0.14763756893575192 | Validation Loss: 0.1380786530673504\n",
      "Epoch no.: 392 | Training Loss: 0.14763590916991234 | Validation Loss: 0.138276045024395\n",
      "Epoch no.: 393 | Training Loss: 0.1476627191901207 | Validation Loss: 0.13805130794644355\n",
      "Epoch no.: 394 | Training Loss: 0.14756900601089 | Validation Loss: 0.13795107007026672\n",
      "Epoch no.: 395 | Training Loss: 0.14760624848306178 | Validation Loss: 0.13801682516932487\n",
      "Epoch no.: 396 | Training Loss: 0.14758257053792476 | Validation Loss: 0.13789496272802354\n",
      "Epoch no.: 397 | Training Loss: 0.14756682462990284 | Validation Loss: 0.13800309002399444\n",
      "Epoch no.: 398 | Training Loss: 0.14763281121850014 | Validation Loss: 0.13846016824245452\n",
      "Epoch no.: 399 | Training Loss: 0.14761896409094333 | Validation Loss: 0.13801798447966576\n",
      "Epoch no.: 400 | Training Loss: 0.14758354648947716 | Validation Loss: 0.13804816901683808\n",
      "Epoch no.: 401 | Training Loss: 0.14764524228870868 | Validation Loss: 0.13846561834216117\n",
      "Epoch no.: 402 | Training Loss: 0.14762465983629228 | Validation Loss: 0.1382956512272358\n",
      "Epoch no.: 403 | Training Loss: 0.14761163093149662 | Validation Loss: 0.13816592767834662\n",
      "Epoch no.: 404 | Training Loss: 0.14761525250971316 | Validation Loss: 0.1381078280508518\n",
      "Epoch no.: 405 | Training Loss: 0.14754196785390378 | Validation Loss: 0.1379604496061802\n",
      "Epoch no.: 406 | Training Loss: 0.1475963731110096 | Validation Loss: 0.13805161118507386\n",
      "Epoch no.: 407 | Training Loss: 0.1476233372837305 | Validation Loss: 0.13794382959604262\n",
      "Epoch no.: 408 | Training Loss: 0.1476081645488739 | Validation Loss: 0.13807130306959153\n",
      "Epoch no.: 409 | Training Loss: 0.14755882903933526 | Validation Loss: 0.13807884305715562\n",
      "Epoch no.: 410 | Training Loss: 0.1475608269125223 | Validation Loss: 0.1380096733570099\n",
      "Epoch no.: 411 | Training Loss: 0.14756912000477315 | Validation Loss: 0.13812998086214065\n",
      "Epoch no.: 412 | Training Loss: 0.1476173394918442 | Validation Loss: 0.13801618292927742\n",
      "Epoch no.: 413 | Training Loss: 0.14758453920483589 | Validation Loss: 0.13808281049132348\n",
      "Epoch no.: 414 | Training Loss: 0.14759141311049462 | Validation Loss: 0.1379097118973732\n",
      "Epoch no.: 415 | Training Loss: 0.14757069781422616 | Validation Loss: 0.13811204433441163\n",
      "Epoch no.: 416 | Training Loss: 0.14758826464414596 | Validation Loss: 0.13803959786891937\n",
      "Epoch no.: 417 | Training Loss: 0.14764704033732415 | Validation Loss: 0.13792134150862695\n",
      "Epoch no.: 418 | Training Loss: 0.1475728527456522 | Validation Loss: 0.13798516243696213\n",
      "Epoch no.: 419 | Training Loss: 0.1475956892222166 | Validation Loss: 0.13804148361086846\n",
      "Epoch no.: 420 | Training Loss: 0.14759054563939572 | Validation Loss: 0.13794523254036903\n",
      "Epoch no.: 421 | Training Loss: 0.14761757366359235 | Validation Loss: 0.1383214883506298\n",
      "Epoch no.: 422 | Training Loss: 0.14762493938207627 | Validation Loss: 0.1380260892212391\n",
      "Epoch no.: 423 | Training Loss: 0.14760946594178675 | Validation Loss: 0.13802212923765184\n",
      "Epoch no.: 424 | Training Loss: 0.14760419465601443 | Validation Loss: 0.1382039152085781\n",
      "Epoch no.: 425 | Training Loss: 0.14761422283947467 | Validation Loss: 0.13799219578504562\n",
      "Epoch no.: 426 | Training Loss: 0.14759319506585597 | Validation Loss: 0.1379987083375454\n",
      "Epoch no.: 427 | Training Loss: 0.14765656888484954 | Validation Loss: 0.13809857219457627\n",
      "Epoch no.: 428 | Training Loss: 0.1475157728046179 | Validation Loss: 0.13794156163930893\n",
      "Epoch no.: 429 | Training Loss: 0.1475924916565418 | Validation Loss: 0.13806913867592813\n",
      "Epoch no.: 430 | Training Loss: 0.14755192950367926 | Validation Loss: 0.13800589218735695\n",
      "Epoch no.: 431 | Training Loss: 0.14762490548193455 | Validation Loss: 0.1380441628396511\n",
      "Epoch no.: 432 | Training Loss: 0.14757694721221923 | Validation Loss: 0.13826794922351837\n",
      "Epoch no.: 433 | Training Loss: 0.1476396581530571 | Validation Loss: 0.1379861168563366\n",
      "Epoch no.: 434 | Training Loss: 0.14768182985484601 | Validation Loss: 0.13800231739878654\n",
      "Epoch no.: 435 | Training Loss: 0.14766251772642136 | Validation Loss: 0.13811013251543044\n",
      "Epoch no.: 436 | Training Loss: 0.14769914731383324 | Validation Loss: 0.13813522160053254\n",
      "Epoch no.: 437 | Training Loss: 0.14760523572564124 | Validation Loss: 0.1380999878048897\n",
      "Epoch no.: 438 | Training Loss: 0.1476032690703869 | Validation Loss: 0.1382034756243229\n",
      "Epoch no.: 439 | Training Loss: 0.14758090287446976 | Validation Loss: 0.1383711390197277\n",
      "Epoch no.: 440 | Training Loss: 0.1475966500490904 | Validation Loss: 0.1382047116756439\n",
      "Epoch no.: 441 | Training Loss: 0.14758349828422068 | Validation Loss: 0.13809517323970794\n",
      "Epoch no.: 442 | Training Loss: 0.14760538846254348 | Validation Loss: 0.1381767950952053\n",
      "Epoch no.: 443 | Training Loss: 0.14761478759348393 | Validation Loss: 0.13812047019600868\n",
      "Epoch no.: 444 | Training Loss: 0.14762535721063613 | Validation Loss: 0.13804725632071496\n",
      "Epoch no.: 445 | Training Loss: 0.14752511270344257 | Validation Loss: 0.1384590096771717\n",
      "Epoch no.: 446 | Training Loss: 0.14759401224553584 | Validation Loss: 0.13817763105034828\n",
      "Epoch no.: 447 | Training Loss: 0.14760487504303454 | Validation Loss: 0.13837141022086144\n",
      "Epoch no.: 448 | Training Loss: 0.14759803913533687 | Validation Loss: 0.13792264088988304\n",
      "Epoch no.: 449 | Training Loss: 0.1475866785645485 | Validation Loss: 0.13808196261525155\n",
      "Epoch no.: 450 | Training Loss: 0.14761603578925134 | Validation Loss: 0.1382428914308548\n",
      "Epoch no.: 451 | Training Loss: 0.14764258176088332 | Validation Loss: 0.1382286712527275\n",
      "Epoch no.: 452 | Training Loss: 0.14761405289173127 | Validation Loss: 0.13805707395076752\n",
      "Epoch no.: 453 | Training Loss: 0.14760530576109887 | Validation Loss: 0.1380339801311493\n",
      "Epoch no.: 454 | Training Loss: 0.1475777604430914 | Validation Loss: 0.13808430656790732\n",
      "Epoch no.: 455 | Training Loss: 0.1475606891512871 | Validation Loss: 0.13793255463242532\n",
      "Epoch no.: 456 | Training Loss: 0.1476046386361122 | Validation Loss: 0.13806647136807443\n",
      "Epoch no.: 457 | Training Loss: 0.14760670214891433 | Validation Loss: 0.1379439152777195\n",
      "Epoch no.: 458 | Training Loss: 0.14758444026112558 | Validation Loss: 0.13808591961860656\n",
      "Epoch no.: 459 | Training Loss: 0.1476108229160309 | Validation Loss: 0.1385357826948166\n",
      "Epoch no.: 460 | Training Loss: 0.14758695900440216 | Validation Loss: 0.13821775317192078\n",
      "Epoch no.: 461 | Training Loss: 0.14757061012089254 | Validation Loss: 0.13830178678035737\n",
      "Epoch no.: 462 | Training Loss: 0.14755945228040218 | Validation Loss: 0.13817722871899604\n",
      "Epoch no.: 463 | Training Loss: 0.14756037011742593 | Validation Loss: 0.13809060230851172\n",
      "Epoch no.: 464 | Training Loss: 0.14754774771630763 | Validation Loss: 0.13794456496834756\n",
      "Epoch no.: 465 | Training Loss: 0.1475831624120474 | Validation Loss: 0.13799399361014367\n",
      "Epoch no.: 466 | Training Loss: 0.1476017277687788 | Validation Loss: 0.13811490312218666\n",
      "Epoch no.: 467 | Training Loss: 0.14757129177451134 | Validation Loss: 0.13817367404699327\n",
      "Epoch no.: 468 | Training Loss: 0.14758762903511524 | Validation Loss: 0.137997879832983\n",
      "Epoch no.: 469 | Training Loss: 0.14767399176955223 | Validation Loss: 0.13806217685341834\n",
      "Epoch no.: 470 | Training Loss: 0.14758378610014916 | Validation Loss: 0.13835682794451715\n",
      "Epoch no.: 471 | Training Loss: 0.14766772583127022 | Validation Loss: 0.13813650086522103\n",
      "Epoch no.: 472 | Training Loss: 0.1475797288864851 | Validation Loss: 0.1382380872964859\n",
      "Epoch no.: 473 | Training Loss: 0.14759344220161438 | Validation Loss: 0.13810854479670526\n",
      "Epoch no.: 474 | Training Loss: 0.14759886279702186 | Validation Loss: 0.1381021559238434\n",
      "Epoch no.: 475 | Training Loss: 0.14757820315659045 | Validation Loss: 0.13838320299983026\n",
      "Epoch no.: 476 | Training Loss: 0.147545021250844 | Validation Loss: 0.13798032701015472\n",
      "Epoch no.: 477 | Training Loss: 0.14761527605354785 | Validation Loss: 0.13804895132780076\n",
      "Epoch no.: 478 | Training Loss: 0.14757335163652896 | Validation Loss: 0.13815950751304626\n",
      "Epoch no.: 479 | Training Loss: 0.14757740311324596 | Validation Loss: 0.13813995122909545\n",
      "Epoch no.: 480 | Training Loss: 0.14754155702888966 | Validation Loss: 0.13799469396471978\n",
      "Epoch no.: 481 | Training Loss: 0.14761912651360035 | Validation Loss: 0.13794762566685675\n",
      "Epoch no.: 482 | Training Loss: 0.1475134366005659 | Validation Loss: 0.13841662481427192\n",
      "Epoch no.: 483 | Training Loss: 0.1475031141936779 | Validation Loss: 0.13862162083387375\n",
      "Epoch no.: 484 | Training Loss: 0.14761273749172688 | Validation Loss: 0.13813312351703644\n",
      "Epoch no.: 485 | Training Loss: 0.1475815063714981 | Validation Loss: 0.1379924535751343\n",
      "Epoch no.: 486 | Training Loss: 0.14758212938904763 | Validation Loss: 0.13809963092207908\n",
      "Epoch no.: 487 | Training Loss: 0.14766238272190094 | Validation Loss: 0.1380079597234726\n",
      "Epoch no.: 488 | Training Loss: 0.14763171710073947 | Validation Loss: 0.13806239143013954\n",
      "Epoch no.: 489 | Training Loss: 0.1475911796092987 | Validation Loss: 0.138065717369318\n",
      "Epoch no.: 490 | Training Loss: 0.14756342433393002 | Validation Loss: 0.13797572925686835\n",
      "Epoch no.: 491 | Training Loss: 0.14761922121047974 | Validation Loss: 0.1382498562335968\n",
      "Epoch no.: 492 | Training Loss: 0.1475919032096863 | Validation Loss: 0.13815651163458825\n",
      "Epoch no.: 493 | Training Loss: 0.14761568441987039 | Validation Loss: 0.1380753718316555\n",
      "Epoch no.: 494 | Training Loss: 0.14754521384835242 | Validation Loss: 0.13799058869481087\n",
      "Epoch no.: 495 | Training Loss: 0.14756006218492984 | Validation Loss: 0.13803312703967094\n",
      "Epoch no.: 496 | Training Loss: 0.14765264704823494 | Validation Loss: 0.1379595898091793\n",
      "Epoch no.: 497 | Training Loss: 0.14754103757441045 | Validation Loss: 0.13819392323493956\n",
      "Epoch no.: 498 | Training Loss: 0.14762069173157216 | Validation Loss: 0.13844689801335336\n",
      "Epoch no.: 499 | Training Loss: 0.14763004116714 | Validation Loss: 0.13797105476260185\n",
      "Epoch no.: 500 | Training Loss: 0.1475742220133543 | Validation Loss: 0.1379999540746212\n",
      "Epoch no.: 501 | Training Loss: 0.1476166568696499 | Validation Loss: 0.13806347474455832\n",
      "Epoch no.: 502 | Training Loss: 0.14751482710242272 | Validation Loss: 0.13797107487916946\n",
      "Epoch no.: 503 | Training Loss: 0.14762879796326162 | Validation Loss: 0.1383425109088421\n",
      "Epoch no.: 504 | Training Loss: 0.147595354616642 | Validation Loss: 0.13793858736753464\n",
      "Epoch no.: 505 | Training Loss: 0.1476117529720068 | Validation Loss: 0.13813407570123673\n",
      "Epoch no.: 506 | Training Loss: 0.14756241507828236 | Validation Loss: 0.13806966543197632\n",
      "Epoch no.: 507 | Training Loss: 0.14759542264044284 | Validation Loss: 0.13815032094717025\n",
      "Epoch no.: 508 | Training Loss: 0.14755501843988894 | Validation Loss: 0.13825119137763978\n",
      "Epoch no.: 509 | Training Loss: 0.14757257901132106 | Validation Loss: 0.1381189212203026\n",
      "Epoch no.: 510 | Training Loss: 0.14755541808903216 | Validation Loss: 0.1381721429526806\n",
      "Epoch no.: 511 | Training Loss: 0.14751950293779373 | Validation Loss: 0.13834259882569314\n",
      "Epoch no.: 512 | Training Loss: 0.14762241579592228 | Validation Loss: 0.1380618207156658\n",
      "Epoch no.: 513 | Training Loss: 0.14760440468788147 | Validation Loss: 0.1380436860024929\n",
      "Epoch no.: 514 | Training Loss: 0.14761901415884496 | Validation Loss: 0.13808934018015862\n",
      "Epoch no.: 515 | Training Loss: 0.14752208992838858 | Validation Loss: 0.13808482587337495\n",
      "Epoch no.: 516 | Training Loss: 0.14758846692740918 | Validation Loss: 0.138217294216156\n",
      "Epoch no.: 517 | Training Loss: 0.1475658069550991 | Validation Loss: 0.13802447244524957\n",
      "Epoch no.: 518 | Training Loss: 0.1476070297509432 | Validation Loss: 0.13812408819794655\n",
      "Epoch no.: 519 | Training Loss: 0.14754697851836682 | Validation Loss: 0.13809262365102767\n",
      "Epoch no.: 520 | Training Loss: 0.14752889692783355 | Validation Loss: 0.1383961834013462\n",
      "Epoch no.: 521 | Training Loss: 0.1476218231022358 | Validation Loss: 0.13805818632245065\n",
      "Epoch no.: 522 | Training Loss: 0.1475773548334837 | Validation Loss: 0.13797681406140327\n",
      "Epoch no.: 523 | Training Loss: 0.14767008550465108 | Validation Loss: 0.13812944963574408\n",
      "Epoch no.: 524 | Training Loss: 0.14757635399699212 | Validation Loss: 0.1380356878042221\n",
      "Epoch no.: 525 | Training Loss: 0.14752949982881547 | Validation Loss: 0.1382772870361805\n",
      "Epoch no.: 526 | Training Loss: 0.14761338338255883 | Validation Loss: 0.1384841874241829\n",
      "Epoch no.: 527 | Training Loss: 0.14762322716414927 | Validation Loss: 0.13806675300002097\n",
      "Epoch no.: 528 | Training Loss: 0.1476194702833891 | Validation Loss: 0.13833704665303231\n",
      "Epoch no.: 529 | Training Loss: 0.14750892251729966 | Validation Loss: 0.13808768689632417\n",
      "Epoch no.: 530 | Training Loss: 0.1475908049941063 | Validation Loss: 0.13813432976603507\n",
      "Epoch no.: 531 | Training Loss: 0.14772965431213378 | Validation Loss: 0.13805604577064515\n",
      "Epoch no.: 532 | Training Loss: 0.14752753682434558 | Validation Loss: 0.13826920092105865\n",
      "Epoch no.: 533 | Training Loss: 0.14757438495755196 | Validation Loss: 0.13808093965053558\n",
      "Epoch no.: 534 | Training Loss: 0.14756802558898927 | Validation Loss: 0.13810492902994156\n",
      "Epoch no.: 535 | Training Loss: 0.14753265127539636 | Validation Loss: 0.1380961060523987\n",
      "Epoch no.: 536 | Training Loss: 0.14758490473032 | Validation Loss: 0.13811571896076202\n",
      "Epoch no.: 537 | Training Loss: 0.14756037794053556 | Validation Loss: 0.13800278902053834\n",
      "Epoch no.: 538 | Training Loss: 0.14757321015000344 | Validation Loss: 0.13807207643985747\n",
      "Epoch no.: 539 | Training Loss: 0.14758429653942584 | Validation Loss: 0.13804446086287497\n",
      "Epoch no.: 540 | Training Loss: 0.14764196813106537 | Validation Loss: 0.13799420073628427\n",
      "Epoch no.: 541 | Training Loss: 0.14758756406605245 | Validation Loss: 0.13803624212741852\n",
      "Epoch no.: 542 | Training Loss: 0.14757524698972702 | Validation Loss: 0.13819419741630554\n",
      "Epoch no.: 543 | Training Loss: 0.14752405665814877 | Validation Loss: 0.13854944631457328\n",
      "Epoch no.: 544 | Training Loss: 0.14755263157188891 | Validation Loss: 0.1379675269126892\n",
      "Epoch no.: 545 | Training Loss: 0.1475798374414444 | Validation Loss: 0.13813984990119935\n",
      "Epoch no.: 546 | Training Loss: 0.1475725945830345 | Validation Loss: 0.13820963501930236\n",
      "Epoch no.: 547 | Training Loss: 0.14763138681650162 | Validation Loss: 0.13800468817353248\n",
      "Epoch no.: 548 | Training Loss: 0.14753304846584797 | Validation Loss: 0.13816346302628518\n",
      "Epoch no.: 549 | Training Loss: 0.14756155595183373 | Validation Loss: 0.13801565691828727\n",
      "Epoch no.: 550 | Training Loss: 0.147618151307106 | Validation Loss: 0.13800619170069695\n",
      "Epoch no.: 551 | Training Loss: 0.14754852131009102 | Validation Loss: 0.13807007372379304\n",
      "Epoch no.: 552 | Training Loss: 0.1476263951510191 | Validation Loss: 0.13819272816181183\n",
      "Epoch no.: 553 | Training Loss: 0.14760848611593247 | Validation Loss: 0.13816884830594062\n",
      "Epoch no.: 554 | Training Loss: 0.14758917465806007 | Validation Loss: 0.13797758668661117\n",
      "Epoch no.: 555 | Training Loss: 0.1476240123063326 | Validation Loss: 0.13805289715528488\n",
      "Epoch no.: 556 | Training Loss: 0.14761271752417088 | Validation Loss: 0.1379999801516533\n",
      "Epoch no.: 557 | Training Loss: 0.1476090758293867 | Validation Loss: 0.13812388032674788\n",
      "Epoch no.: 558 | Training Loss: 0.1475364547967911 | Validation Loss: 0.13854677081108094\n",
      "Epoch no.: 559 | Training Loss: 0.14757968679070474 | Validation Loss: 0.13834066838026046\n",
      "Epoch no.: 560 | Training Loss: 0.14755678974092007 | Validation Loss: 0.13846998587250708\n",
      "Epoch no.: 561 | Training Loss: 0.14757537849247457 | Validation Loss: 0.13808833733201026\n",
      "Epoch no.: 562 | Training Loss: 0.14755823388695716 | Validation Loss: 0.13820150047540664\n",
      "Epoch no.: 563 | Training Loss: 0.14752340041100978 | Validation Loss: 0.13799393624067308\n",
      "Epoch no.: 564 | Training Loss: 0.14763144984841348 | Validation Loss: 0.13805821537971497\n",
      "Epoch no.: 565 | Training Loss: 0.14757453873753548 | Validation Loss: 0.138243817538023\n",
      "Epoch no.: 566 | Training Loss: 0.14754957027733326 | Validation Loss: 0.1380662053823471\n",
      "Epoch no.: 567 | Training Loss: 0.14754780411720275 | Validation Loss: 0.13804396912455558\n",
      "Epoch no.: 568 | Training Loss: 0.14758145563304426 | Validation Loss: 0.13822086974978448\n",
      "Epoch no.: 569 | Training Loss: 0.14756922237575054 | Validation Loss: 0.13824556544423103\n",
      "Epoch no.: 570 | Training Loss: 0.14754126347601415 | Validation Loss: 0.13799350336194038\n",
      "Epoch no.: 571 | Training Loss: 0.1476238214224577 | Validation Loss: 0.13806854337453842\n",
      "Epoch no.: 572 | Training Loss: 0.1475962059944868 | Validation Loss: 0.1383028194308281\n",
      "Epoch no.: 573 | Training Loss: 0.14760152883827687 | Validation Loss: 0.1382426753640175\n",
      "Epoch no.: 574 | Training Loss: 0.14753455482423306 | Validation Loss: 0.1379968985915184\n",
      "Epoch no.: 575 | Training Loss: 0.14759659923613072 | Validation Loss: 0.13802654519677163\n",
      "Epoch no.: 576 | Training Loss: 0.1476206485182047 | Validation Loss: 0.13819763362407683\n",
      "Epoch no.: 577 | Training Loss: 0.14763660691678523 | Validation Loss: 0.13816383779048919\n",
      "Epoch no.: 578 | Training Loss: 0.14762342527508734 | Validation Loss: 0.13810172230005263\n",
      "Epoch no.: 579 | Training Loss: 0.14755884155631066 | Validation Loss: 0.13805748671293258\n",
      "Epoch no.: 580 | Training Loss: 0.14750191882252695 | Validation Loss: 0.13819319307804107\n",
      "Epoch no.: 581 | Training Loss: 0.14759012021124363 | Validation Loss: 0.13799786642193795\n",
      "Epoch no.: 582 | Training Loss: 0.14753029294312 | Validation Loss: 0.1380377322435379\n",
      "Epoch no.: 583 | Training Loss: 0.14764888122677802 | Validation Loss: 0.13827928081154822\n",
      "Epoch no.: 584 | Training Loss: 0.1476171585917473 | Validation Loss: 0.13811834380030633\n",
      "Epoch no.: 585 | Training Loss: 0.1475945081561804 | Validation Loss: 0.13802749365568162\n",
      "Epoch no.: 586 | Training Loss: 0.14759049974381924 | Validation Loss: 0.13818996027112007\n",
      "Epoch no.: 587 | Training Loss: 0.14759105511009693 | Validation Loss: 0.13804589286446572\n",
      "Epoch no.: 588 | Training Loss: 0.14752453364431858 | Validation Loss: 0.13826379477977752\n",
      "Epoch no.: 589 | Training Loss: 0.1476007392257452 | Validation Loss: 0.13802741095423698\n",
      "Epoch no.: 590 | Training Loss: 0.14756093725562094 | Validation Loss: 0.13810208141803743\n",
      "Epoch no.: 591 | Training Loss: 0.14762889981269836 | Validation Loss: 0.1381581835448742\n",
      "Epoch no.: 592 | Training Loss: 0.14751017145812512 | Validation Loss: 0.13798827603459357\n",
      "Epoch no.: 593 | Training Loss: 0.1475138909369707 | Validation Loss: 0.13822103515267373\n",
      "Epoch no.: 594 | Training Loss: 0.14754193678498267 | Validation Loss: 0.1380836009979248\n",
      "Epoch no.: 595 | Training Loss: 0.14751564413309098 | Validation Loss: 0.1379799135029316\n",
      "Epoch no.: 596 | Training Loss: 0.14751143783330917 | Validation Loss: 0.13796431571245193\n",
      "Epoch no.: 597 | Training Loss: 0.14766225308179856 | Validation Loss: 0.13804711475968362\n",
      "Epoch no.: 598 | Training Loss: 0.14754998318850995 | Validation Loss: 0.13832836747169494\n",
      "Epoch no.: 599 | Training Loss: 0.1475648743659258 | Validation Loss: 0.13824068382382393\n",
      "Epoch no.: 600 | Training Loss: 0.1475354941934347 | Validation Loss: 0.13805335983633996\n",
      "Epoch no.: 601 | Training Loss: 0.14748537294566633 | Validation Loss: 0.13808905333280563\n",
      "Epoch no.: 602 | Training Loss: 0.14759779460728167 | Validation Loss: 0.1381971813738346\n",
      "Epoch no.: 603 | Training Loss: 0.14755714260041713 | Validation Loss: 0.13828450366854667\n",
      "Epoch no.: 604 | Training Loss: 0.1475487192720175 | Validation Loss: 0.13811046183109282\n",
      "Epoch no.: 605 | Training Loss: 0.14761106885969638 | Validation Loss: 0.13809574767947197\n",
      "Epoch no.: 606 | Training Loss: 0.14757905066013335 | Validation Loss: 0.13810817450284957\n",
      "Epoch no.: 607 | Training Loss: 0.14758018024265765 | Validation Loss: 0.1382081091403961\n",
      "Epoch no.: 608 | Training Loss: 0.14750403851270677 | Validation Loss: 0.1379839949309826\n",
      "Epoch no.: 609 | Training Loss: 0.1476205039769411 | Validation Loss: 0.13802657425403594\n",
      "Epoch no.: 610 | Training Loss: 0.14761725753545762 | Validation Loss: 0.1381254695355892\n",
      "Epoch no.: 611 | Training Loss: 0.14754495985805988 | Validation Loss: 0.1381018728017807\n",
      "Epoch no.: 612 | Training Loss: 0.147554579526186 | Validation Loss: 0.13835979402065277\n",
      "Epoch no.: 613 | Training Loss: 0.14765791684389115 | Validation Loss: 0.13826391249895095\n",
      "Epoch no.: 614 | Training Loss: 0.14754677452147008 | Validation Loss: 0.13805778175592423\n",
      "Epoch no.: 615 | Training Loss: 0.14762141659855843 | Validation Loss: 0.13819745630025865\n",
      "Epoch no.: 616 | Training Loss: 0.14755842074751854 | Validation Loss: 0.1380451500415802\n",
      "Epoch no.: 617 | Training Loss: 0.14753848493099211 | Validation Loss: 0.13816747963428497\n",
      "Epoch no.: 618 | Training Loss: 0.1475849113613367 | Validation Loss: 0.13836189582943917\n",
      "Epoch no.: 619 | Training Loss: 0.1475926587730646 | Validation Loss: 0.13821716085076333\n",
      "Epoch no.: 620 | Training Loss: 0.14766429513692855 | Validation Loss: 0.1383952707052231\n",
      "Epoch no.: 621 | Training Loss: 0.14757670924067498 | Validation Loss: 0.138066116720438\n",
      "Epoch no.: 622 | Training Loss: 0.14757073000073434 | Validation Loss: 0.13813632801175119\n",
      "Epoch no.: 623 | Training Loss: 0.14762656062841414 | Validation Loss: 0.13816941380500794\n",
      "Epoch no.: 624 | Training Loss: 0.14763934709131718 | Validation Loss: 0.13807265907526017\n",
      "Epoch no.: 625 | Training Loss: 0.14760673515498637 | Validation Loss: 0.1383274368941784\n",
      "Epoch no.: 626 | Training Loss: 0.14760691314935684 | Validation Loss: 0.1381131201982498\n",
      "Epoch no.: 627 | Training Loss: 0.14755421817302705 | Validation Loss: 0.13808346092700957\n",
      "Epoch no.: 628 | Training Loss: 0.14754091396927835 | Validation Loss: 0.138077113032341\n",
      "Epoch no.: 629 | Training Loss: 0.14759383223950862 | Validation Loss: 0.13805188089609147\n",
      "Epoch no.: 630 | Training Loss: 0.14760738417506217 | Validation Loss: 0.1381637707352638\n",
      "Epoch no.: 631 | Training Loss: 0.14753624960780143 | Validation Loss: 0.13798415884375573\n",
      "Epoch no.: 632 | Training Loss: 0.14750889852643012 | Validation Loss: 0.1383357711136341\n",
      "Epoch no.: 633 | Training Loss: 0.14764292292296888 | Validation Loss: 0.1382531151175499\n",
      "Epoch no.: 634 | Training Loss: 0.14753992758691312 | Validation Loss: 0.13804113417863845\n",
      "Epoch no.: 635 | Training Loss: 0.14759990125894545 | Validation Loss: 0.13843521177768708\n",
      "Epoch no.: 636 | Training Loss: 0.14754245951771736 | Validation Loss: 0.13810693398118018\n",
      "Epoch no.: 637 | Training Loss: 0.1475500974059105 | Validation Loss: 0.13832088187336922\n",
      "Epoch no.: 638 | Training Loss: 0.14754700899124146 | Validation Loss: 0.1386231929063797\n",
      "Epoch no.: 639 | Training Loss: 0.14758813865482806 | Validation Loss: 0.1380495011806488\n",
      "Epoch no.: 640 | Training Loss: 0.14755431674420832 | Validation Loss: 0.13808641582727432\n",
      "Epoch no.: 641 | Training Loss: 0.14758050315082072 | Validation Loss: 0.13812587186694145\n",
      "Epoch no.: 642 | Training Loss: 0.14752865791320802 | Validation Loss: 0.13814540356397628\n",
      "Epoch no.: 643 | Training Loss: 0.14755678713321685 | Validation Loss: 0.1383766047656536\n",
      "Epoch no.: 644 | Training Loss: 0.14757877379655837 | Validation Loss: 0.13805149048566817\n",
      "Epoch no.: 645 | Training Loss: 0.14755183041095735 | Validation Loss: 0.1380790166556835\n",
      "Epoch no.: 646 | Training Loss: 0.1475695776194334 | Validation Loss: 0.13823821991682053\n",
      "Epoch no.: 647 | Training Loss: 0.14754071608185768 | Validation Loss: 0.13856207132339476\n",
      "Epoch no.: 648 | Training Loss: 0.14753503397107123 | Validation Loss: 0.1380497433245182\n",
      "Epoch no.: 649 | Training Loss: 0.14758344285190106 | Validation Loss: 0.13813985884189606\n",
      "Epoch no.: 650 | Training Loss: 0.14763503693044186 | Validation Loss: 0.13818675726652146\n",
      "Epoch no.: 651 | Training Loss: 0.14760916255414486 | Validation Loss: 0.13805565237998962\n",
      "Epoch no.: 652 | Training Loss: 0.14770820155739783 | Validation Loss: 0.1386253222823143\n",
      "Epoch no.: 653 | Training Loss: 0.1475561009347439 | Validation Loss: 0.1381508804857731\n",
      "Epoch no.: 654 | Training Loss: 0.14749772034585476 | Validation Loss: 0.13835680484771729\n",
      "Epoch no.: 655 | Training Loss: 0.14760183423757553 | Validation Loss: 0.13829569071531295\n",
      "Epoch no.: 656 | Training Loss: 0.1474354825913906 | Validation Loss: 0.13799599781632424\n",
      "Epoch no.: 657 | Training Loss: 0.147612888738513 | Validation Loss: 0.13833241164684296\n",
      "Epoch no.: 658 | Training Loss: 0.14757153019309044 | Validation Loss: 0.13810158222913743\n",
      "Epoch no.: 659 | Training Loss: 0.1475939839333296 | Validation Loss: 0.13810252919793128\n",
      "Epoch no.: 660 | Training Loss: 0.14754977740347386 | Validation Loss: 0.13822533339262008\n",
      "Epoch no.: 661 | Training Loss: 0.14761759720742704 | Validation Loss: 0.1379925511777401\n",
      "Epoch no.: 662 | Training Loss: 0.1475980944186449 | Validation Loss: 0.13818644732236862\n",
      "Epoch no.: 663 | Training Loss: 0.14750623613595962 | Validation Loss: 0.13811675310134888\n",
      "Epoch no.: 664 | Training Loss: 0.14755158826708795 | Validation Loss: 0.13801625445485116\n",
      "Epoch no.: 665 | Training Loss: 0.1475386494398117 | Validation Loss: 0.13885420486330985\n",
      "Epoch no.: 666 | Training Loss: 0.14761125296354294 | Validation Loss: 0.13855550885200502\n",
      "Epoch no.: 667 | Training Loss: 0.14762127459049224 | Validation Loss: 0.13818773105740548\n",
      "Epoch no.: 668 | Training Loss: 0.14754323959350585 | Validation Loss: 0.13818772360682488\n",
      "Epoch no.: 669 | Training Loss: 0.14755137920379638 | Validation Loss: 0.1380474679172039\n",
      "Epoch no.: 670 | Training Loss: 0.14758588284254073 | Validation Loss: 0.13804123625159265\n",
      "Epoch no.: 671 | Training Loss: 0.14753852978348733 | Validation Loss: 0.13841048553586005\n",
      "Epoch no.: 672 | Training Loss: 0.14755282372236253 | Validation Loss: 0.13803504928946495\n",
      "Epoch no.: 673 | Training Loss: 0.1475601854175329 | Validation Loss: 0.1381074033677578\n",
      "Epoch no.: 674 | Training Loss: 0.14752878725528717 | Validation Loss: 0.13859169259667398\n",
      "Epoch no.: 675 | Training Loss: 0.14758182622492313 | Validation Loss: 0.13811451718211173\n",
      "Epoch no.: 676 | Training Loss: 0.14757046982645988 | Validation Loss: 0.13809775933623314\n",
      "Epoch no.: 677 | Training Loss: 0.1475815737247467 | Validation Loss: 0.13815750628709794\n",
      "Epoch no.: 678 | Training Loss: 0.1475525461882353 | Validation Loss: 0.1381621964275837\n",
      "Epoch no.: 679 | Training Loss: 0.14755033500492573 | Validation Loss: 0.1383411966264248\n",
      "Epoch no.: 680 | Training Loss: 0.14757757239043712 | Validation Loss: 0.13850948065519333\n",
      "Epoch no.: 681 | Training Loss: 0.1476002249866724 | Validation Loss: 0.13810118064284324\n",
      "Epoch no.: 682 | Training Loss: 0.14759877540171146 | Validation Loss: 0.13848407194018364\n",
      "Epoch no.: 683 | Training Loss: 0.14760807588696478 | Validation Loss: 0.13824475854635238\n",
      "Epoch no.: 684 | Training Loss: 0.14755169957876205 | Validation Loss: 0.13810389414429664\n",
      "Epoch no.: 685 | Training Loss: 0.14750258937478067 | Validation Loss: 0.13811720982193948\n",
      "Epoch no.: 686 | Training Loss: 0.14757385239005089 | Validation Loss: 0.1380645178258419\n",
      "Epoch no.: 687 | Training Loss: 0.14753754012286663 | Validation Loss: 0.13853806257247925\n",
      "Epoch no.: 688 | Training Loss: 0.14756435550749303 | Validation Loss: 0.13811948373913766\n",
      "Epoch no.: 689 | Training Loss: 0.14765025056898595 | Validation Loss: 0.13812987953424455\n",
      "Epoch no.: 690 | Training Loss: 0.14753140904009343 | Validation Loss: 0.13802840262651445\n",
      "Epoch no.: 691 | Training Loss: 0.14758917272090913 | Validation Loss: 0.138274597376585\n",
      "Epoch no.: 692 | Training Loss: 0.14754036553204058 | Validation Loss: 0.13812276646494864\n",
      "Epoch no.: 693 | Training Loss: 0.14756194218993188 | Validation Loss: 0.13814280405640603\n",
      "Epoch no.: 694 | Training Loss: 0.14754605419933797 | Validation Loss: 0.13823045939207076\n",
      "Epoch no.: 695 | Training Loss: 0.14756678119301797 | Validation Loss: 0.13823419213294982\n",
      "Epoch no.: 696 | Training Loss: 0.147589600533247 | Validation Loss: 0.13810304328799247\n",
      "Epoch no.: 697 | Training Loss: 0.14755353920161723 | Validation Loss: 0.1382330447435379\n",
      "Epoch no.: 698 | Training Loss: 0.14760609813034534 | Validation Loss: 0.13812321051955223\n",
      "Epoch no.: 699 | Training Loss: 0.14758266665041447 | Validation Loss: 0.13819249868392944\n",
      "Epoch no.: 700 | Training Loss: 0.14752547405660152 | Validation Loss: 0.13822486847639084\n",
      "Epoch no.: 701 | Training Loss: 0.14756002843379976 | Validation Loss: 0.13814422562718393\n",
      "Epoch no.: 702 | Training Loss: 0.14752968527376653 | Validation Loss: 0.13800338208675383\n",
      "Epoch no.: 703 | Training Loss: 0.14755246333777905 | Validation Loss: 0.13810844719409943\n",
      "Epoch no.: 704 | Training Loss: 0.14755992032587528 | Validation Loss: 0.1382967695593834\n",
      "Epoch no.: 705 | Training Loss: 0.14758372023701669 | Validation Loss: 0.13816773295402526\n",
      "Epoch no.: 706 | Training Loss: 0.1475630060583353 | Validation Loss: 0.1380804844200611\n",
      "Epoch no.: 707 | Training Loss: 0.14752544224262237 | Validation Loss: 0.1380332313477993\n",
      "Epoch no.: 708 | Training Loss: 0.147598290592432 | Validation Loss: 0.13824591487646104\n",
      "Epoch no.: 709 | Training Loss: 0.14752757534384728 | Validation Loss: 0.13821193799376488\n",
      "Epoch no.: 710 | Training Loss: 0.14756203353404998 | Validation Loss: 0.13847062215209008\n",
      "Epoch no.: 711 | Training Loss: 0.14753725521266461 | Validation Loss: 0.13842521607875824\n",
      "Epoch no.: 712 | Training Loss: 0.14758804827928543 | Validation Loss: 0.1381947986781597\n",
      "Epoch no.: 713 | Training Loss: 0.14760798521339893 | Validation Loss: 0.13831964880228043\n",
      "Epoch no.: 714 | Training Loss: 0.14755468159914018 | Validation Loss: 0.13810750916600228\n",
      "Epoch no.: 715 | Training Loss: 0.14761658117175103 | Validation Loss: 0.1382681168615818\n",
      "Epoch no.: 716 | Training Loss: 0.14765999309718608 | Validation Loss: 0.1381327785551548\n",
      "Epoch no.: 717 | Training Loss: 0.1475487668067217 | Validation Loss: 0.13822616264224052\n",
      "Epoch no.: 718 | Training Loss: 0.14762746229767798 | Validation Loss: 0.13817024677991868\n",
      "Epoch no.: 719 | Training Loss: 0.14759858936071396 | Validation Loss: 0.13807592764496804\n",
      "Epoch no.: 720 | Training Loss: 0.14753576561808587 | Validation Loss: 0.13809986934065818\n",
      "Epoch no.: 721 | Training Loss: 0.14751094676554202 | Validation Loss: 0.13821105435490608\n",
      "Epoch no.: 722 | Training Loss: 0.14749084725975992 | Validation Loss: 0.13813542500138282\n",
      "Epoch no.: 723 | Training Loss: 0.14751602433621883 | Validation Loss: 0.1382140524685383\n",
      "Epoch no.: 724 | Training Loss: 0.1475378107279539 | Validation Loss: 0.13828313052654267\n",
      "Epoch no.: 725 | Training Loss: 0.14752618350088598 | Validation Loss: 0.13847509697079657\n",
      "Epoch no.: 726 | Training Loss: 0.147534514144063 | Validation Loss: 0.13808364421129227\n",
      "Epoch no.: 727 | Training Loss: 0.14756727382540702 | Validation Loss: 0.1381337597966194\n",
      "Epoch no.: 728 | Training Loss: 0.14755079530179502 | Validation Loss: 0.13812996968626975\n",
      "Epoch no.: 729 | Training Loss: 0.1476104512810707 | Validation Loss: 0.13856312036514282\n",
      "Epoch no.: 730 | Training Loss: 0.14762990944087506 | Validation Loss: 0.13831521570682526\n",
      "Epoch no.: 731 | Training Loss: 0.14750968508422374 | Validation Loss: 0.13815367743372917\n",
      "Epoch no.: 732 | Training Loss: 0.14756838642060757 | Validation Loss: 0.1381410054862499\n",
      "Epoch no.: 733 | Training Loss: 0.14757406242191792 | Validation Loss: 0.13821829706430436\n",
      "Epoch no.: 734 | Training Loss: 0.14754022635519504 | Validation Loss: 0.13833015114068986\n",
      "Epoch no.: 735 | Training Loss: 0.14754569306969642 | Validation Loss: 0.1382353775203228\n",
      "Epoch no.: 736 | Training Loss: 0.14753183960914612 | Validation Loss: 0.13821710124611855\n",
      "Epoch no.: 737 | Training Loss: 0.14762450277805328 | Validation Loss: 0.13823893591761588\n",
      "Epoch no.: 738 | Training Loss: 0.1475467935949564 | Validation Loss: 0.1381956472992897\n",
      "Epoch no.: 739 | Training Loss: 0.14753387585282327 | Validation Loss: 0.13841691240668297\n",
      "Epoch no.: 740 | Training Loss: 0.1475409208983183 | Validation Loss: 0.13817407861351966\n",
      "Epoch no.: 741 | Training Loss: 0.14759281203150748 | Validation Loss: 0.13831781670451165\n",
      "Epoch no.: 742 | Training Loss: 0.14754513911902906 | Validation Loss: 0.1382744498550892\n",
      "Epoch no.: 743 | Training Loss: 0.1476243180781603 | Validation Loss: 0.13807834386825563\n",
      "Epoch no.: 744 | Training Loss: 0.14763255812227727 | Validation Loss: 0.13827801272273063\n",
      "Epoch no.: 745 | Training Loss: 0.14758353628218174 | Validation Loss: 0.138093052059412\n",
      "Epoch no.: 746 | Training Loss: 0.14760343804955484 | Validation Loss: 0.13825058788061143\n",
      "Epoch no.: 747 | Training Loss: 0.14754622489213942 | Validation Loss: 0.13818045929074288\n",
      "Epoch no.: 748 | Training Loss: 0.14757065527141094 | Validation Loss: 0.13817027434706688\n",
      "Epoch no.: 749 | Training Loss: 0.14760804183781148 | Validation Loss: 0.13812372833490372\n",
      "Epoch no.: 750 | Training Loss: 0.14762509763240814 | Validation Loss: 0.13821022137999533\n",
      "Epoch no.: 751 | Training Loss: 0.1475218703597784 | Validation Loss: 0.13812318444252014\n",
      "Epoch no.: 752 | Training Loss: 0.14752738147974015 | Validation Loss: 0.1382540449500084\n",
      "Epoch no.: 753 | Training Loss: 0.14757945097982883 | Validation Loss: 0.13826607391238213\n",
      "Epoch no.: 754 | Training Loss: 0.14755308128893374 | Validation Loss: 0.13827736154198647\n",
      "Epoch no.: 755 | Training Loss: 0.1475480867922306 | Validation Loss: 0.13827429860830306\n",
      "Epoch no.: 756 | Training Loss: 0.14755321115255357 | Validation Loss: 0.1381143569946289\n",
      "Epoch no.: 757 | Training Loss: 0.14767167791724206 | Validation Loss: 0.13845648989081383\n",
      "Epoch no.: 758 | Training Loss: 0.14756214663386344 | Validation Loss: 0.13836251720786094\n",
      "Epoch no.: 759 | Training Loss: 0.1475770839303732 | Validation Loss: 0.1383010484278202\n",
      "Epoch no.: 760 | Training Loss: 0.14755552455782892 | Validation Loss: 0.1381521537899971\n",
      "Epoch no.: 761 | Training Loss: 0.14754067480564118 | Validation Loss: 0.13806864768266677\n",
      "Epoch no.: 762 | Training Loss: 0.14753107577562333 | Validation Loss: 0.13823741376399995\n",
      "Epoch no.: 763 | Training Loss: 0.14757432885468005 | Validation Loss: 0.13819814547896386\n",
      "Epoch no.: 764 | Training Loss: 0.14754067175090313 | Validation Loss: 0.13830641582608222\n",
      "Epoch no.: 765 | Training Loss: 0.14754028826951981 | Validation Loss: 0.13817435055971145\n",
      "Epoch no.: 766 | Training Loss: 0.14753691360354423 | Validation Loss: 0.1381085716187954\n",
      "Epoch no.: 767 | Training Loss: 0.14752437748014927 | Validation Loss: 0.13820911571383476\n",
      "Epoch no.: 768 | Training Loss: 0.14755675368010998 | Validation Loss: 0.13815531358122826\n",
      "Epoch no.: 769 | Training Loss: 0.1475086258351803 | Validation Loss: 0.13807347193360328\n",
      "Epoch no.: 770 | Training Loss: 0.1475611862540245 | Validation Loss: 0.1381213255226612\n",
      "Epoch no.: 771 | Training Loss: 0.14755929961800576 | Validation Loss: 0.13827319890260698\n",
      "Epoch no.: 772 | Training Loss: 0.14753921531140804 | Validation Loss: 0.13829662501811982\n",
      "Epoch no.: 773 | Training Loss: 0.14750487469136714 | Validation Loss: 0.13808371797204017\n",
      "Epoch no.: 774 | Training Loss: 0.14770087212324143 | Validation Loss: 0.1381566159427166\n",
      "Epoch no.: 775 | Training Loss: 0.147593964189291 | Validation Loss: 0.13837359845638275\n",
      "Epoch no.: 776 | Training Loss: 0.14753261409699917 | Validation Loss: 0.13817570060491563\n",
      "Epoch no.: 777 | Training Loss: 0.14750441171228887 | Validation Loss: 0.13819923549890517\n",
      "Epoch no.: 778 | Training Loss: 0.14757974788546563 | Validation Loss: 0.1383906878530979\n",
      "Epoch no.: 779 | Training Loss: 0.14747804455459118 | Validation Loss: 0.13841193839907645\n",
      "Epoch no.: 780 | Training Loss: 0.14759174801409244 | Validation Loss: 0.13806455209851265\n",
      "Epoch no.: 781 | Training Loss: 0.14756700180470944 | Validation Loss: 0.13806253895163537\n",
      "Epoch no.: 782 | Training Loss: 0.147595177218318 | Validation Loss: 0.13813689798116685\n",
      "Epoch no.: 783 | Training Loss: 0.14755034305155276 | Validation Loss: 0.13827745243906975\n",
      "Epoch no.: 784 | Training Loss: 0.1474944879859686 | Validation Loss: 0.13837017342448235\n",
      "Epoch no.: 785 | Training Loss: 0.1475351744145155 | Validation Loss: 0.13809726536273956\n",
      "Epoch no.: 786 | Training Loss: 0.14752576678991317 | Validation Loss: 0.1380595751106739\n",
      "Epoch no.: 787 | Training Loss: 0.1475290272384882 | Validation Loss: 0.13840271830558776\n",
      "Epoch no.: 788 | Training Loss: 0.1475349999219179 | Validation Loss: 0.13810517862439156\n",
      "Epoch no.: 789 | Training Loss: 0.1475813317298889 | Validation Loss: 0.1382062591612339\n",
      "Epoch no.: 790 | Training Loss: 0.14755778677761555 | Validation Loss: 0.13820158839225768\n",
      "Epoch no.: 791 | Training Loss: 0.1475162795931101 | Validation Loss: 0.13819245770573615\n",
      "Epoch no.: 792 | Training Loss: 0.14754297688603402 | Validation Loss: 0.13809801638126373\n",
      "Epoch no.: 793 | Training Loss: 0.1474980019032955 | Validation Loss: 0.1383645012974739\n",
      "Epoch no.: 794 | Training Loss: 0.14756891675293446 | Validation Loss: 0.1380769982933998\n",
      "Epoch no.: 795 | Training Loss: 0.14757254488766194 | Validation Loss: 0.13821623101830482\n",
      "Epoch no.: 796 | Training Loss: 0.14753000907599925 | Validation Loss: 0.1384321190416813\n",
      "Epoch no.: 797 | Training Loss: 0.14759877666831017 | Validation Loss: 0.13822899609804154\n",
      "Epoch no.: 798 | Training Loss: 0.1475610190629959 | Validation Loss: 0.13810344338417052\n",
      "Epoch no.: 799 | Training Loss: 0.14762913972139358 | Validation Loss: 0.13807055056095124\n",
      "Epoch no.: 800 | Training Loss: 0.14758890308439732 | Validation Loss: 0.1380593828856945\n",
      "Epoch no.: 801 | Training Loss: 0.1475560922920704 | Validation Loss: 0.1380888305604458\n",
      "Epoch no.: 802 | Training Loss: 0.14757114566862584 | Validation Loss: 0.13812780007719994\n",
      "Epoch no.: 803 | Training Loss: 0.14753460101783275 | Validation Loss: 0.13826097920536995\n",
      "Epoch no.: 804 | Training Loss: 0.14759457975625992 | Validation Loss: 0.1381589464843273\n",
      "Epoch no.: 805 | Training Loss: 0.14759257040917872 | Validation Loss: 0.1381584659218788\n",
      "Epoch no.: 806 | Training Loss: 0.14751297630369664 | Validation Loss: 0.13831818997859954\n",
      "Epoch no.: 807 | Training Loss: 0.14757282920181752 | Validation Loss: 0.13816672042012215\n",
      "Epoch no.: 808 | Training Loss: 0.14758566603064538 | Validation Loss: 0.1381609968841076\n",
      "Epoch no.: 809 | Training Loss: 0.14755782768130302 | Validation Loss: 0.1380881167948246\n",
      "Epoch no.: 810 | Training Loss: 0.14754925549030304 | Validation Loss: 0.13812793269753457\n",
      "Epoch no.: 811 | Training Loss: 0.14757803075015544 | Validation Loss: 0.13818154633045196\n",
      "Epoch no.: 812 | Training Loss: 0.14756197825074197 | Validation Loss: 0.13837693855166436\n",
      "Epoch no.: 813 | Training Loss: 0.1475019870698452 | Validation Loss: 0.1382732793688774\n",
      "Epoch no.: 814 | Training Loss: 0.14754555366933345 | Validation Loss: 0.1381664291024208\n",
      "Epoch no.: 815 | Training Loss: 0.14755262590944768 | Validation Loss: 0.13813767433166504\n",
      "Epoch no.: 816 | Training Loss: 0.14749740585684776 | Validation Loss: 0.138224758207798\n",
      "Epoch no.: 817 | Training Loss: 0.14755183935165406 | Validation Loss: 0.1386092834174633\n",
      "Epoch no.: 818 | Training Loss: 0.14757385708391665 | Validation Loss: 0.138189647346735\n",
      "Epoch no.: 819 | Training Loss: 0.14754050076007844 | Validation Loss: 0.13830548003315926\n",
      "Epoch no.: 820 | Training Loss: 0.1475496283173561 | Validation Loss: 0.1383235514163971\n",
      "Epoch no.: 821 | Training Loss: 0.14750021480023862 | Validation Loss: 0.13845211938023566\n",
      "Epoch no.: 822 | Training Loss: 0.14754514172673225 | Validation Loss: 0.1380948469042778\n",
      "Epoch no.: 823 | Training Loss: 0.14751617029309272 | Validation Loss: 0.1381867751479149\n",
      "Epoch no.: 824 | Training Loss: 0.14755118563771247 | Validation Loss: 0.13812016546726227\n",
      "Epoch no.: 825 | Training Loss: 0.1476010924577713 | Validation Loss: 0.13831268176436423\n",
      "Epoch no.: 826 | Training Loss: 0.14752142116427422 | Validation Loss: 0.13829367905855178\n",
      "Epoch no.: 827 | Training Loss: 0.1475514405965805 | Validation Loss: 0.13816206604242326\n",
      "Epoch no.: 828 | Training Loss: 0.14759302377700806 | Validation Loss: 0.13814663887023926\n",
      "Epoch no.: 829 | Training Loss: 0.1475652015209198 | Validation Loss: 0.1381823793053627\n",
      "Epoch no.: 830 | Training Loss: 0.14755654364824294 | Validation Loss: 0.13848680928349494\n",
      "Epoch no.: 831 | Training Loss: 0.14749414943158626 | Validation Loss: 0.13870946317911148\n",
      "Epoch no.: 832 | Training Loss: 0.14756977282464503 | Validation Loss: 0.13809318840503693\n",
      "Epoch no.: 833 | Training Loss: 0.1475034212321043 | Validation Loss: 0.1382969707250595\n",
      "Epoch no.: 834 | Training Loss: 0.14752028666436673 | Validation Loss: 0.1384233556687832\n",
      "Epoch no.: 835 | Training Loss: 0.14753505997359753 | Validation Loss: 0.13820988312363625\n",
      "Epoch no.: 836 | Training Loss: 0.14752014547586442 | Validation Loss: 0.13808925002813338\n",
      "Epoch no.: 837 | Training Loss: 0.14758274711668493 | Validation Loss: 0.1382261537015438\n",
      "Epoch no.: 838 | Training Loss: 0.14754648372530937 | Validation Loss: 0.1382492758333683\n",
      "Epoch no.: 839 | Training Loss: 0.1475386366248131 | Validation Loss: 0.13847912102937698\n",
      "Epoch no.: 840 | Training Loss: 0.1475110600888729 | Validation Loss: 0.1383748956024647\n",
      "Epoch no.: 841 | Training Loss: 0.1475510037690401 | Validation Loss: 0.13824670985341073\n",
      "Epoch no.: 842 | Training Loss: 0.14752574935555457 | Validation Loss: 0.13819490745663643\n",
      "Epoch no.: 843 | Training Loss: 0.14754520028829574 | Validation Loss: 0.13818870037794112\n",
      "Epoch no.: 844 | Training Loss: 0.14754180312156678 | Validation Loss: 0.13810454159975052\n",
      "Epoch no.: 845 | Training Loss: 0.14753789372742176 | Validation Loss: 0.13814448863267897\n",
      "Epoch no.: 846 | Training Loss: 0.147588536888361 | Validation Loss: 0.13825740814208984\n",
      "Epoch no.: 847 | Training Loss: 0.1475182016938925 | Validation Loss: 0.13822995126247406\n",
      "Epoch no.: 848 | Training Loss: 0.1475871556997299 | Validation Loss: 0.13810777440667152\n",
      "Epoch no.: 849 | Training Loss: 0.14755296982824803 | Validation Loss: 0.1381454922258854\n",
      "Epoch no.: 850 | Training Loss: 0.14752466879785062 | Validation Loss: 0.13812601044774056\n",
      "Epoch no.: 851 | Training Loss: 0.1475816509872675 | Validation Loss: 0.13823674470186234\n",
      "Epoch no.: 852 | Training Loss: 0.14752256274223327 | Validation Loss: 0.13855839744210244\n",
      "Epoch no.: 853 | Training Loss: 0.14752039462327957 | Validation Loss: 0.13815355077385902\n",
      "Epoch no.: 854 | Training Loss: 0.14758894078433513 | Validation Loss: 0.1382562078535557\n",
      "Epoch no.: 855 | Training Loss: 0.14752534806728362 | Validation Loss: 0.13807516917586327\n",
      "Epoch no.: 856 | Training Loss: 0.14758227072656155 | Validation Loss: 0.1381364218890667\n",
      "Epoch no.: 857 | Training Loss: 0.14757306218147279 | Validation Loss: 0.13822986483573912\n",
      "Epoch no.: 858 | Training Loss: 0.1475525003671646 | Validation Loss: 0.13830815777182578\n",
      "Epoch no.: 859 | Training Loss: 0.14756986364722252 | Validation Loss: 0.1385535828769207\n",
      "Epoch no.: 860 | Training Loss: 0.14758635126054287 | Validation Loss: 0.13823046833276748\n",
      "Epoch no.: 861 | Training Loss: 0.14750986322760581 | Validation Loss: 0.13806199058890342\n",
      "Epoch no.: 862 | Training Loss: 0.14753409914672375 | Validation Loss: 0.13822753876447677\n",
      "Epoch no.: 863 | Training Loss: 0.14754103399813176 | Validation Loss: 0.13817355707287787\n",
      "Epoch no.: 864 | Training Loss: 0.14756530597805978 | Validation Loss: 0.1380751185119152\n",
      "Epoch no.: 865 | Training Loss: 0.14750895857810975 | Validation Loss: 0.13824503496289253\n",
      "Epoch no.: 866 | Training Loss: 0.14748190231621267 | Validation Loss: 0.1382378302514553\n",
      "Epoch no.: 867 | Training Loss: 0.1475470121204853 | Validation Loss: 0.13829121738672256\n",
      "Epoch no.: 868 | Training Loss: 0.14754993930459023 | Validation Loss: 0.1383416973054409\n",
      "Epoch no.: 869 | Training Loss: 0.1475139469653368 | Validation Loss: 0.13842995688319207\n",
      "Epoch no.: 870 | Training Loss: 0.1475386107712984 | Validation Loss: 0.13809192553162575\n",
      "Epoch no.: 871 | Training Loss: 0.1474911693483591 | Validation Loss: 0.13810699358582496\n",
      "Epoch no.: 872 | Training Loss: 0.14756375566124916 | Validation Loss: 0.13850179687142372\n",
      "Epoch no.: 873 | Training Loss: 0.14748980350792407 | Validation Loss: 0.13824406415224075\n",
      "Epoch no.: 874 | Training Loss: 0.14758232936263085 | Validation Loss: 0.13820450380444527\n",
      "Epoch no.: 875 | Training Loss: 0.1475746411830187 | Validation Loss: 0.13813393339514732\n",
      "Epoch no.: 876 | Training Loss: 0.14749337933957576 | Validation Loss: 0.138220976293087\n",
      "Epoch no.: 877 | Training Loss: 0.14753822781145573 | Validation Loss: 0.1384694069623947\n",
      "Epoch no.: 878 | Training Loss: 0.14755205914378167 | Validation Loss: 0.13821211755275725\n",
      "Epoch no.: 879 | Training Loss: 0.14756095930933952 | Validation Loss: 0.13822765201330184\n",
      "Epoch no.: 880 | Training Loss: 0.14747943855822088 | Validation Loss: 0.1384727366268635\n",
      "Epoch no.: 881 | Training Loss: 0.14759840197861196 | Validation Loss: 0.1380726896226406\n",
      "Epoch no.: 882 | Training Loss: 0.1475607968121767 | Validation Loss: 0.13829193711280824\n",
      "Epoch no.: 883 | Training Loss: 0.14752077825367452 | Validation Loss: 0.13841840475797654\n",
      "Epoch no.: 884 | Training Loss: 0.14752318531274797 | Validation Loss: 0.13833402916789056\n",
      "Epoch no.: 885 | Training Loss: 0.14752613112330437 | Validation Loss: 0.1383273907005787\n",
      "Epoch no.: 886 | Training Loss: 0.1475434087961912 | Validation Loss: 0.13825506791472436\n",
      "Epoch no.: 887 | Training Loss: 0.14757647044956684 | Validation Loss: 0.13809467405080794\n",
      "Epoch no.: 888 | Training Loss: 0.14751837149262428 | Validation Loss: 0.1381843775510788\n",
      "Epoch no.: 889 | Training Loss: 0.1475262673199177 | Validation Loss: 0.1381332755088806\n",
      "Epoch no.: 890 | Training Loss: 0.14750969871878625 | Validation Loss: 0.13826105371117592\n",
      "Epoch no.: 891 | Training Loss: 0.147544629201293 | Validation Loss: 0.13814002200961112\n",
      "Epoch no.: 892 | Training Loss: 0.14760120958089828 | Validation Loss: 0.13837097808718682\n",
      "Epoch no.: 893 | Training Loss: 0.14757786385715008 | Validation Loss: 0.13824305310845375\n",
      "Epoch no.: 894 | Training Loss: 0.14755033373832702 | Validation Loss: 0.1381528452038765\n",
      "Epoch no.: 895 | Training Loss: 0.14752241231501104 | Validation Loss: 0.13847223669290543\n",
      "Epoch no.: 896 | Training Loss: 0.1475833098590374 | Validation Loss: 0.13845078125596047\n",
      "Epoch no.: 897 | Training Loss: 0.1475338326394558 | Validation Loss: 0.13810852840542792\n",
      "Epoch no.: 898 | Training Loss: 0.1474965399503708 | Validation Loss: 0.13839114978909492\n",
      "Epoch no.: 899 | Training Loss: 0.14753731444478035 | Validation Loss: 0.1383063241839409\n",
      "Epoch no.: 900 | Training Loss: 0.14754528127610683 | Validation Loss: 0.13827227503061296\n",
      "Epoch no.: 901 | Training Loss: 0.1475746065378189 | Validation Loss: 0.13830228969454766\n",
      "Epoch no.: 902 | Training Loss: 0.1475496131181717 | Validation Loss: 0.1381426565349102\n",
      "Epoch no.: 903 | Training Loss: 0.14755287162959577 | Validation Loss: 0.13819020316004754\n",
      "Epoch no.: 904 | Training Loss: 0.1475492838025093 | Validation Loss: 0.13820757120847701\n",
      "Epoch no.: 905 | Training Loss: 0.1474969071894884 | Validation Loss: 0.13819195628166198\n",
      "Epoch no.: 906 | Training Loss: 0.14753853186964988 | Validation Loss: 0.13812030106782913\n",
      "Epoch no.: 907 | Training Loss: 0.14754164054989816 | Validation Loss: 0.1381238393485546\n",
      "Epoch no.: 908 | Training Loss: 0.14752413712441922 | Validation Loss: 0.13818361163139342\n",
      "Epoch no.: 909 | Training Loss: 0.14755828626453876 | Validation Loss: 0.13820217400789261\n",
      "Epoch no.: 910 | Training Loss: 0.14755228251218797 | Validation Loss: 0.13819451928138732\n",
      "Epoch no.: 911 | Training Loss: 0.147516173645854 | Validation Loss: 0.1384027361869812\n",
      "Epoch no.: 912 | Training Loss: 0.147564630061388 | Validation Loss: 0.1383892387151718\n",
      "Epoch no.: 913 | Training Loss: 0.14754269368946551 | Validation Loss: 0.1381193332374096\n",
      "Epoch no.: 914 | Training Loss: 0.14760192714631556 | Validation Loss: 0.13822339102625847\n",
      "Epoch no.: 915 | Training Loss: 0.1474400082975626 | Validation Loss: 0.13830984085798265\n",
      "Epoch no.: 916 | Training Loss: 0.14753683157265185 | Validation Loss: 0.13814123645424842\n",
      "Epoch no.: 917 | Training Loss: 0.147501804754138 | Validation Loss: 0.1382405489683151\n",
      "Epoch no.: 918 | Training Loss: 0.14753650933504103 | Validation Loss: 0.1382792167365551\n",
      "Epoch no.: 919 | Training Loss: 0.1475001598149538 | Validation Loss: 0.1383800432085991\n",
      "Epoch no.: 920 | Training Loss: 0.14752162896096707 | Validation Loss: 0.1381193995475769\n",
      "Epoch no.: 921 | Training Loss: 0.14755748987197875 | Validation Loss: 0.13808804675936698\n",
      "Epoch no.: 922 | Training Loss: 0.14755654387176037 | Validation Loss: 0.1380866177380085\n",
      "Epoch no.: 923 | Training Loss: 0.14753794215619565 | Validation Loss: 0.13816503062844276\n",
      "Epoch no.: 924 | Training Loss: 0.147493427246809 | Validation Loss: 0.13862830102443696\n",
      "Epoch no.: 925 | Training Loss: 0.14750998355448247 | Validation Loss: 0.1382538303732872\n",
      "Epoch no.: 926 | Training Loss: 0.14753042183816434 | Validation Loss: 0.1383825086057186\n",
      "Epoch no.: 927 | Training Loss: 0.14752079479396343 | Validation Loss: 0.13816296979784964\n",
      "Epoch no.: 928 | Training Loss: 0.14756113022565842 | Validation Loss: 0.13830121085047722\n",
      "Epoch no.: 929 | Training Loss: 0.14754743732511996 | Validation Loss: 0.1381944991648197\n",
      "Epoch no.: 930 | Training Loss: 0.14756291709840297 | Validation Loss: 0.13815274462103844\n",
      "Epoch no.: 931 | Training Loss: 0.1475216756761074 | Validation Loss: 0.13815553933382035\n",
      "Epoch no.: 932 | Training Loss: 0.14756671622395515 | Validation Loss: 0.13827603533864022\n",
      "Epoch no.: 933 | Training Loss: 0.14752575494349002 | Validation Loss: 0.13886650800704955\n",
      "Epoch no.: 934 | Training Loss: 0.14756374284625054 | Validation Loss: 0.13844650089740754\n",
      "Epoch no.: 935 | Training Loss: 0.14753780625760554 | Validation Loss: 0.13821240812540053\n",
      "Epoch no.: 936 | Training Loss: 0.14752356328070163 | Validation Loss: 0.13813235908746718\n",
      "Epoch no.: 937 | Training Loss: 0.14753655277192593 | Validation Loss: 0.13818287923932077\n",
      "Epoch no.: 938 | Training Loss: 0.14752239242196083 | Validation Loss: 0.1382902018725872\n",
      "Epoch no.: 939 | Training Loss: 0.14750645346939564 | Validation Loss: 0.13822053968906403\n",
      "Epoch no.: 940 | Training Loss: 0.147499582991004 | Validation Loss: 0.13835349231958388\n",
      "Epoch no.: 941 | Training Loss: 0.14751082278788089 | Validation Loss: 0.1382525973021984\n",
      "Epoch no.: 942 | Training Loss: 0.1475538868457079 | Validation Loss: 0.138153325766325\n",
      "Epoch no.: 943 | Training Loss: 0.14753949984908105 | Validation Loss: 0.138165844976902\n",
      "Epoch no.: 944 | Training Loss: 0.1474905788898468 | Validation Loss: 0.13809962198138237\n",
      "Epoch no.: 945 | Training Loss: 0.14750087358057498 | Validation Loss: 0.13819844275712967\n",
      "Epoch no.: 946 | Training Loss: 0.14754836432635784 | Validation Loss: 0.1381707400083542\n",
      "Epoch no.: 947 | Training Loss: 0.1475182382017374 | Validation Loss: 0.13841184750199317\n",
      "Epoch no.: 948 | Training Loss: 0.1475324470549822 | Validation Loss: 0.13824814707040786\n",
      "Epoch no.: 949 | Training Loss: 0.147568226903677 | Validation Loss: 0.13822603449225426\n",
      "Epoch no.: 950 | Training Loss: 0.14754379712045193 | Validation Loss: 0.13811386823654176\n",
      "Epoch no.: 951 | Training Loss: 0.14761199064552785 | Validation Loss: 0.13816102221608162\n",
      "Epoch no.: 952 | Training Loss: 0.14758044056594372 | Validation Loss: 0.13812432512640954\n",
      "Epoch no.: 953 | Training Loss: 0.14752596512436866 | Validation Loss: 0.13816786780953408\n",
      "Epoch no.: 954 | Training Loss: 0.14748500108718873 | Validation Loss: 0.13808414936065674\n",
      "Epoch no.: 955 | Training Loss: 0.1475168101489544 | Validation Loss: 0.13832537904381753\n",
      "Epoch no.: 956 | Training Loss: 0.1475087710469961 | Validation Loss: 0.13813589364290238\n",
      "Epoch no.: 957 | Training Loss: 0.14750449910759925 | Validation Loss: 0.13839645385742189\n",
      "Epoch no.: 958 | Training Loss: 0.14752068988978861 | Validation Loss: 0.13823106735944748\n",
      "Epoch no.: 959 | Training Loss: 0.1475469161570072 | Validation Loss: 0.13818573206663132\n",
      "Epoch no.: 960 | Training Loss: 0.1475125741958618 | Validation Loss: 0.13828247040510178\n",
      "Epoch no.: 961 | Training Loss: 0.14750660628080367 | Validation Loss: 0.1382601387798786\n",
      "Epoch no.: 962 | Training Loss: 0.14765226893126965 | Validation Loss: 0.13830741718411446\n",
      "Epoch no.: 963 | Training Loss: 0.14754520781338215 | Validation Loss: 0.13813253343105317\n",
      "Epoch no.: 964 | Training Loss: 0.14751799814403058 | Validation Loss: 0.13823458924889565\n",
      "Epoch no.: 965 | Training Loss: 0.14755489334464073 | Validation Loss: 0.13815052732825278\n",
      "Epoch no.: 966 | Training Loss: 0.14751613669097424 | Validation Loss: 0.13819440752267836\n",
      "Epoch no.: 967 | Training Loss: 0.1475135973840952 | Validation Loss: 0.13808087185025214\n",
      "Epoch no.: 968 | Training Loss: 0.14754155963659288 | Validation Loss: 0.1382534086704254\n",
      "Epoch no.: 969 | Training Loss: 0.14750271096825598 | Validation Loss: 0.13831501826643944\n",
      "Epoch no.: 970 | Training Loss: 0.14759006723761559 | Validation Loss: 0.1381941020488739\n",
      "Epoch no.: 971 | Training Loss: 0.14754865743219853 | Validation Loss: 0.13833744302392006\n"
     ]
    }
   ],
   "source": [
    "customnet_start_time = time.time()\n",
    "train_and_validate(epochs, device, customnet, train_loader, val_loader, criterion, customnet_optimizer, customnet_train_loss_array, customnet_val_loss_array)\n",
    "customnet_end_time = time.time()\n",
    "customnet_total_training_time = customnet_end_time - customnet_start_time\n",
    "customnet_avg_training_time_per_epoch = customnet_total_training_time / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bedf4-f1fc-42fd-9d20-8eca910c23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet_predictions = lstmnet(X_test.to(device)).cpu().detach().numpy()\n",
    "customnet_predictions = customnet(X_test.to(device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daaf7ef-2332-4a1a-8243-1515da878b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL REPORT PRINT\n",
    "print()\n",
    "print()\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"Total Parameters:     LSTMNet: {total_lstmnet_parameters}  |  CustomNet: {total_customnet_parameters}\")\n",
    "print()\n",
    "print(f\"Total Training Time:     LSTMNet: {lstmnet_total_training_time}s  |  CustomNet: {customnet_total_training_time}s\")\n",
    "print()\n",
    "print(f\"Average Training Time Per Epoch:     LSTMNet: {lstmnet_avg_training_time_per_epoch}s  |  CustomNet: {customnet_avg_training_time_per_epoch}s\")\n",
    "\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04092862-d9b3-4298-b05d-0394fedcde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_comparison(epochs, lstmnet_train_loss_array, customnet_train_loss_array, \"LSTM Training Loss\", \"CustomNet Training Loss\", \"Training Loss Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704973ce-70d8-4e08-a498-ff1165b083d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_comparison(epochs, lstmnet_val_loss_array, customnet_val_loss_array, \"LSTM Validation Loss\", \"CustomNet Validation Loss\", \"Validation Loss Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ba5a6-74a9-4d3f-a71d-69004c325d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_predictions(Y_test, lstmnet_predictions, customnet_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5df72b-e9e6-497b-a1a1-21ebce31bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mse_loss(Y_test, lstmnet_predictions, customnet_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20b7ee-5595-427c-85d1-568da04a8802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fe2a3-72c0-4ca6-a0ba-057316b02df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btp",
   "language": "python",
   "name": "btp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
