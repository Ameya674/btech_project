{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb40e788-efa9-4a9a-9c14-905feeb157a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d68264-3c1f-44bb-9fc9-721dc29088ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use NVIDIA Geforce GTX 1650\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f9674c-b71e-46c0-b7dc-e6d74e832bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #forget gate\n",
    "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size), requires_grad = True)\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size), requires_grad = True)\n",
    "\n",
    "        #input gate \n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size), requires_grad = True)\n",
    "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size), requires_grad = True)\n",
    "\n",
    "        #c_t\n",
    "        self.W_c = nn.Parameter(torch.Tensor(input_size, hidden_size), requires_grad = True)\n",
    "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size), requires_grad = True)\n",
    "\n",
    "        #output gate\n",
    "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size), requires_grad = True)\n",
    "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size), requires_grad = True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for weight in self.parameters():\n",
    "            if weight.data.ndimension() < 2:  # Bias or 1D weights\n",
    "                nn.init.zeros_(weight)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(weight)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        bs, seq, ip = x.size()\n",
    "        hidden_seq = []\n",
    "\n",
    "        if hidden is None:\n",
    "            h_prev, c_prev = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device),\n",
    "            )\n",
    "        else:\n",
    "            h_prev, c_prev = hidden\n",
    "\n",
    "        for t in range(seq):\n",
    "            x_t = x[:, t, :]\n",
    "            i_t = torch.sigmoid(x_t @ self.W_i + h_prev @ self.U_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.W_c + h_prev @ self.U_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.W_o + h_prev @ self.U_o + self.b_o)\n",
    "            c_t = f_t * c_prev + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim = 0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef76486c-504e-4369-999b-185542bdad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_, (h_t, c_t) = self.lstm(x)\n",
    "        out = F.relu(self.fc(x_[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309fee00-e443-4183-84e9-ddce5b5d02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnit(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # input weight\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size), requires_grad = True)\n",
    "\n",
    "        # forget gate\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size), requires_grad = True)\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size), requires_grad = True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for weight in self.parameters():\n",
    "            if weight.data.ndimension() < 2:  # Bias or 1D weights\n",
    "                nn.init.zeros_(weight)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(weight)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "\n",
    "        bs, seq, ip = x.size()\n",
    "        hidden_seq = []\n",
    "\n",
    "        if hidden is None:\n",
    "            h_prev, c_prev = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device),\n",
    "            )\n",
    "        else:\n",
    "            h_prev, c_prev = hidden\n",
    "\n",
    "        for t in range(seq):\n",
    "            x_t = x[:, t, :]\n",
    "            x_t = torch.tanh(x_t @ self.W_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
    "            h_t = (f_t * h_prev) + ((1 - f_t) * x_t)\n",
    "            c_t = c_prev\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim = 0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1649f63c-5525-4c6f-8846-e33bfcff7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.custom_layer = CustomUnit(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_, (h_t, c_t) = self.custom_layer(x)\n",
    "        out = F.relu(self.fc(x_[:, -1, :]))\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3f12a-4fba-4491-827a-de19331fae09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btp",
   "language": "python",
   "name": "btp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
